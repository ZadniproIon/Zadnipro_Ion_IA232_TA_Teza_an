Harnessing Text-to-Image Attention Prior for Reference-based Multi-view Image Synthesis

By Chenjie Cao and Yunuo Cai and Qiaole Dong and Yikai Wang and Yanwei Fu

Abstract

This paper explores the domain of multi-view image synthesis, aiming to create specific image elements or entire scenes while ensuring visual consistency with reference images. We categorize this task into two approaches: local synthesis, guided by structural cues from reference images (Reference-based inpainting, Ref-inpainting), and global synthesis, which generates entirely new images based solely on reference examples (Novel View Synthesis, NVS). In recent years, Text-to-Image (T2I) generative models have gained attention in various domains. However, adapting them for multi-view synthesis is challenging due to the intricate correlations between reference and target images. To address these challenges efficiently, we introduce Attention Reactivated Contextual Inpainting (ARCI), a unified approach that reformulates both local and global reference-based multi-view synthesis as contextual inpainting, which is enhanced with pre-existing attention mechanisms in T2I models. Formally, self-attention is leveraged to learn feature correlations across different reference views, while cross-attention is utilized to control the generation through prompt tuning. Our contributions of ARCI, built upon the StableDiffusion fine-tuned for text-guided inpainting, include skillfully handling difficult multi-view synthesis tasks with off-the-shelf T2I models, introducing task and view-specific prompt tuning for generative control, achieving end-to-end Ref-inpainting, and implementing block causal masking for autoregressive NVS. We also show the versatility of ARCI by extending it to multi-view generation for superior consistency with the same architecture, which has also been validated through extensive experiments. Codes and models will be released in https://github.com/ewrfcas/ARCI .

1 Introduction

This paper delves into the intricate domain of multi-view image synthesis, with a central focus on crafting specific image elements or complete scenes by leveraging reference images as the foundation. The objective is to generate specific components or even entire images through these references, while meticulously preserving visual coherence in aspects such as geometry, color, and texture between the reference images and their synthesized counterparts. This task can be broadly categorized into two facets: local and global multi-view image synthesis from reference images. The local variant involves the creation of specific image segments by aligning with the locally inherent structural cues found in the reference images. This technique is essentially related to a previously defined concept known as Reference-guided inpainting (Ref-inpainting) Zhou et al. [2021] , as illustrated in Fig. 1 (a). Conversely, the global multi-view image synthesis aims to generate entirely new images, drawing inspiration solely from reference examples, as depicted in Fig. 1 (b). This approach is closely associated with Novel View Synthesis (NVS) Liu et al. [2023b] . In this paper, we introduce a unified methodology to tackle this task by reactivating attention priors derived from extensive text-to-image models Rombach et al. [2022] as in Fig. 2 (a).

In recent years, Text-to-Image (T2I) generative models have garnered substantial attention across various domains, finding applications in diverse areas such as personalization Gal et al. [2022] , controllable image-to-image generation Zhang and Agrawala [2023] , and even 3D generation Poole et al. [2023] . While it may seem intuitive to harness the power of T2I generative models to directly address multi-view synthesis by training additional adapters with zero-initialization, as demonstrated in previous works Hu et al. [2021] , these adapter-based fine-tuning strategies have inherent limitations. They struggle to capture fine-grained correlations, including object orientations and precise object locations, between reference and target images. These nuanced details are pivotal for tasks such as multi-view generation, as exemplified by Ref-inpainting. Furthermore, leveraging T2I models to address image-to-image tasks requires image-based guidance rather than text-based ones. Thus some approaches replace textual encoders with visual ones and optimize them for full fine-tuning of the entire T2I model. This transition often demands substantial computational resources, with training sessions extending to hundreds of GPU hours, as in Yang et al. [2023] . However, training these large T2I models with unfamiliar visual encoders can be computationally intensive and challenging to converge, particularly when working with limited batch sizes. Additionally, most visual encoders, such as image CLIP Radford et al. [2021] , tend to emphasize the learning of semantic features rather than the intricate spatial details essential for tasks involving multi-view synthesis.

Our innovative approach is designed to harness off-the-shelf T2I generative models to tackle both local and global multi-view synthesis, overcoming the aforementioned challenges of extensive training cost and imperfect image encoding. It stems from a profound realization: most T2I models inherently incorporate attention mechanisms adept at discerning spatial correlations within images and text. These self and cross-attention, originally acquired through training with large diffusion generative models, can serve as an intrinsic guiding prior to multi-view image synthesis.

This leads us to a pivotal inquiry: “ Could pre-existing attention mechanisms have already established meaningful correlations between reference images and the intended generative targets? ” To leverage this untapped potential, we introduce Attention Reactivated Contextual Inpainting (ARCI), a unified approach that encompasses both reference-based local and global multi-view synthesis. ARCI ingeniously reformulates reference-based multi-view synthesis as a contextual inpainting process. This involves seamlessly integrating the reference conditions and masked targets into a unified tensor within the self-attention module as in Fig. 2 (a). We then employ pre-trained textual encoders and cross-attention modules to guide the generation of T2I models, infusing them with critical information for specific generative tasks and desired view orders. The contextual inpainting Yu et al. [2018] was originally proposed to leverage attention to infill missing features for inpainting, sharing some similarities to our work, which inpaints target views through information aggregated from references.

Formally, ARCI represents an innovative approach, built upon the StableDiffusion (SD) Rombach et al. [2022] \footnote{ * StableDiffusion 2.0: https://github.com/Stability-AI/stablediffusion .} fine-tuned under text-guided inpainting. ARCI is primarily designed to address a diverse range of image synthesis tasks, including Ref-inpainting and NVS from reference images. Importantly, these tasks can be seamlessly extended into the multi-view scenario, as depicted in Fig. 1 (c)(d). Additionally, we introduce task and view-specific prompt tuning to effectively control generative tasks and define specific view orders. Remarkably, even for the prohibitive Ref-inpainting task, which typically demands sophisticated 3D geometrical warping and 2D inpainting techniques Zhou et al. [2021] , our ARCI framework addresses it end-to-end with minimal additional parameters while keeping all other weights from SD frozen. On the other hand, to tackle the more intricate NVS task, we propose the novel technique of block causal masking, facilitating self-attention-based T2I models in achieving consistent autoregressive generation as in Fig. 1 (d).

The novel contextual formulation of ARCI results in faster convergence and efficient training, outperforming other baselines Zhang and Agrawala [2023] with the same training computation and parameters. Moreover, the introduction of task and view-specific prompt tuning marks a significant advancement in the field, allowing us to efficiently control the generation of T2I models for multi-view synthesis.

We highlight the key contributions as follows: 1) Efficient multi-view synthesis with T2I models: Benefiting from the novel contextual inpainting formulation and inherent attention mechanisms from generative T2I models, the ARCI provides an efficient solution for multi-view synthesis without thoroughly laborious re-training T2I models. 2) Task and view-specific prompt tuning: Our work pioneers the use of task and view-specific prompt tuning, allowing for precise control over generative tasks and view orders. 3) End-to-end Ref-inpainting: Notably, our ARCI addresses the challenging Ref-inpainting end-to-end, without complex 3D geometrical warping and 2D inpainting techniques. 4) Autoregressive NVS with block causal masking : For the intricate NVS task, we introduce the novel concept of block causal masking, enabling self-attention-based T2I models to achieve Autoregressive generation for superior quality and geometric consistency.

2 Related Work

Personalization and Controllability of T2I Models. Recent achievements on T2I have produced impressive visual generations, which could be further extended into local editing Avrahami et al. [2022] . However, these models could only be controlled by natural languages. As “an image is worth hundreds of words”, T2I models based on natural texts fail to produce images with specific textures, locations, identities, and appearances Gal et al. [2022] . Textual inversion Gal et al. [2022] and fine-tuning techniques Ruiz et al. [2022] are proposed for personalized T2I. Meanwhile, many works pay attention to image-guided generations Voynov et al. [2022] . ControlNet Zhang and Agrawala [2023] and T2I-Adapter Mou et al. [2023] learn trainable adapters Houlsby et al. [2019] to inject visual clues to pre-trained T2I models without losing generalization and diversity. But these moderate methods only work for simple style transfers. More spatially complex tasks, such as Ref-inpainting, are difficult to handle by ControlNet as verified in Sec. 4 . In contrast, T2I-based exemplar-editing and NVS have to be fine-tuned on large-scale datasets with strong data augmentation Yang et al. [2023] and large batch size Liu et al. [2023b] . Compared with these aforementioned manners, ARCI enjoys both spatial modeling capability and computational efficiency.

Prompt Tuning Lester et al. [2021] indicates fine-tuning token embeddings for transformers with frozen backbone to preserve the capacity. Prompt tuning is first explored for adaptively learning suitable prompt features for language models rather than manually selecting them for different downstream tasks Liu et al. [2023a] . Moreover, prompt tuning has been further investigated in vision-language models Radford et al. [2021] and discriminative vision models Jia et al. [2022] . Visual prompt tuning in Sohn et al. [2022] prepends trainable tokens before the visual sequence for transferred generations. Though both ARCI and Sohn et al. [2022] aim to tackle image synthesis, our prompt tuning is used for controlling text encoders rather than visual ones. Thus ARCI enjoys more intuitive prompt initialization from task-related textual descriptions. Besides, we should clarify that prompt tuning is different from textual inversion Gal et al. [2022] . In our work, we use prompt tuning to address specific downstream tasks with view information, while textual inversion tends to present personalized image subsets.

Reference-guided Image Inpainting. Image inpainting is a long-standing vision task, which aims to fill missing image regions with coherent results. Both traditional methods Bertalmio et al. [2000] and learning-based ones Zeng et al. [2020] achieved great progress in image inpainting. Furthermore, Ref-inpainting requires recovering a target image with one or several reference views Oh et al. [2019] , which is useful for repairing old buildings or removing occlusions in popular attractions. But Ref-inpainting usually suffers from a sophisticated pipeline Zhou et al. [2021] , including depth estimation, pose estimation, homography warping, and single-view inpainting. Note that for large holes, the geometric pose is not reliable; and the pipeline will be largely degraded. Thus an end-to-end Ref-inpainting pipeline is meaningful. To the best of our knowledge, we are the first ones to tackle such a difficult reference-guided task with T2I models.

Novel View Synthesis from a Single Image. NVS based on a single image is an intractable ill-posed problem, requiring both sufficient geometry understanding and expressive textural presentation Fahim et al. [2021] . Many previous works could partially tackle this problem through single view 3D reconstruction Wu et al. [2017] , 2D generative models Niklaus et al. [2019] , feature cost volumes Chan et al. [2023] , and GAN-based methods Schwarz et al. [2020] . However, these manners still suffer from limited generalization or small angle variations. Recent works devoted to incorporating the knowledge from 2D diffusion-based T2I models into the 3D reconstruction Poole et al. [2023] , largely alleviating the demand for 3D annotated data. On the other hand, Zero123 Liu et al. [2023b] uses another image CLIP encoder to inject image features from the reference view for NVS. Although Zero123 unlocks the view synthesis capability of T2I models, it requires a large batch size and expensive computational resources to stabilize the training stage with an unknown reference image encoder. Moreover, the image encoder in Zero123 can only tackle one reference image, which fails to generate consistent multi-view images.

3 Attention Reactivated Contextual Inpainting

Overview. Our ARCI model is formulated in Sec. 3.1 . Then we explain using attention prior to learn multi-view correspondence in Sec. 3.2 , as well as enhancing self-attention for AR generation. Finally, we discuss the task and view-specific prompt tuning for cross-attention modules in Sec. 3.3 .

3.1 Framework of ARCI

As shown in Fig. 2 , our ARCI framework is built upon the inpainting pre-trained LDM Rombach et al. [2022] . Essentially, we reformulate both Ref-inpainting and NVS as contextual image inpainting problems. The conventional approach to integrating reference images into the generation process involves either concatenating them along the channel dimension of input images or introducing an additional image encoder for the network Yang et al. [2023] . However, both strategies require additional training to rebuild the correlation between the reference and target for the generative model. This is contrary to our motivation to harness the potential of large T2I models with minimal architectural alterations.

Thanks to the convolutional U-net architecture in LDM, we can expand the input image in the spatial dimensions without extra modification. To illustrate, let’s consider a scenario with a single reference image at first. Our input $\mathbf{I}^{\prime}$ is a stitching image of $\mathbf{I}_{ref}$ and the masked target $\hat{\mathbf{I}}_{tar}$ , forming as $\mathbf{I}^{\prime}=[\mathbf{I}_{ref};\hat{\mathbf{I}}_{tar}]\in\mathbb{R}^{H\times 2W}$ . In practice, we take the reference image on the left side, while the target one is placed on the right side. For the multi-view references, we stitch each reference with the specific target as shown in Fig. 3 . All views are learned separately for convolutions and cross-attention, while they share the same self-attention processing as discussed in Sec. 3.2 . As shown in Fig. 3 (a), the multi-view Ref-inpainting leverages information from different reference views to repair the same target one, while multi-view NVS could be seen as an AR generation for sequentially consistent view synthesis (Fig. 3 (b)). Masked targets could be generated successfully along with the ARCI filling the hole in the target side. The inpainting formulation fully exploits the contextual learning capability of pre-trained self-attention in LDM to address the reference-guided generation. More details about the processing of data and masks are discussed in Appendix. A.2 .

Simply stitching reference images and masked targets along the spatial space and self-attention modules could not establish the necessary correlation between them, thus failing to generate the desired target results. Text-guided inpainting needs image-dependent text prompts to drive the diffusion model for the desired generation. It is non-trivial to define Ref-inpainting and NVS with natural languages. On the other hand, it is beneficial to have an image-independent prompt to guide the diffusion model to perform specific tasks. To this end, we propose to use prompt tuning to learn task and view-specific prompts as in Sec. 3.3 . Except for the prompt tuning, all weights in LDM are frozen in Ref-inpainting to maintain the proper generalization as shown in Fig. 2 (b). For the NVS, we need to fine-tune the whole LDM as in Fig. 2 (c), but we clarify that ARCI enjoys much better convergence compared with other fine-tuning based methods, such as Zero123 Liu et al. [2023b] .

3.2 Reactivating Self-Attention Modules

As shown in Fig. 4 (a), given multi-view features $\mathbf{F}_{i}\in\mathbb{R}^{b\times v\times h\times w\times c}$ to layer $i$ with $v$ views, $b,h,w,c$ indicate the batch size, feature height, width, and channels respectively, all MLP, convolutional, and cross-attention layers handle $\mathbf{F}_{i}$ separately. We could easily achieve this by reshaping the view and batch dimension together as $\mathbf{\hat{F}}_{i}\in\mathbb{R}^{bv\times h\times w\times c}$ or $\mathbf{\hat{F}}_{i}\in\mathbb{R}^{bv\times hw\times c}$ . Before the self-attention encoding, we adjust the feature shape as $\mathbf{\tilde{F}}_{i}\in\mathbb{R}^{b\times vhw\times c}$ , thus features across different views could be learned together. To further incorporate positional clues to T2I models for distinguishing different sides of reference and target, we incrementally add positional encoding $P_{i}$ to $\mathbf{F}_{i}$ as 
 
 $P_{i}=\gamma_{i}\cdot\mathrm{cat}([P_{v};P_{Fourier}]),$  (1) 
 where $P_{v},P_{Fourier}$ indicate the trainable view embedding and Fourier absolute positional encoding Vaswani et al. [2017] respectively; $\gamma_{i}$ is a zero-initialized learnable coefficient for each layer.

For the Ref-inpainting, no masking strategy should be considered in self-attention modules. All reference views share the same target one, thus it is unnecessary for ARCI to sequentially repair target views. In contrast, generating consistent novel views from a single image needs our model to handle the sequential generation with dynamic reference views. For example, the same ARCI should accomplish the NVS from one view, two views, and even more. So the AR generation Van den Oord et al. [2016] is suitable to formulate this task.

Block Causal Masking. Although fine-tuning ARCI mitigates heavy training costs of T2I models, it still needs a certain fine-tuning for LDM to effectively tackle challenging NVS as shown in Fig. 2 (c). The intuitive solution is to train an AR-based generative model that can generalize across various view numbers for multi-view synthesis. Converting a pre-trained diffusion model to an AR-based generative model is non-trivial. However, the inpainting formulation utilized by ARCI makes this conversion feasible. Specifically, we just need to adjust the masking strategy during the self-attention learning. We propose the block casual masking as shown in Fig. 4 (b), while the block size of each view is $h\times w$ , matching the size of the stitched reference and target pair. Different from the traditional casual mask which is a lower triangular matrix, the block casual mask enlarges the minimal unit from one token to a $h\times w$ block, ensuring reasonable block-wise receptive fields. In practice, all uncolored tokens in the attention score are masked with “ $-\inf$ ” before the softmax operation. We should clarify the block casual mask can be achieved parallelly and efficiently as discussed in Alg. 1 of Appendix.

3.3 Task&View Prompt Tuning

The prompt embedding is adopted as the textual branch of Fig. 2 . Specifically, we prepare a set of trainable text embeddings for different generative tasks, which are further categorized into task and view prompts. Specifically, task prompt embeddings are shared in the same task, e.g. , all views of Ref-inpainting using the same task embeddings. In contrast, view prompt embeddings are applied to different views to inject different view information through cross-attention modules. Though there are only a few trainable parameters (0.05M to 0.065M), we astonishingly find that prompt tuning is sufficient to drive complex generative tasks such as Ref-inpainting, even though the LDM backbone is completely frozen. The trainable task and view prompt embeddings $p_{t},p_{v}$ are initialized as the averaged embedding of the natural task description. The optimization target can be defined as 
 
 $\footnotesize\negthickspace\{p_{t},p_{v}\}_{*}=\mathop{\arg\min}\limits_{\{p_{t},p_{v}\}}\mathbb{E}\left[\left\|\varepsilon-\varepsilon_{\theta}\left([z_{t};\hat{z}_{0};\mathbf{M}],c_{\phi}(p_{t},p_{v}),t\right)\right\|^{2}\right],$  (2) 
 where $\varepsilon_{\theta}(\cdot)$ is the estimated noise by LDM; $c_{\phi}(\cdot)$ means the frozen CLIP-H; $z_{t}$ is a noisy latent feature of input $z_{0}$ in step $t$ ; $\hat{z}_{0}=z_{0}\odot(1-\mathbf{M})$ are masked latent features that are concatenated to $z_{t}$ with mask $\mathbf{M}$ . Task and view-specific prompt tuning enjoy not only training efficiency but also lightweight saving Lester et al. [2021] . For example, we could address Ref-inpainting for different reference views with the same ARCI, while only 0.01% additional weights of $\{p_{t},p_{v}\}_{*}$ are required to be changed for each view condition. In NVS, we further provide relative poses to ARCI. Following Liu et al. [2023b] , we calculate the 4-channel relative pose in the polar coordinate from the first view to the target one for each view, which is encoded by a two-layer FC. Then the pose feature replaces the last padding token in the prompt embeddings before being applied to the CLIP-H.

4 Experiments

Datasets. For Ref-inpainting, we use the resized 512 $\times$ 512 image pairs from MegaDepth Li and Snavely [2018] , which includes many multi-view famous scenes collected from the Internet. To trade-off between the image correlation and the inpainting difficulty, we empirically retain image pairs with 40% to 70% co-occurrence with about 80k images and 820k pairs. The validation of Ref-inpainting also includes some manual masks from ETH3D scenes Schops et al. [2017] to verify the generalization. For the NVS, we use Objaverse Deitke et al. [2022] rendered by Liu et al. [2023b] including 800k various scenes with object masks. We resize all images to 256 $\times$ 256 as Liu et al. [2023b] . Note that some extreme views with elevation angles less than -10 \textsuperscript{∘} are filtered due to excessive ambiguity. More details about the masking and datasets are in Appendix. A.2 .

Implementation Details. Our ARCI is based on the inpainting fine-tuned SD Rombach et al. [2022] with 0.8 billion parameters. For the task and view prompt tuning, there are 50 trainable prompt tokens at all. We use 90% tokens to represent the task embeddings, while 10% tokens indicate each view respectively. We use the AdamW optimizer with a weight decay of 0.01 to optimize ARCI. For the Ref-inpainting, the prompt tuning’s learning rate is 3e-5. Moreover, 75% masks are randomly generated, and 25% of them are matching-based masks (Appendix. A.2.1 ). For the NVS, ARCI could be tested with the adaptive masking strengthened by the foreground segmentation model as in Appendix. A.3.1 . ARCI could be converged with just 48 batch size and learning rate 1e-5. The multi-view ARCI is fine-tuned based on the one-view version. We also train a multi-view ARCI with 512 batch size and learning rate 3e-5 for NVS. More details are listed in Appendix. A.2.3 .

4.1 Results of Reference-guided Inpainting

Results of One-view Reference. We thoroughly compared the specific Ref-inpainting method Zhou et al. [2021] and existing image reference-based variants of LDM with one-view reference in Tab. 1 and Fig. 5 . Note that ControlNet Zhang and Agrawala [2023] fails to learn the correct spatial correlation between reference images and masked targets, even enhanced with trainable cross-attention learned between reference and target features. Furthermore, we try to warp ground-truth latent features with image matching Tang et al. [2022] as the reference guidance for ControlNet, but the improvement is not prominent. Perceiver Jaegle et al. [2021] and Paint-by-Example Yang et al. [2023] align and learn image features from Image CLIP. Since image features from CLIP contain high-level semantics, they fail to deal with the fine-grained Ref-inpainting as shown in Fig. 5 (e)(f). Though TransFill Zhou et al. [2021] achieves proper results in PSNR and SSIM, it suffers from blur and color difference as in Fig. 5 (g) with challenging viewpoints. ARCI enjoys substantial advantages in both qualitative and quantitative comparisons with negligible trainable weights. We further compare ARCI with TransFill on the officially provided real-world dataset in Tab. 9 and Fig. 14 of the Appendix. Since most instances should be defined as object removal tasks without ground truth, quantitative metrics are for reference only. But ARCI still outperforms TransFill in FID and LPIPS with perceptually pleasant results. Moreover, as shown in Fig. 14 , ARCI enjoys good generalization in unseen or occluded regions, because it gets rid of the constrained geometric warping.

Results of Multi-view Ref-inpainting. We verified models trained with different numbers of reference views in Tab. 1 . The 2-view ARCI suffers from a minor performance decrease, which we attribute to the inherent ambiguities within the MegaDepth validation. However, as the number of reference views increases, there is a notable enhancement in inpainting capability. As shown in Fig. 16 , more consistent references lead to robust inpainting results with sensible structures.

4.2 Results of Novel View Synthesis

Results of single-view NVS. Different from Zero123 Liu et al. [2023b] , we explore the feasibility of learning the challenging NVS with limited resources as in the upper of Tab. 2 and Fig. 6 . The CLIP score Radford et al. [2021] is compared to evaluate the similarity between the generation and the target. Prompt tuning and LoRA-based Hu et al. [2021] fine-tuning are insufficient to achieve high-fidelity results as in Tab. 2 . Fine-tuning the whole LDM enjoys substantial improvements. But all variants of ARCI outperform the state-of-the-art competitor Zero123 with the same training setting (batch size 48 with 2 A6000 GPUs) in Tab. 2 because the latter needs a much larger batch size (1536) for stable training. Thus our contextual inpainting-based ARCI enjoys a good balance between training efficiency and performance. Moreover, we evaluate the effectiveness of Classifier-Free Guidance (CFG) Ho and Salimans [2022] during the training phase, which enjoys better pose guidance with a CFG coefficient of 2.5 for the inference. We further provide the training log of ARCI and Zero123 in Fig. 10 of the Appendix. Obviously, the contextual inpainting-based ARCI enjoys a substantially faster convergence and superior performance.

Results of Multi-view NVS. We further fine-tune the ARCI for multi-view NVS through the AR generation and large batch size (ARCI $\dagger$ ). Quantitative results are shown in the lower of Tab. 2 . Obviously, more reference views lead to better reconstruction quality of ARCI $\dagger$ . Moreover, additional reference images could substantially alleviate the ambiguity, improving the final results with consistent geometry as shown in Fig. 19 . Benefited by the AR, ARCI $\dagger$ can be also generalized to synthesize a group of consistent images with different viewpoints from a single view as shown in Fig. 7 and Fig. 18 . We also show that our method can be generalized to real-world data as in Fig. 12 .

4.3 Analysis and Ablation Studies

Self-Attention Analysis. We show the visualization of self-attention scores attended by masked regions for Ref-inpainting (Alg. 2 ) across different DDIM steps in Fig. 8 . Self-attention can capture correct feature correlations without any backbone fine-tuning. As diffusion sampling progresses, self-attention modules gradually shift their focus from specific key points to broader related regions, which is convincing and intuitive. Because the key landmarks help to swiftly locate the spatial correlation between the reference and target, while the extended receptive fields further refine the generation for the following sampling steps.

Prompt Settings. The length and depth used in the task and view prompt tuning are explored in Tab. 3 . Different from Jia et al. [2022] , we find that ARCI is relatively robust in the length selection. Thus we select 50 for both Ref-inpainting and NVS. Moreover, the deep prompt with much more trainable prompts for different cross-attention layers does not perform well, which may suffer from a little overfitting. For the multi-view scene, we empirically evaluate the 3-view-based Ref-inpainting performance with various proportions of task&view prompt lengths in the right of Tab. 3 . Increasing the proportion of view tokens initially improves the results, followed by a subsequent decline. We think that a few unshared view tokens contribute valuable view orders, while too many unshared tokens would increase the learning difficulty, leading to an inferior prompt tuning performance.

Incremental Positional Encoding. We incrementally add the concatenation of learnable view embedding and absolute positional encoding to each attention block for NVS (Eq. 1 ), improving the performance of both single-view and multi-view-based NVS as verified in Tab. 4 . More ablation studies are verified in the Appendix. A.4.1 .

5 Conclusion

In this paper, we propose ARCI, formulating reference-based multi-view image synthesis as inpainting tasks and addressing them end-to-end. Benefiting from the prompt tuning and the well-learned attention modules in large T2I models, ARCI can address the spatially sophisticated Ref-inpainting and NVS efficiently. Moreover, ARCI could be easily extended to tackle multi-view generation tasks. We also propose block casual masking to accomplish NVS with consistent results autoregressively. Comprehensive experiments on Ref-inpainting and NVS show the effectiveness of ARCI.

Appendix A Appendix

This paper exploited image synthesis with text-to-image models. Because of their impressive generative abilities, these models may produce misinformation or fake images. So we sincerely remind users to pay attention to it. Besides, privacy and consent also become important considerations, as generative models are often trained on large-scale data. Furthermore, generative models may perpetuate biases present in the training data, leading to unfair outcomes. Therefore, we recommend users be responsible and inclusive while using these text-to-image generative models. Note that our method only focuses on technical aspects. Both images and pre-trained models used in this paper are all open-released.

For the Ref-inpainting, we find that the widely used irregular mask Dong et al. [2022] fails to reliably evaluate the capability of spatial transformation and structural preserving. Therefore, as shown in Fig. 9 (a), we propose the matching-based masking method. Specifically, we first utilize the scene info provided by MegaDepth Li and Snavely [2018] to select out the image pairs which have an overlap rate between 40% and 70% Second, for each image pair, we use a feature matching model Tang et al. [2022] to detect matching key-points between the images and assign each key-points pair a confidence score. Next, we filter out those pairs with low confidence scores with the threshold of 0.8. Then we randomly crop a 20% to 50% sub-space in the matched region and sample 15 to 30 key points as vertices to be painted across for the final masks. The matching-based mask not only improves the reliability during the evaluation but also facilitates the performance in the training phase as in Tab. 6 .

We split 505 pairs from MegaDepth Li and Snavely [2018] as the validation, including some manual masks from ETH3D scenes Schops et al. [2017] . For the multi-view testing set, we further filter all scenes and retain the ones with at least 4 reference views. Thus there are 482 images in the final multi-view testing set.

For the NVS, we first dilate the object mask and randomly sample points in the enlarged mask bounding box to paint the irregular mask. Then, we unite the dilated object mask to completely cover target images as in Fig. 9 (b). We find that local masking is still very important for fast convergence and stable fine-tuning as empirically verified in experiments. For the data processing on Objaverse Deitke et al. [2022] , Zero123 Liu et al. [2023b] provided images including 800k various scenes with object masks. For each scene, 12 images are rendered in 256 $\times$ 256 with different viewpoints. Following Liu et al. [2023b] , the spherical coordinate system is used to convert the relative pose $\Delta p$ into the polar angle $\theta$ , azimuth angle $\phi$ , and radius $r$ distanced from the canonical center as $\Delta p=(\Delta\theta,\mathrm{sin}\Delta\phi,\mathrm{cos}\Delta\phi,\Delta r)$ , where the azimuth angle is sinusoidally encoded to address the non-continuity. For the masking of Objaverse images, we dilate the object mask and related bounding box with 10 to 25 kernel size and 5% to 20% respectively. Then we randomly sample 20 to 45 points to paint the irregular masks.

We select 500 scenes from Objaverse as the validation, while others are used as the training set. Note that there exists an overlap between our validation and Zero123’s training set Liu et al. [2023b] , but our method still outperforms the official Zero123 as in Tab. 2 .

We show the training details in Tab. 5 . ARCI is efficient in being trained for various tasks. To further demonstrate the effectiveness of ARCI, we provide the training log of ARCI and Zero123 in Fig. 10 . Obviously, the contextual inpainting-based ARCI enjoys a substantially faster convergence and superior performance.

To verify the generalization of our method, we generate more groups of multi-view images through a single input view as in Fig. 18 . Moreover, we test several real-world cases with one RGB input in Fig. 12 . All poses are initialized to [ $0.5\pi,0,1.5$ ] for polar angle, azimuth angle, and radius distance, respectively. The proposed ARCI can be well generalized to real-world cases.

One may ask that the masking strategy used in Fig. 9 (b) suffers from shape leakages, which lead to unreliable metrics in Tab. 2 . We should clarify that our method can perform well only with the reference mask, which is easy to get by the salient object detection Qin et al. [2020] . Specifically, we dilate the reference mask as Fig. 9 (b). Then, a few DDIM steps Song et al. [2020] are used to generate a rough synthesis in the target view. After that, we detect the foreground mask based on the rough synthesis by Qin et al. [2020] and further dilate this mask for the second synthesis with full DDIM steps. The adaptive masking can be well generalized to the NVS as verified in Fig. 11 . All testing results in this paper are already based on adaptive masking. Besides, we think that providing target masks according to the distance and direction priors manually is also convincing to address the challenging single-view NVS.

Matching-based Masks and Noise Coefficient. On the left of Tab. 6 , we find that the matching-based mask enjoys substantial improvement in the reference-guided inpainting. Besides, setting the noise coefficient $\eta=1$ achieves consistent improvements in our ARCI even sampled as the DDIM Song et al. [2020] . So all LDMs are worked under $\eta=1$ without special illustrations.

Prompt Initialization. We tried three initialization ways for the prompt tuning on the right of Tab. 6 . The random initialization performs worst. Both ‘token-wise’ and ‘token-avgs’ leverage text embeddings from a task-specific descriptive sentence that is detailed in the supplementary. ‘Token-wise’ means repeating descriptive sentences until the prompt length, while each token is initialized for one prompt token. ‘Token-avgs’ indicates that all prompt tokens are initialized with the average of the descriptive sentence. Meaningful initialization is useful for task-specific prompt tuning.

Effectiveness of CFG. We remove the pose condition with 15% to train the ARCI for NVS. Then the CFG coefficient 2.5 is used during the inference. As verified in Tab. 7 , CFG could improve the performance with better pose control. Moreover, we find that CFG can also enhance the performance of Ref-inpainting even without training with prompts dropout as in Tab. 8 . The LPIPS first decreases, but then increases as the CFG decreasing from 2.5 to 1.0, while the PSNR and the SSIM keep increasing. We regard LPIPS as the most important metric, which conforms to the human perception. Therefore, we set CFG to 2.0 when testing our model for Ref-inpainting.

We provide more qualitative and quantitative results of Ref-inpainting \footnote{ † Since TransFill Zhou et al. [2021] is not released, we send our images and masks to the authors and take their inpainted results for the evaluation.} in Fig. 14 , Fig. 15 , and Tab. 9 . We further provide qualitatively multi-view Ref-inpainting results in Fig. 16

Besides, we show some diverse NVS on Objaverse Deitke et al. [2022] in Fig. 17 . Different random seeds are utilized to process the DDIM sampling. ARCI can achieve reasonable results with correct target poses. More qualitative results are in Fig. 13 and Fig. 19 .

Comparison on Google Scanned Objects (GSO). We compare the proposed ARCI and zero123 Liu et al. [2023b] on the out-of-distribution GSO dataset Downs et al. [2022] in Tab. 10 and Fig. 20 . ARCI enjoys good zero-shot generalization, which outperforms zero123 with 1-view inputs. More reference views can further improve the quality of ARCI, benefiting from our multi-view-based NVS design and AR training.

We provide the inference speed for different input resolutions in Tab. 11 . All tests are based on one 32GB V100 GPU with 50 DDIM steps. ARCI needs to stitch two images together, which would double the input size. But the inference time is not doubled as shown in Tab. 11 . Note that when the image size is smaller than 512, the difference in inference costs is not obvious. Therefore, we think the proposed ARCI’s inference cost is still acceptable in most real-world applications.

To evaluate the effectiveness of our ARCI in Ref-inpainting. We further test the user study as the human perceptual metric in Fig. 21 . Formally, 50 masked image pairs are randomly selected from our test set which are compared among SD Rombach et al. [2022] , ControlNet Zhang and Agrawala [2023] +match Tang et al. [2022] , Perciver Jaegle et al. [2021] , Paint-by-Example Yang et al. [2023] , TransFill Zhou et al. [2021] , and ARCI. Although TransFill was not open-released, we thank TransFill’s authors for kindly testing these samples for us. There are 10 volunteers who are not familiar with image generation attending this study. Given masked target images and reference ones, we ask volunteers to vote for the best recovery from the 6 competitors mentioned above. The voting criterion should consider both the faithful recovery according to the reference and natural generations of color and texture. As shown in Fig. 21 , ARCI outperforms other competitors.

Although the proposed ARCI enjoys good performance and geometric consistency in multi-view NVS, it still suffers from the drawback of error accumulation as shown in Fig. 22 . To eliminate this problem, we recommend providing a few more views (2,3,4) for more robust geometric priors.

A.1 Broader Impacts

This paper exploited image synthesis with text-to-image models. Because of their impressive generative abilities, these models may produce misinformation or fake images. So we sincerely remind users to pay attention to it. Besides, privacy and consent also become important considerations, as generative models are often trained on large-scale data. Furthermore, generative models may perpetuate biases present in the training data, leading to unfair outcomes. Therefore, we recommend users be responsible and inclusive while using these text-to-image generative models. Note that our method only focuses on technical aspects. Both images and pre-trained models used in this paper are all open-released.

A.2 Data Processing and Implementation Details

For the Ref-inpainting, we find that the widely used irregular mask Dong et al. [2022] fails to reliably evaluate the capability of spatial transformation and structural preserving. Therefore, as shown in Fig. 9 (a), we propose the matching-based masking method. Specifically, we first utilize the scene info provided by MegaDepth Li and Snavely [2018] to select out the image pairs which have an overlap rate between 40% and 70% Second, for each image pair, we use a feature matching model Tang et al. [2022] to detect matching key-points between the images and assign each key-points pair a confidence score. Next, we filter out those pairs with low confidence scores with the threshold of 0.8. Then we randomly crop a 20% to 50% sub-space in the matched region and sample 15 to 30 key points as vertices to be painted across for the final masks. The matching-based mask not only improves the reliability during the evaluation but also facilitates the performance in the training phase as in Tab. 6 .

We split 505 pairs from MegaDepth Li and Snavely [2018] as the validation, including some manual masks from ETH3D scenes Schops et al. [2017] . For the multi-view testing set, we further filter all scenes and retain the ones with at least 4 reference views. Thus there are 482 images in the final multi-view testing set.

For the NVS, we first dilate the object mask and randomly sample points in the enlarged mask bounding box to paint the irregular mask. Then, we unite the dilated object mask to completely cover target images as in Fig. 9 (b). We find that local masking is still very important for fast convergence and stable fine-tuning as empirically verified in experiments. For the data processing on Objaverse Deitke et al. [2022] , Zero123 Liu et al. [2023b] provided images including 800k various scenes with object masks. For each scene, 12 images are rendered in 256 $\times$ 256 with different viewpoints. Following Liu et al. [2023b] , the spherical coordinate system is used to convert the relative pose $\Delta p$ into the polar angle $\theta$ , azimuth angle $\phi$ , and radius $r$ distanced from the canonical center as $\Delta p=(\Delta\theta,\mathrm{sin}\Delta\phi,\mathrm{cos}\Delta\phi,\Delta r)$ , where the azimuth angle is sinusoidally encoded to address the non-continuity. For the masking of Objaverse images, we dilate the object mask and related bounding box with 10 to 25 kernel size and 5% to 20% respectively. Then we randomly sample 20 to 45 points to paint the irregular masks.

We select 500 scenes from Objaverse as the validation, while others are used as the training set. Note that there exists an overlap between our validation and Zero123’s training set Liu et al. [2023b] , but our method still outperforms the official Zero123 as in Tab. 2 .

We show the training details in Tab. 5 . ARCI is efficient in being trained for various tasks. To further demonstrate the effectiveness of ARCI, we provide the training log of ARCI and Zero123 in Fig. 10 . Obviously, the contextual inpainting-based ARCI enjoys a substantially faster convergence and superior performance.

A.2.1 Matching-based Masking and Data Processing for Ref-inpainting

For the Ref-inpainting, we find that the widely used irregular mask Dong et al. [2022] fails to reliably evaluate the capability of spatial transformation and structural preserving. Therefore, as shown in Fig. 9 (a), we propose the matching-based masking method. Specifically, we first utilize the scene info provided by MegaDepth Li and Snavely [2018] to select out the image pairs which have an overlap rate between 40% and 70% Second, for each image pair, we use a feature matching model Tang et al. [2022] to detect matching key-points between the images and assign each key-points pair a confidence score. Next, we filter out those pairs with low confidence scores with the threshold of 0.8. Then we randomly crop a 20% to 50% sub-space in the matched region and sample 15 to 30 key points as vertices to be painted across for the final masks. The matching-based mask not only improves the reliability during the evaluation but also facilitates the performance in the training phase as in Tab. 6 .

We split 505 pairs from MegaDepth Li and Snavely [2018] as the validation, including some manual masks from ETH3D scenes Schops et al. [2017] . For the multi-view testing set, we further filter all scenes and retain the ones with at least 4 reference views. Thus there are 482 images in the final multi-view testing set.

A.2.2 Data Processing for NVS

For the NVS, we first dilate the object mask and randomly sample points in the enlarged mask bounding box to paint the irregular mask. Then, we unite the dilated object mask to completely cover target images as in Fig. 9 (b). We find that local masking is still very important for fast convergence and stable fine-tuning as empirically verified in experiments. For the data processing on Objaverse Deitke et al. [2022] , Zero123 Liu et al. [2023b] provided images including 800k various scenes with object masks. For each scene, 12 images are rendered in 256 $\times$ 256 with different viewpoints. Following Liu et al. [2023b] , the spherical coordinate system is used to convert the relative pose $\Delta p$ into the polar angle $\theta$ , azimuth angle $\phi$ , and radius $r$ distanced from the canonical center as $\Delta p=(\Delta\theta,\mathrm{sin}\Delta\phi,\mathrm{cos}\Delta\phi,\Delta r)$ , where the azimuth angle is sinusoidally encoded to address the non-continuity. For the masking of Objaverse images, we dilate the object mask and related bounding box with 10 to 25 kernel size and 5% to 20% respectively. Then we randomly sample 20 to 45 points to paint the irregular masks.

We select 500 scenes from Objaverse as the validation, while others are used as the training set. Note that there exists an overlap between our validation and Zero123’s training set Liu et al. [2023b] , but our method still outperforms the official Zero123 as in Tab. 2 .

A.2.3 Training Details

We show the training details in Tab. 5 . ARCI is efficient in being trained for various tasks. To further demonstrate the effectiveness of ARCI, we provide the training log of ARCI and Zero123 in Fig. 10 . Obviously, the contextual inpainting-based ARCI enjoys a substantially faster convergence and superior performance.

A.3 Autoregressively Sequential Generation

To verify the generalization of our method, we generate more groups of multi-view images through a single input view as in Fig. 18 . Moreover, we test several real-world cases with one RGB input in Fig. 12 . All poses are initialized to [ $0.5\pi,0,1.5$ ] for polar angle, azimuth angle, and radius distance, respectively. The proposed ARCI can be well generalized to real-world cases.

A.3.1 Adaptive Masking

One may ask that the masking strategy used in Fig. 9 (b) suffers from shape leakages, which lead to unreliable metrics in Tab. 2 . We should clarify that our method can perform well only with the reference mask, which is easy to get by the salient object detection Qin et al. [2020] . Specifically, we dilate the reference mask as Fig. 9 (b). Then, a few DDIM steps Song et al. [2020] are used to generate a rough synthesis in the target view. After that, we detect the foreground mask based on the rough synthesis by Qin et al. [2020] and further dilate this mask for the second synthesis with full DDIM steps. The adaptive masking can be well generalized to the NVS as verified in Fig. 11 . All testing results in this paper are already based on adaptive masking. Besides, we think that providing target masks according to the distance and direction priors manually is also convincing to address the challenging single-view NVS.

A.4 Supplemental Experimental Results

Matching-based Masks and Noise Coefficient. On the left of Tab. 6 , we find that the matching-based mask enjoys substantial improvement in the reference-guided inpainting. Besides, setting the noise coefficient $\eta=1$ achieves consistent improvements in our ARCI even sampled as the DDIM Song et al. [2020] . So all LDMs are worked under $\eta=1$ without special illustrations.

Prompt Initialization. We tried three initialization ways for the prompt tuning on the right of Tab. 6 . The random initialization performs worst. Both ‘token-wise’ and ‘token-avgs’ leverage text embeddings from a task-specific descriptive sentence that is detailed in the supplementary. ‘Token-wise’ means repeating descriptive sentences until the prompt length, while each token is initialized for one prompt token. ‘Token-avgs’ indicates that all prompt tokens are initialized with the average of the descriptive sentence. Meaningful initialization is useful for task-specific prompt tuning.

Effectiveness of CFG. We remove the pose condition with 15% to train the ARCI for NVS. Then the CFG coefficient 2.5 is used during the inference. As verified in Tab. 7 , CFG could improve the performance with better pose control. Moreover, we find that CFG can also enhance the performance of Ref-inpainting even without training with prompts dropout as in Tab. 8 . The LPIPS first decreases, but then increases as the CFG decreasing from 2.5 to 1.0, while the PSNR and the SSIM keep increasing. We regard LPIPS as the most important metric, which conforms to the human perception. Therefore, we set CFG to 2.0 when testing our model for Ref-inpainting.

We provide more qualitative and quantitative results of Ref-inpainting \footnote{ † Since TransFill Zhou et al. [2021] is not released, we send our images and masks to the authors and take their inpainted results for the evaluation.} in Fig. 14 , Fig. 15 , and Tab. 9 . We further provide qualitatively multi-view Ref-inpainting results in Fig. 16

Besides, we show some diverse NVS on Objaverse Deitke et al. [2022] in Fig. 17 . Different random seeds are utilized to process the DDIM sampling. ARCI can achieve reasonable results with correct target poses. More qualitative results are in Fig. 13 and Fig. 19 .

Comparison on Google Scanned Objects (GSO). We compare the proposed ARCI and zero123 Liu et al. [2023b] on the out-of-distribution GSO dataset Downs et al. [2022] in Tab. 10 and Fig. 20 . ARCI enjoys good zero-shot generalization, which outperforms zero123 with 1-view inputs. More reference views can further improve the quality of ARCI, benefiting from our multi-view-based NVS design and AR training.

A.4.1 Supplemental Ablation Studies

Matching-based Masks and Noise Coefficient. On the left of Tab. 6 , we find that the matching-based mask enjoys substantial improvement in the reference-guided inpainting. Besides, setting the noise coefficient $\eta=1$ achieves consistent improvements in our ARCI even sampled as the DDIM Song et al. [2020] . So all LDMs are worked under $\eta=1$ without special illustrations.

Prompt Initialization. We tried three initialization ways for the prompt tuning on the right of Tab. 6 . The random initialization performs worst. Both ‘token-wise’ and ‘token-avgs’ leverage text embeddings from a task-specific descriptive sentence that is detailed in the supplementary. ‘Token-wise’ means repeating descriptive sentences until the prompt length, while each token is initialized for one prompt token. ‘Token-avgs’ indicates that all prompt tokens are initialized with the average of the descriptive sentence. Meaningful initialization is useful for task-specific prompt tuning.

Effectiveness of CFG. We remove the pose condition with 15% to train the ARCI for NVS. Then the CFG coefficient 2.5 is used during the inference. As verified in Tab. 7 , CFG could improve the performance with better pose control. Moreover, we find that CFG can also enhance the performance of Ref-inpainting even without training with prompts dropout as in Tab. 8 . The LPIPS first decreases, but then increases as the CFG decreasing from 2.5 to 1.0, while the PSNR and the SSIM keep increasing. We regard LPIPS as the most important metric, which conforms to the human perception. Therefore, we set CFG to 2.0 when testing our model for Ref-inpainting.

A.4.2 Results of Ref-inpainting

We provide more qualitative and quantitative results of Ref-inpainting \footnote{ † Since TransFill Zhou et al. [2021] is not released, we send our images and masks to the authors and take their inpainted results for the evaluation.} in Fig. 14 , Fig. 15 , and Tab. 9 . We further provide qualitatively multi-view Ref-inpainting results in Fig. 16

A.4.3 Results of NVS

Besides, we show some diverse NVS on Objaverse Deitke et al. [2022] in Fig. 17 . Different random seeds are utilized to process the DDIM sampling. ARCI can achieve reasonable results with correct target poses. More qualitative results are in Fig. 13 and Fig. 19 .

Comparison on Google Scanned Objects (GSO). We compare the proposed ARCI and zero123 Liu et al. [2023b] on the out-of-distribution GSO dataset Downs et al. [2022] in Tab. 10 and Fig. 20 . ARCI enjoys good zero-shot generalization, which outperforms zero123 with 1-view inputs. More reference views can further improve the quality of ARCI, benefiting from our multi-view-based NVS design and AR training.

A.5 Inference Speed

We provide the inference speed for different input resolutions in Tab. 11 . All tests are based on one 32GB V100 GPU with 50 DDIM steps. ARCI needs to stitch two images together, which would double the input size. But the inference time is not doubled as shown in Tab. 11 . Note that when the image size is smaller than 512, the difference in inference costs is not obvious. Therefore, we think the proposed ARCI’s inference cost is still acceptable in most real-world applications.

A.6 User Study

To evaluate the effectiveness of our ARCI in Ref-inpainting. We further test the user study as the human perceptual metric in Fig. 21 . Formally, 50 masked image pairs are randomly selected from our test set which are compared among SD Rombach et al. [2022] , ControlNet Zhang and Agrawala [2023] +match Tang et al. [2022] , Perciver Jaegle et al. [2021] , Paint-by-Example Yang et al. [2023] , TransFill Zhou et al. [2021] , and ARCI. Although TransFill was not open-released, we thank TransFill’s authors for kindly testing these samples for us. There are 10 volunteers who are not familiar with image generation attending this study. Given masked target images and reference ones, we ask volunteers to vote for the best recovery from the 6 competitors mentioned above. The voting criterion should consider both the faithful recovery according to the reference and natural generations of color and texture. As shown in Fig. 21 , ARCI outperforms other competitors.

A.7 Limitation

Although the proposed ARCI enjoys good performance and geometric consistency in multi-view NVS, it still suffers from the drawback of error accumulation as shown in Fig. 22 . To eliminate this problem, we recommend providing a few more views (2,3,4) for more robust geometric priors.

References

Avrahami et al. [2022] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18208–18218, 2022.
Bar-Tal et al. [2023] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. arXiv preprint arXiv:2302.08113, 2, 2023.
Bertalmio et al. [2000] Marcelo Bertalmio, Guillermo Sapiro, Vincent Caselles, and Coloma Ballester. Image inpainting. InProceedings of the 27th annual conference on Computer graphics and interactive techniques, pages 417–424, 2000.
Chan et al. [2022] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16123–16133, 2022.
Chan et al. [2023] Eric R Chan, Koki Nagano, Matthew A Chan, Alexander W Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini De Mello, Tero Karras, and Gordon Wetzstein. Generative novel view synthesis with 3d-aware diffusion models. arXiv preprint arXiv:2304.02602, 2023.
Chen et al. [2019] Wenzheng Chen, Huan Ling, Jun Gao, Edward Smith, Jaakko Lehtinen, Alec Jacobson, and Sanja Fidler. Learning to predict 3d objects with an interpolation-based differentiable renderer. Advances in neural information processing systems, 32, 2019.
Couairon et al. [2022] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based semantic image editing with mask guidance. arXiv preprint arXiv:2210.11427, 2022.
Criminisi et al. [2003] Antonio Criminisi, Patrick Perez, and Kentaro Toyama. Object removal by exemplar-based inpainting. In2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings., pages II–II. IEEE, 2003.
Deitke et al. [2022] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3d objects. arXiv preprint arXiv:2212.08051, 2022.
Dong et al. [2022] Qiaole Dong, Chenjie Cao, and Yanwei Fu. Incremental transformer structure enhanced image inpainting with masking positional encoding. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11358–11368, 2022.
Downs et al. [2022] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas B McHugh, and Vincent Vanhoucke. Google scanned objects: A high-quality dataset of 3d scanned household items. In2022 International Conference on Robotics and Automation (ICRA), pages 2553–2560. IEEE, 2022.
Esser et al. [2021] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12873–12883, 2021.
Fahim et al. [2021] George Fahim, Khalid Amin, and Sameh Zarif. Single-view 3d reconstruction: A survey of deep learning methods. Computers & Graphics, 94:164–190, 2021.
Gal et al. [2022] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022.
Ge et al. [2022] Chunjiang Ge, Rui Huang, Mixue Xie, Zihang Lai, Shiji Song, Shuang Li, and Gao Huang. Domain adaptation via prompt learning. arXiv preprint arXiv:2202.06687, 2022.
Hays and Efros [2007] James Hays and Alexei A Efros. Scene completion using millions of photographs. ACM Transactions on Graphics (ToG), 26(3):4–es, 2007.
Hertz et al. [2022] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022.
Ho and Salimans [2022] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.
Houlsby et al. [2019] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. InInternational Conference on Machine Learning, pages 2790–2799. PMLR, 2019.
Hu et al. [2021] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.
Jaegle et al. [2021] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. InInternational conference on machine learning, pages 4651–4664. PMLR, 2021.
Jia et al. [2022] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. InComputer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXIII, pages 709–727. Springer, 2022.
Lester et al. [2021] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.
Li et al. [2022] Wenbo Li, Zhe Lin, Kun Zhou, Lu Qi, Yi Wang, and Jiaya Jia. Mat: Mask-aware transformer for large hole image inpainting. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10758–10768, 2022.
Li et al. [2023] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. arXiv preprint arXiv:2301.07093, 2023.
Li and Snavely [2018] Zhengqi Li and Noah Snavely. Megadepth: Learning single-view depth prediction from internet photos. InProceedings of the IEEE conference on computer vision and pattern recognition, pages 2041–2050, 2018.
Liao et al. [2023] Ning Liao, Bowen Shi, Min Cao, Xiaopeng Zhang, Qi Tian, and Junchi Yan. Rethinking visual prompt learning as masked visual token modeling. arXiv preprint arXiv:2303.04998, 2023.
Lin et al. [2023] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 300–309, 2023.
Liu et al. [2023a] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1–35, 2023a.
Liu et al. [2023b] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. InProceedings of the IEEE/CVF international conference on computer vision, 2023b.
Liu et al. [2019] Shichen Liu, Tianye Li, Weikai Chen, and Hao Li. Soft rasterizer: A differentiable renderer for image-based 3d reasoning. InProceedings of the IEEE/CVF International Conference on Computer Vision, pages 7708–7717, 2019.
Liu et al. [2021a] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602, 2021a.
Liu et al. [2021b] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt understands, too. arXiv preprint arXiv:2103.10385, 2021b.
Ma et al. [2023] Yiyang Ma, Huan Yang, Wenjing Wang, Jianlong Fu, and Jiaying Liu. Unified multi-modal latent diffusion for joint subject and text conditional image generation. arXiv preprint arXiv:2303.09319, 2023.
Mokady et al. [2022] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. arXiv preprint arXiv:2211.09794, 2022.
Mou et al. [2023] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv preprint arXiv:2302.08453, 2023.
Niemeyer and Geiger [2021] Michael Niemeyer and Andreas Geiger. Giraffe: Representing scenes as compositional generative neural feature fields. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11453–11464, 2021.
Niklaus et al. [2019] Simon Niklaus, Long Mai, Jimei Yang, and Feng Liu. 3d ken burns effect from a single image. ACM Transactions on Graphics (ToG), 38(6):1–15, 2019.
Oh et al. [2019] Seoung Wug Oh, Sungho Lee, Joon-Young Lee, and Seon Joo Kim. Onion-peel networks for deep video completion. Inproceedings of the IEEE/cvf international conference on computer vision, pages 4403–4412, 2019.
Poole et al. [2023] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. InInternational Conference on Learning Representations (ICLR), 2023.
Qian et al. [2023] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843, 2023.
Qin et al. [2020] Xuebin Qin, Zichen Zhang, Chenyang Huang, Masood Dehghan, Osmar R Zaiane, and Martin Jagersand. U2-net: Going deeper with nested u-structure for salient object detection. Pattern recognition, 106:107404, 2020.
Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. InInternational conference on machine learning, pages 8748–8763. PMLR, 2021.
Rombach et al. [2021] Robin Rombach, Patrick Esser, and Björn Ommer. Geometry-free view synthesis: Transformers and no 3d priors. InProceedings of the IEEE/CVF International Conference on Computer Vision, pages 14356–14366, 2021.
Rombach et al. [2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684–10695, 2022.
Ruiz et al. [2022] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arXiv:2208.12242, 2022.
Salimans et al. [2017] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. InInternational Conference on Learning Representations (ICLR), 2017.
Schops et al. [2017] Thomas Schops, Johannes L Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas Geiger. A multi-view stereo benchmark with high-resolution images and multi-camera videos. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3260–3269, 2017.
Schwarz et al. [2020] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. Graf: Generative radiance fields for 3d-aware image synthesis. Advances in Neural Information Processing Systems, 33:20154–20166, 2020.
Shih et al. [2020] Meng-Li Shih, Shih-Yang Su, Johannes Kopf, and Jia-Bin Huang. 3d photography using context-aware layered depth inpainting. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8028–8038, 2020.
Sohn et al. [2022] Kihyuk Sohn, Yuan Hao, José Lezama, Luisa Polania, Huiwen Chang, Han Zhang, Irfan Essa, and Lu Jiang. Visual prompt tuning for generative transfer learning. arXiv preprint arXiv:2210.00990, 2022.
Song et al. [2020] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.
Suvorov et al. [2022] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust large mask inpainting with fourier convolutions. InProceedings of the IEEE/CVF winter conference on applications of computer vision, pages 2149–2159, 2022.
Tang et al. [2023] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity 3d creation from a single image with diffusion prior. arXiv preprint arXiv:2303.14184, 2023.
Tang et al. [2022] Shitao Tang, Jiahui Zhang, Siyu Zhu, and Ping Tan. Quadtree attention for vision transformers. arXiv preprint arXiv:2201.02767, 2022.
Van den Oord et al. [2016] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. Advances in neural information processing systems, 29, 2016.
Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.
Voynov et al. [2022] Andrey Voynov, Kfir Aberman, and Daniel Cohen-Or. Sketch-guided text-to-image diffusion models. arXiv preprint arXiv:2211.13752, 2022.
Wang et al. [2023] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12619–12629, 2023.
Wang et al. [2018] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, and Yu-Gang Jiang. Pixel2mesh: Generating 3d mesh models from single rgb images. InProceedings of the European conference on computer vision (ECCV), pages 52–67, 2018.
Wu et al. [2017] Jiajun Wu, Yifan Wang, Tianfan Xue, Xingyuan Sun, Bill Freeman, and Josh Tenenbaum. Marrnet: 3d shape reconstruction via 2.5 d sketches. InAdvances in neural information processing systems, 2017.
Xu et al. [2019] Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir Mech, and Ulrich Neumann. Disn: Deep implicit surface network for high-quality single-view 3d reconstruction. InAdvances in neural information processing systems, 2019.
Yang et al. [2023] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18381–18391, 2023.
Yu et al. [2018] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang. Generative image inpainting with contextual attention. InProceedings of the IEEE conference on computer vision and pattern recognition, pages 5505–5514, 2018.
Zeng et al. [2020] Yu Zeng, Zhe Lin, Jimei Yang, Jianming Zhang, Eli Shechtman, and Huchuan Lu. High-resolution image inpainting with iterative confidence feedback and guided upsampling. InComputer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XIX 16, pages 1–17. Springer, 2020.
Zhang and Agrawala [2023] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. InProceedings of the IEEE/CVF international conference on computer vision, 2023.
Zhao et al. [2022a] Liang Zhao, Xinyuan Zhao, Hailong Ma, Xinyu Zhang, and Long Zeng. 3dfill: Reference-guided image inpainting by self-supervised 3d image alignment. arXiv preprint arXiv:2211.04831, 2022a.
Zhao et al. [2021] Shengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao Liang, Eric I Chang, and Yan Xu. Large scale image completion via co-modulated generative adversarial networks. arXiv preprint arXiv:2103.10428, 2021.
Zhao et al. [2022b] Yunhan Zhao, Connelly Barnes, Yuqian Zhou, Eli Shechtman, Sohrab Amirghodsi, and Charless Fowlkes. Geofill: Reference-based image inpainting of scenes with complex geometry. arXiv preprint arXiv:2201.08131, 2022b.
Zhou et al. [2021] Yuqian Zhou, Connelly Barnes, Eli Shechtman, and Sohrab Amirghodsi. Transfill: Reference-guided image inpainting by merging multiple color and spatial transformations. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2266–2276, 2021.
