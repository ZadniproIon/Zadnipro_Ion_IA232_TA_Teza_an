The Devil is the Classifier: Investigating Long Tail Relation Classification with Decoupling Analysis

By Haiyang Yu and Ningyu Zhang and Shumin Deng and Zonggang Yuan and Yantao Jia and Huajun Chen

Abstract

Long-tailed relation classification is a challenging problem as the head classes may dominate the training phase, thereby leading to the deterioration of the tail performance. Existing solutions usually address this issue via class-balancing strategies, e.g., data re-sampling and loss re-weighting, but all these methods adhere to the schema of entangling learning of the representation and classifier . In this study, we conduct an in-depth empirical investigation into the long-tailed problem and found that pre-trained models with instance-balanced sampling already capture the well-learned representations for all classes ; moreover, it is possible to achieve better long-tailed classification ability at low cost by only adjusting the classifier . Inspired by this observation, we propose a robust classifier with attentive relation routing , which assigns soft weights by automatically aggregating the relations. Extensive experiments on two datasets demonstrate the effectiveness of our proposed approach. Code and datasets are available in https://github.com/zjunlp/deepke .

1 Introduction

Relation classification (RC) is a fundamental task in natural language processing (NLP) for constructing knowledge graphs and downstream-related tasks, such as question answering Jin et al. (2019) and information retrieval Zamani et al. (2020) . Most approaches for relation extraction Zhang et al. (2018) primarily focus on frequently seen relations. However, in more practical scenarios, large numbers of samples with long-tailed distributions of the relation frequencies are inevitable. Expressly, nearly 70% of the relations (NA excluded) in the widely used TACRED Zhang et al. (2017) dataset have long-tailed characteristics. Therefore, it is crucial for the models to be able to extract such long-tailed relations.

For the long-tailed data, a commonly noted issue is that the head classes dominate the training stage, thereby leading to the deterioration of performance on the tail. To address this issue, one feasible solution is to learn a better representation Zhang et al. (2019a) via re-sampling strategies or transfer learning for long-tailed relations. Another solution is to design specific loss functions that better facilitate learning with long-tailed data to acquire a robust classifier for the tail.

Recently proposed methods Wang et al. (2017) have generally studied the problem of extracting long-tailed relations via jointly learning representation and classifier. However, the mechanisms behind such jointly schema are not thoroughly understood, thus making it unclear how the long-tailed classification ability is achieved– is it from learning a better representation or by handling the discriminability better via robust classifier decision boundaries? Note that pre-trained language models such as BERT Devlin et al. (2018) have the ability to represent various texts robustly; hence, we can argue whether a well-performing classifier is more critical for the long-tailed problem. To answer the above questions, in this study, we conducted an empirical investigation on long-tailed RC.

To this end, we first conduct experiments (§ 3 ) to identify the working mechanism of representation and classifier learning. We observe that previous re-weighting operations achieve performance gains, while the re-sampling approaches slightly deteriorate the performance when retrained with fixed encoder; this necessitated that classifier is more important than representation learning for the long-tailed problem . Hence, we further introduce an effective classifier, namely attention relation routing . We argue that previous approaches Joulin et al. (2016) assigns hard weights, which are suboptimal; therefore, we propose a soft weighting mechanism for classifier training with dynamic routing Sabour et al. (2017) (§ 4 ). Experimental results on Fewrel-LT and TACRED demonstrate the efficacy of our proposed approach. Our key contributions can be summarized as follows:

• We empirically investigate long-tailed relation extraction and observe that classifier is more important than representation learning. • We introduce attentive relation routing to assign soft weights for strong classifier learning. • We report the empirical results on two datasets and release a long-tailed RC testbed.

2 Background

There are two main methods by which the long-tailed issue can be addressed Buda et al. (2018) . The first involves re-balancing strategies , which are intuitive methods to solve the problem of long-tailed distributions. Concerning these, Joulin et al. (2016) proposed the class-balanced sampling (CBS) approach to sample instances for each class. Other sampling methods, such as square-root sampling (SRS) Mahajan et al. (2018) , progressively balanced sampling (PBS) Kang et al. (2020) , mixed class and instance distributions, are also introduced. The second type of solution involves re-weighting methods , including re-weight loss (RWL) Ronneberger et al. (2015) , focal loss (FCL) Lin et al. (2017) , and dice loss (DSL) Milletari et al. (2016) , which assign weights to different training samples for each class to boost the discriminability via robust classifier decision boundaries. More recently, Zhou et al. (2019) proposed a unified bilateral-branch network (BBN) to comprehensively consider both representation learning and classifier learning for long-tailed recognition. In Kang et al. (2020) , the authors introduced a $\tau$ -norm approach to re-balance the classifiers’ decision boundaries. Besides, Wang et al. (2017) propose to transfer knowledge from -head to -tail. However, most of the approaches train the classifiers entangled with the representations, thus rendering the essence of the mode abilities not to be understood adequately.

3 Empirical Decoupling Analysis

In this section, we present to investigate the working mechanism for representation learning and classifier training by empirical decoupling analysis. We divide the model into two parts: the feature extractor (i.e., backbone networks such as BERT) and the classifier (i.e., last fully connected layers).

3.1 Representation & Classifier

Since BERT Devlin et al. (2018) has been proved to be powerful and effective in many NLP tasks, we first preprocess the sentence $\mathbf{x}=$ $\{w_{1},$ $w_{2},$ $e_{1},$ $\dots,$ $e_{2}$ $w_{n}\}$ to BERT’s input form: $\mathbf{x}=$ $\{$ [CLS] , $w_{1},$ $w_{2},$ [E1], $e_{1},$ [/E1], $\dots,$ [E2], $e_{2},$ [/E2], $w_{n},$ [SEP] $\}$ , where $w_{i},i\in[1,n]$ refers to each word in a sentence; $e_{1}$ and $e_{2}$ are two labeled entities; [E1], [/E1], [E2], and [/E2] are four special tokens used to mark the positions of the entities. Note that we are aimed to investigate the representation and classifier, we utilize the simplest way, namely, the [CLS] token, as the sentence feature representation: $\mathbf{x}=[CLS]$ , $x\in\mathbb{R}^{d_{bert}}$ , and $d_{bert}$ is the predefined output size in BERT.

Generally, the BERT output is followed by a fully connected layer to match sample features with relation categories that need to be classified. Other kinds of classifiers can also be leveraged. We utilize the softmax function to predict relation logits as follows:


 
 $\mathbf{y}_{pred}=softmax(\mathbf{W}\mathbf{x}+b)$  (1) 


3.2 Decoupling Procedure

To decouple the representation learning and classifier training, we conduct a two-stage training process on the FewRel-LT dataset \footnote{ 1 The construction process of the FewRel-LT dataset is described in § 5.1 .} , In the first stage , we train three models with instance-balanced sampling (vanilla setting, IBS ), class-balanced sampling ( CBS ), and re-weighted loss ( RWL ) strategies. In the second stage , we fix the parameters of the feature extractors and retrain the classifiers from scratch with the learning mentioned above methods. In principle, we design these experiments to fairly compare the quality of the representations and classifiers learned via different schema by following the control variates method.

From Figure 1 , we empirically observe that 1) when we apply the same representation learning technique, it is observed that the RWL/CBS always achieve better performance than the IBS, which is attributable to their re-balancing operations adjusting the classifier weights and updating them to match the test distributions; 2) when applying the same classifier learning scheme, it can be seen that the F1 score of IBS is consistently higher than that of RWL/CBS. The worst results of the RWL/CBS method reveal that they lead to inferior discriminability of the deep learned features. These empirical observations indicate that the previous performance gains are mainly based on enhancing the discriminability , which indicates that a robust classifier is essential for the long-tailed problem.

4 Classifier for Long-Tailed RC

Motivated by the observations mentioned above, we believe that a more effective classifier can boost performance. Note that the previous approaches (e.g., focal loss Lin et al. (2017) and dice loss Milletari et al. (2016) ) assign hard weights to adjust and update the classifier to match the test distributions. Inspired by the dynamic routing scheme Sabour et al. (2017) , which is able to assign soft weights via route-by-agreement, we introduce a simple yet effective approach, namely attentive relation routing (ARR ), to learn the classifier weights.

4.1 Attentive Relation Routing

Specifically, given a sentence representation $\mathbf{x}$ , we transform it into the primary capsules ( $\bm{\Omega}^{p}$ ), where $\bm{\Omega}^{p}$ $\in$ $\mathbb{R}^{a\times m}$ , ${a}$ is the number of primary capsules, and ${m}$ is the hidden size of the capsule. The overall routing can be summarized as \footnote{ 2 More details are provided in the supplementary materials} follows: 
 
 $\mathbf{y}_{pred}=\textbf{Attentive\_Routing}(\mathbf{x}\mapsto\bm{\Omega}^{p})$  (2) 


The routing procedure consists of multiple iterations, and each iteration process consists of two steps. The first step computes the agreement coefficient between the primary capsules ( $\bm{\Omega}^{p}$ ) and relation capsules ( $\bm{\Omega}^{r}$ ), and the second step updates the relation capsules by the calculated routing coefficient. Here, $\bm{\Omega}^{r}$ $\in$ $\mathbb{R}^{b\times n}$ , $b$ is the number of relation capsules, and $n$ is the capsule dimension.

We first transform $\bm{\Omega}^{p}$ to the vote ( $\mathcal{V}$ ) using a learned transformation matrix: $\mathcal{V}=\mathsf{W}\cdot\bm{\Omega}^{p}$ , where the matrix $\mathsf{W}$ $\in$ $\mathbb{R}^{b\times a}$ . Next, the agreement ( ${\alpha}_{mn}$ ) is computed by the dot-product similarity between the relation capsules and vote: ${\alpha}_{mn}={\bm{\Omega}^{r}}^{\top}\cdot\mathcal{V}$ , After that we can get routng coefficient: $r_{mn}=softmax({\alpha}_{mn})$ . Inspired by Tsai et al. (2020) , we updated $\bm{\Omega}^{r}$ with layer normalization rather than the squash function as follows: $\bm{\Omega}^{r}=LayerNorm(\sum r_{mn}\mathcal{V})$ . After several routing iterations, we use a single linear transformation to squeeze each relation capsule into a scalar and generate the final prediction result as follows: $\mathbf{y}_{pred}=softmax(\mathsf{W}_{r}\bm{\Omega}^{r}+b)$ , where $\mathsf{W}_{r}$ $\in$ $\mathbb{R}^{n\times 1}$ .

5 Experiments

We performed experiments on two long-tailed relation classification datasets: FewRel Han et al. (2018) and TACRED Zhang et al. (2017) . For TACRED, we used the original dataset. However, for FewRel, we randomly selected 50 classes and reconstructed a long-tailed dataset, FewRel-LT, by reducing the number of training samples per class according to an exponential function $n_{i}=n_{max}\eta^{i}$ , where $i$ is the class index, $n_{max}$ are the training samples in the largest class (index 0), and $\eta$ is the imbalance ratio defined as dividing the maximum number of the class by the minimum. We take $n_{max}$ equal to 500, $\eta$ equal to 100. For the validation and test datasets, we select 100 samples for each class. Additional dataset statistics are shown in Table 1 .

We utilize bert-base-uncased as the representation from Wolf et al. (2019) . We employed Adam Kingma and Ba (2015) as the optimizer, the initial learning rate $\alpha$ is set to 0.01 (the learning rate of the classifier, more details in supplementary materials), and we reduce the rate by 20% every 8 epochs. The coefficient for the regularization term $\lambda$ is 5e-4, the batch size is 64, and the total number of epochs is 50. We evaluate the performance of FewRel-LT with macro F1 score and TACRED with offical micro F1 score.

We compare our attentive relation routing (ARR) with extensive baselines (We do not compare with the transfer learning baseline Zhang et al. (2019b) as it utilize external class hierarchy.). From Table 2 , we observe that , data re-sampling strategies such as CBS Joulin et al. (2016) and SRS Mahajan et al. (2018) obtain about 1.5% improvement compared with vanilla BERT; meanwhile, PBS Kang et al. (2020) which smoothly transitioned from IBS to CBS, achieves about 2% improvement. We also observe that re-weighting methods, including RWL Ronneberger et al. (2015) , FCL Lin et al. (2017) , and DSL Milletari et al. (2016) , all achieve better performance compared with vanilla BERT. Further, we find that BBN Zhou et al. (2019) and $\tau$ -norm Kang et al. (2020) both achieve better improvements. Our ARR achieve 5% and 3% improvements on each dataset.

To better analyze the performance variations across classes with different numbers of samples, we further report the F1 score on subsets of the FewRel-LT: Head (more than 100 samples), Medium (20 $\sim$ 100 samples), and Tail (less than 20 samples). To prevent the leakage of entity information, we exclude samples with shared entity pairs . From Figure 2 , we observe that the data re-sampling and loss re-weighting methods improve the tail performance, but deteriorate the performance of the head. However, our ARR method improves the tail score without reducing head performance, which indicates that our approach with dynamic weights is able to enhance the discriminability of the tail without reducing the robustness of the head.

5.1 Datasets & Settings

We performed experiments on two long-tailed relation classification datasets: FewRel Han et al. (2018) and TACRED Zhang et al. (2017) . For TACRED, we used the original dataset. However, for FewRel, we randomly selected 50 classes and reconstructed a long-tailed dataset, FewRel-LT, by reducing the number of training samples per class according to an exponential function $n_{i}=n_{max}\eta^{i}$ , where $i$ is the class index, $n_{max}$ are the training samples in the largest class (index 0), and $\eta$ is the imbalance ratio defined as dividing the maximum number of the class by the minimum. We take $n_{max}$ equal to 500, $\eta$ equal to 100. For the validation and test datasets, we select 100 samples for each class. Additional dataset statistics are shown in Table 1 .

We utilize bert-base-uncased as the representation from Wolf et al. (2019) . We employed Adam Kingma and Ba (2015) as the optimizer, the initial learning rate $\alpha$ is set to 0.01 (the learning rate of the classifier, more details in supplementary materials), and we reduce the rate by 20% every 8 epochs. The coefficient for the regularization term $\lambda$ is 5e-4, the batch size is 64, and the total number of epochs is 50. We evaluate the performance of FewRel-LT with macro F1 score and TACRED with offical micro F1 score.

5.2 Main Results

We compare our attentive relation routing (ARR) with extensive baselines (We do not compare with the transfer learning baseline Zhang et al. (2019b) as it utilize external class hierarchy.). From Table 2 , we observe that , data re-sampling strategies such as CBS Joulin et al. (2016) and SRS Mahajan et al. (2018) obtain about 1.5% improvement compared with vanilla BERT; meanwhile, PBS Kang et al. (2020) which smoothly transitioned from IBS to CBS, achieves about 2% improvement. We also observe that re-weighting methods, including RWL Ronneberger et al. (2015) , FCL Lin et al. (2017) , and DSL Milletari et al. (2016) , all achieve better performance compared with vanilla BERT. Further, we find that BBN Zhou et al. (2019) and $\tau$ -norm Kang et al. (2020) both achieve better improvements. Our ARR achieve 5% and 3% improvements on each dataset.

5.3 Further Analysis

To better analyze the performance variations across classes with different numbers of samples, we further report the F1 score on subsets of the FewRel-LT: Head (more than 100 samples), Medium (20 $\sim$ 100 samples), and Tail (less than 20 samples). To prevent the leakage of entity information, we exclude samples with shared entity pairs . From Figure 2 , we observe that the data re-sampling and loss re-weighting methods improve the tail performance, but deteriorate the performance of the head. However, our ARR method improves the tail score without reducing head performance, which indicates that our approach with dynamic weights is able to enhance the discriminability of the tail without reducing the robustness of the head.

6 Conclusion and Future Work

We study the problem of long-tailed relation classification and develop a preliminary step towards decoupling representation and classifier learning. We empirically observed that with pre-trained language models, a good classifier was the most important requirement for long-tailed classification, which might shed light on future works with long-tailed problems. We further introduce a robust classifier with attentive relation routing. Extensive experiments on two benchmark datasets demonstrate the effectiveness of our approach. We anticipate further research on promising directions, including 1) exploiting a more effective classifier to boost long-tailed discriminability, 2) distinguishing task-specific representation, and classifier automatically via neural architecture search.

References

Buda et al. (2018) Mateusz Buda, Atsuto Maki, and Maciej A. Mazurowski. 2018. A systematic study of the class imbalance problem in convolutional neural networks. Neural networks : the official journal of the International Neural Network Society, 106:249–259.
Cui et al. (2019) Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. 2019. Class-balanced loss based on effective number of samples. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9268–9277.
Devlin et al. (2018) Jacob Devlin, Mingwei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv: Computation and Language.
Han et al. (2018) Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, and Maosong Sun. 2018. Fewrel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation. ArXiv, abs/1810.10147.
Jin et al. (2019) Hai Jin, Yi Luo, Chenjing Gao, Xunzhu Tang, and Pingpeng Yuan. 2019. Comqa: Question answering over knowledge base via semantic matching. IEEE Access, 7:75235–75246.
Joulin et al. (2016) Armand Joulin, Laurens van der Maaten, Allan Jabri, and Nicolas Vasilache. 2016. Learning visual features from large weakly supervised data. InECCV.
Kang et al. (2020) Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis Kalantidis. 2020. Decoupling representation and classifier for long-tailed recognition. ArXiv, abs/1910.09217.
Kingma and Ba (2015) Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. CoRR, abs/1412.6980.
Li et al. (2019) Xiaoya Li, Xiaofei Sun, Yuxian Meng, Junjun Liang, Fei Wu, and Jiwei Li. 2019. Dice loss for data-imbalanced nlp tasks. ArXiv, abs/1911.02855.
Lin et al. (2017) Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Dollár. 2017. Focal loss for dense object detection. 2017 IEEE International Conference on Computer Vision (ICCV), pages 2999–3007.
Mahajan et al. (2018) Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten. 2018. Exploring the limits of weakly supervised pretraining. InProceedings of the European Conference on Computer Vision (ECCV), pages 181–196.
Milletari et al. (2016) Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. 2016. V-net: Fully convolutional neural networks for volumetric medical image segmentation. 2016 Fourth International Conference on 3D Vision (3DV), pages 565–571.
Nan et al. (2020) Guoshun Nan, Zhijiang Guo, Ivan Sekulić, and Wei Lu. 2020. Reasoning with latent structure refinement for document-level relation extraction. InIn ACL.
Ronneberger et al. (2015) Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional networks for biomedical image segmentation. ArXiv, abs/1505.04597.
Sabour et al. (2017) Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. 2017. Dynamic routing between capsules. arXiv: Computer Vision and Pattern Recognition.
Tsai et al. (2020) Yaohung Hubert Tsai, Nitish Srivastava, Hanlin Goh, and Ruslan Salakhutdinov. 2020. Capsules with inverted dot-product attention routing. arXiv: Learning.
Wang et al. (2017) Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. 2017. Learning to model the tail. InAdvances in Neural Information Processing Systems, pages 7029–7039.
Wolf et al. (2019) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R’emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface’s transformers: State-of-the-art natural language processing. ArXiv, abs/1910.03771.
Ye et al. (2020) Hongbin Ye, Ningyu Zhang, Shumin Deng, Mosha Chen, Chuanqi Tan, Fei Huang, and Huajun Chen. 2020. Contrastive triple extraction with generative transformer. arXiv preprint arXiv:2009.06207.
Zamani et al. (2020) Hamed Zamani, Susan Dumais, Nick Craswell, Paul Bennett, and Gord Lueck. 2020. Generating clarifying questions for information retrieval. InProceedings of The Web Conference 2020, pages 418–428.
Zhang et al. (2019a) Junjie Zhang, Lingqiao Liu, Peng Wang, and Chunhua Shen. 2019a. To balance or not to balance: A simple-yet-effective approach for learning with long-tailed distributions. arXiv, pages arXiv–1912.
Zhang et al. (2020a) Ningyu Zhang, Shumin Deng, Zhanlin Sun, Jiaoyan Chen, Wei Zhang, and Huajun Chen. 2020a. Relation adversarial network for low resource knowledge graph completion. InProceedings of The Web Conference 2020, pages 1–12.
Zhang et al. (2019b) Ningyu Zhang, Shumin Deng, Zhanlin Sun, Guanying Wang, Xi Chen, Wei Zhang, and Huajun Chen. 2019b. Long-tail relation extraction via knowledge graph embeddings and graph convolution networks. arXiv preprint arXiv:1903.01306.
Zhang et al. (2018) Ningyu Zhang, Shumin Deng, Zhanling Sun, Xi Chen, Wei Zhang, and Huajun Chen. 2018. Attention-based capsule networks with dynamic routing for relation extraction. InProceedings of EMNLP.
Zhang et al. (2020b) Ningyu Zhang, Luoqiu Li, Shumin Deng, Haiyang Yu, Xu Cheng, Wei Zhang, and Huajun Chen. 2020b. Can fine-tuning pre-trained models lead to perfect nlp? a study of the generalizability of relation extraction. arXiv preprint arXiv:2009.06206.
Zhang et al. (2017) Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, and Christopher D Manning. 2017. Position-aware attention and supervised data improve slot filling. InProceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 35–45.
Zhou et al. (2019) Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhaomin Chen. 2019. Bbn: Bilateral-branch network with cumulative learning for long-tailed visual recognition. ArXiv, abs/1912.02413.
