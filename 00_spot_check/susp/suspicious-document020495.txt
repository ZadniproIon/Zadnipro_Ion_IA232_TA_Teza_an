Structure-Informed Neural Networks for Boundary Observation Problems

By Jakub Horsky and Andrew Wynn

Abstract

We propose Structure Informed Neural Networks (SINNs), a novel approach to solving boundary observation problems involving partial differential equations (PDEs). This data-driven framework generates approximate solutions for internal variables within a domain's interior using only boundary data. By leveraging neural networks to identify a coordinate transformation into a latent space, we construct a well-posed elliptic system of PDEs, enabling efficient information transfer from the boundary to the interior. This allows for the approximation of solutions to PDE boundary observation problems, including generic and ill-posed cases. Furthermore, SINNs can be trained on experimental or numerical data without requiring knowledge of the underlying PDE, making them a versatile tool. The effectiveness of SINNs is demonstrated through two challenging examples: a nonlinear heat equation and boundary observation for the Navier-Stokes equations, showcasing their ability to accurately solve complex boundary observation problems.

This research focuses on the intersection of several key areas, including data-driven scientific computing, reduced order modeling, and machine learning, with a particular emphasis on the application of these techniques to partial differential equations and the development of operator learning methods.

1 Introduction

Boundary observation problems aim to discover the value of a physical quantity inside a domain by using only observations from its boundary. If possible, this means that potentially complex physical information can be obtained without the need for invasive internal sensors. Many fundamental problems in engineering and physics have this form with applications, for example, in fluid mechanics Illingworth et al. [2018] , medical imaging Song et al. [2022] , geophysics Snieder [1988] , and thermal sensing Bryan and Caudill [1996] .

In many applications, the internal physical quantity of interest is connected to boundary observations through a partial differential equation (PDE), making analytical analysis and numerical solution highly challenging. A notable example is fluid mechanics, where the nonlinear Navier-Stokes equations relate internal fluid properties, such as velocity or temperature, to easily observable boundary data like pressure or shear stress. The inherent complexity of solutions to these nonlinear PDEs poses a significant challenge in solving boundary observation problems of practical significance.

This paper presents a novel data-driven approach, termed Structure Informed Neural Networks (SINNs), for tackling boundary observation problems that involve nonlinear partial differential equations (PDEs). Our methodology embeds a well-posed structure for boundary observation problems into a data-driven framework, aiming to facilitate efficient and low-order approximate solutions. The process unfolds in three stages, as illustrated in Figure 1. Initially, a neural network encodes the boundary data and boundary geometry into a simplified latent space, capturing the essential boundary variables. Next, an elliptic system, as described by Giaquinta and Martinazzi [2012], is used to propagate information from the boundary to the interior of the latent space, effectively incorporating a broad class of well-posed PDEs into the SINN framework. Finally, a second neural network decodes the interior latent variables back into physical variables, yielding the solution.

The idea of using elliptic systems in a data-driven approach is the main novelty of this paper. Boundary value problems for elliptic systems were widely studied in the “golden age” of PDE analysis in the 1950s Morrey Jr. and Nirenberg [1957] . Our motivation for using them now is that they can describe a significant range of boundary value problems, are numerically tractable to solve, and can be defined with only small number of parameters. The second major contribution of this paper is to develop an operator-theoretic framework for embedding elliptic systems within the classical encoder-decoder structure of neural network-based reduced order modelling. This underpins the efficient numerical identification of SINNs, enables a powerful coupling of elliptic systems with deep neural networks, and opens the door to the data-driven solution of a wide range of challenging nonlinear boundary observation problems.

The structure-informed neural networks (SINNs) developed here have some similarities, and take inspiration from, a number of existing data-driven methods for PDE analysis. For example, Koopman-based modal decomposition methods Bevanda et al. [2021] possess the same three-stage mapping structure as in Figure 1 ; Physics-Inspired Neural Networks (PINNs) Raissi et al. [2019] use neural networks to efficiently solve PDEs, including boundary value problems Li et al. [2023] ; and Neural Operators Kovachki et al. [2023] use a kernel-based Neural Networks to construct solution operators for PDE parameter identification. To enable a full discussion of the relation and distinction between SINNs and existing methods in § 1.3 we must first define the mathematical structure of the boundary observations problems we aim to solve and give an overview of the SINN methodology.

1.1 Boundary observation problems

Consider a physical domain $\Omega\subset\mathbb{R}^{d}$ , with $d=2$ or $3$ , and let $\partial\Omega$ denote its boundary. At each point $\boldsymbol{x}\in\Omega$ , we want to recover the value of an $n$ -dimensional physical variable $\boldsymbol{u}(\boldsymbol{x})\in\mathbb{R}^{n}$ . To do this, we can only make use of boundary data $\boldsymbol{b}(\boldsymbol{z})\in\mathbb{R}^{n_{\partial}}$ which can be measured at each point $\boldsymbol{z}\in\partial\Omega$ . It will be assumed that both interior and boundary data are square-integrable functions in the sense that $\boldsymbol{u}\in X$ , where 
 
 $X=L^{2}(\Omega,\mathbb{R}^{n})=\left\{f:\Omega\rightarrow\mathbb{R}^{n}:\int_{\Omega}\|f(\boldsymbol{x})\|^{2}d\boldsymbol{x}<\infty\right\},$ 
 and that $\boldsymbol{b}\in Y$ , where 
 
 $Y=L^{2}(\partial\Omega,\mathbb{R}^{n_{\partial}})=\left\{f:\partial\Omega\rightarrow\mathbb{R}^{n_{\partial}}:\int_{\partial\Omega}\|f(\boldsymbol{z})\|^{2}d\boldsymbol{z}<\infty\right\}.$ 


A typical situation in which such data arises is if the internal and boundary data satisfy a PDE of the form 
 
 $\begin{split}\mathcal{L}(\boldsymbol{u},\boldsymbol{\lambda})&=0,\qquad\text{in}\;\Omega\\
\mathcal{B}(\boldsymbol{u})&=\boldsymbol{b},\qquad\text{on}\;\partial\Omega,\end{split}$  (1) 
 where $\mathcal{L}$ is a differential operator, $\boldsymbol{\lambda}$ are any parameters, and $\mathcal{B}$ is an output operator linking the interior to boundary variables. We do not assume that ( 1 ) is a well-posed in the sense that for any boundary function $\boldsymbol{b}$ , there is a unique solution $\boldsymbol{u}$ satisfying the PDE. Instead, the output operator $\mathcal{B}$ should be viewed simply as shorthand for the “available information” which may be observed on the boundary, given that a physical system is in state $\boldsymbol{u}$ inside the domain.

In this abstract language, the structure-informed neural networks (SINNs) constructed in this paper are operators that map observable measurements from the function space $Y$ to distributions of interior physical values in the space $X$, denoted as $\mathcal{F}: Y \rightarrow X, \boldsymbol{b} \mapsto \boldsymbol{u}$ (2). The identification of SINNs as operators is crucial, as it enables a single SINN $\mathcal{F}$ to approximate internal variables $\boldsymbol{u}$ for any given boundary observation $\boldsymbol{b} \in Y$. This operator-based approach aligns SINNs with recent neural-network-based operator identification methods, such as Neural Operators (Kovachki et al., 2023) and DeepONets (Lu et al., 2021), as will be discussed in §1.3.

A key objective of this paper is to identify SINN operators from data, assuming the availability of a data ensemble $\mathcal{U}:=\{(\boldsymbol{u}_{i},\boldsymbol{b}_{i})\}_{i=1}^{N_{T}}\subset(X\times Y)^{N_{T}}$, which consists of $N_{T}$ pairs of internal and boundary data derived from solutions to equation (1). The goal is to construct a mapping of the form (2) that optimally fits the data $\mathcal{U}$. However, due to the infinite-dimensional nature of the underlying problem and the finite-dimensional nature of the data, any practical numerical method must impose a priori restrictions on the possible forms that $\mathcal{F}$ can take.

1.2 SINN operators

We assume that $\mathcal{F}$ is the composition of three operators 
 
 $\mathcal{F}=\delta\circ\mathcal{E}_{\mathcal{A}}\circ\epsilon^{\partial},$  (3) 
 the structure of which is shown schematically in Figure 1 . The first operator is called a boundary encoder $\epsilon^{\partial}:Y\rightarrow Y_{L}$ . This is a nonlinear operator, defined in terms of a neural network, which maps both the boundary data and geometry into a boundary latent space, $Y_{L}:=L^{2}(\partial\Omega,\mathbb{R}^{r})$ , where the parameter $r$ governs the order and complexity of the latent space.

To facilitate data-driven training, we further restrict the operator $\epsilon^{\partial}$ to act semi-locally, meaning that its value at any point $\boldsymbol{z}\in\partial\Omega$ depends only on the values of the boundary data $\boldsymbol{b}\in Y$ within a small neighbourhood $N_{\boldsymbol{z}}\subset\partial\Omega$ of $\boldsymbol{z}$. This is achieved by training a neural network $\mathcal{N}^{\partial}$ that maps the local values of $\boldsymbol{b}$ in $N_{\boldsymbol{z}}$ to the value of $(\epsilon^{\partial}\boldsymbol{b})(\boldsymbol{z})$. As described in §2.2, the local nature of the input to $\mathcal{N}^{\partial}$ allows a single neural network to be repeatedly applied to build up the definition of $\epsilon^{\partial}:Y\rightarrow Y_{L}$, enabling the consideration of a wide class of nonlinear operators without significantly increasing the number of optimisation parameters.

The introduction of latent variables serves to establish a common framework for transferring information from the boundary latent space $Y_{L}$ to the interior latent space $X_{L}=L^{2}(\Omega,\mathbb{R}^{r})$. This information transfer is facilitated in a SINN through the use of an elliptic system of PDEs, which is characterized by a second-order differential operator $D_{\mathcal{A}}\boldsymbol{\ell}=\sum_{i,j=1}^{d}A_{ij}\frac{\partial^{2}\boldsymbol{\ell}}{\partial x_{i}\partial x_{j}}$. Here, $A_{ij}\in\mathbb{R}^{r\times r}$ are symmetric matrices that satisfy two key conditions: firstly, $A_{ij}=A_{ji}$ for all $i,j=1,\dots,d$, ensuring symmetry; and secondly, the block matrix $\mathcal{A}=(A_{ij})\in\mathbb{R}^{rd\times rd}$ is strictly positive definite, guaranteeing the stability of the system.

Information is passed from the boundary latent space $Y_{L}$ to an interior latent space $X_{L}$ by solving the following boundary problem: 
 
 $\begin{split}D_{\mathcal{A}}\boldsymbol{\ell}&=0,\phantom{(b)^{\partial}}\qquad\text{in}\;\Omega\\
\boldsymbol{\ell}&=\epsilon^{\partial}(\boldsymbol{b}),\qquad\text{on}\;\partial\Omega.\end{split}$  (4) 
 The assumption that $\mathcal{A}$ is positive definite is crucial. This implies that ( 4 ) is a strongly elliptic system of PDEs. It then follows, under appropriate smoothness conditions Giaquinta and Martinazzi [2012] on the latent boundary data $\epsilon^{\partial}(\boldsymbol{b})$ and the boundary geometry, that ( 4 ) has a unique solution $\boldsymbol{\ell}\in X_{L}$ . We let $\mathcal{E}_{\mathcal{A}}:Y_{L}\rightarrow X_{L}$ denote the operator which maps boundary data to internal variables when solving the elliptic boundary value problem ( 4 ). The structure of the SINN mapping ( 3 ) is hence specifically designed to create a latent space in which passage of data from boundary to the interior is well-posed. This is achieved irrespective of the properties of the PDE or the observation mapping structure ( 1 ) from which the physical data was sampled.

The third and final component of a SINN operator is a decoder, denoted as $\delta$, which maps a distribution of interior latent variables $\boldsymbol{\ell}\in X_{L}$ to the physical space $\boldsymbol{u}\in X$, as expressed by the equation $\delta: X_{L} \longrightarrow X$ with $\boldsymbol{\ell}\longmapsto\boldsymbol{u}$. Similar to the boundary encoder, the decoder $\delta$ is assumed to be nonlinear and semi-local, meaning that for any point $\boldsymbol{x}$ in the domain $\Omega$, the value of $(\delta\boldsymbol{\ell})(\boldsymbol{x})$ depends only on the values of $\boldsymbol{\ell}$ within a small neighborhood $N_{\boldsymbol{x}}$ of $\boldsymbol{x}$. This can be achieved using a single neural network $\mathcal{N}$ that takes the values of $\boldsymbol{\ell}$ in the neighborhood $N_{\boldsymbol{x}}$ as input and produces the value of $\boldsymbol{\ell}$ at $\boldsymbol{x}$, and this network can be applied repeatedly to define the operator $\delta$, as described in detail in §2.3.

A structure-informed neural network (SINN) $\mathcal{F}$ is an operator that combines three components: a semi-local nonlinear neural network $\epsilon^{\partial}:Y\rightarrow Y_{L}$, a global elliptic system $\mathcal{E}_{\mathcal{A}}:Y_{L}\rightarrow X_{L}$, and another semi-local nonlinear neural network $\delta:X_{L}\rightarrow X$. The semi-local architecture of the encoder and decoder mappings is designed to reduce the number of degrees of freedom in defining the nonlinear components. The global elliptic system enables the transfer of information from the boundary to the interior in the latent space, embedding a natural and general object into the SINN, which is tailored to solve boundary observation problems. This approach has a key advantage: the coefficients of the elliptic system can be identified efficiently using local training data, and once trained, the system can be applied globally to solve the original problem, as described in §3.

In § 2 we introduce the concept of a generating function which underpins the semi-local structure of the encoder and decoder operators, before introducing these operator formally and deriving their inherited mathematical properties. The method of training SINNs from data is described in § 3 and its numerical implemention discussed in § 4 . Implementation of our approach on a pair of challenging test-cases is given in § 5 . Before this, we first comment briefly on the relation between the proposed SINN architecture and other, related, data-driven approaches to PDE analysis.

1.3 Relation of SINNs to existing methods

The use of neural networks to solve PDEs has received much recent interest with the development of Physics-inspired Neural Networks (PINNs) Raissi et al. [2019] . In the context of solving a PDE of the form ( 1 ), the idea is to view the solution $\boldsymbol{u}$ as a mapping $\mathbb{R}^{d}\ni\boldsymbol{x}\mapsto\boldsymbol{u}(\boldsymbol{x})\in\mathbb{R}^{n}$ and to therefore seek to construct a neural network $\mathcal{N}_{P}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{n}$ which approximates the solution. The crucial step is to add so-called physics-inspired constraints, namely $\mathcal{L}(\mathcal{N}_{P}(\boldsymbol{x}),\boldsymbol{\lambda})_{|_{\Omega}}=0$ and $\left[\mathcal{B}(\mathcal{N}_{P}(\boldsymbol{x}))-\boldsymbol{b}\right]_{|_{\partial\Omega}}=0$ , to force the constructed solution to satisfy the underlying PDE.

In contrast to the SINN operators $\mathcal{F}:Y\rightarrow X$ which act between functions spaces, PINNs are finite-dimensional mappings that directly attempt to replicate the solution mapping $\boldsymbol{x}\mapsto\boldsymbol{u}(\boldsymbol{x})$ . They require knowledge of the underlying PDE they seek to solve (i.e., of $\mathcal{L},\boldsymbol{\lambda}$ and $\mathcal{B}$ ) and, when applied to boundary observation problems, must be trained using knowledge of the specific boundary data $\boldsymbol{b}$ . In contrast, SINNs do not require such information: the identification of operators means that such boundary data is not required in the SINN methodology.

The three-operator structure of the mapping $\mathcal{F}=\delta\circ\mathcal{E}_{\mathcal{A}}\circ\epsilon^{\partial}$ is commonly employed in various data-driven methods for low-order modelling, including Koopman-based modelling. In the latter, this structure is used to approximate the time-evolution of complex, infinite-dimensional dynamical systems, where the central operator $\mathcal{E}_{\mathcal{A}}$ represents temporal evolution. In contrast to the approach presented in this paper, where $\mathcal{E}_{\mathcal{A}}$ models information passage from a domain's boundary to its interior, Koopman methodology utilizes a finite-dimensional latent space and a finite-dimensional ordinary differential equation (ODE) as the temporal operator, distinguishing it from the SINNs.

This represents an important distinction with the SINN methodology. To explain, consider the case of the decoder $\delta$ operator, and assume that it maps from a finite dimensional latent space, say $\mathbb{R}^{r}$ , into the infinite dimensional space of physical variables $X=L^{2}(\Omega,\mathbb{R}^{n})$ . The mismatch in dimensions between latent and physical space implies that the decoder must have an inherent method of translating finite-dimensional latent variables to infinite-dimensional functions. In Koopman-based approaches, this is typically achieved by considering a basis of functions $\{\Phi_{i}\}_{i=1}^{N}\subset X$ and letting $\delta$ involve a mapping from the latent space $\mathbb{R}^{r}$ to the coefficients $\{\hat{f}_{i}\}\subset\mathbb{R}^{N}$ of a series expansion $\sum_{i=1}^{N}\hat{f}_{i}\Phi_{i}\in X$ . A major challenge of this approach is the choice of an appropriate basis $\{\Phi_{i}\}$ and attempting to solve this problem has motivated a range of different Koopman-based methods Wynn et al. [2013] .

In contrast, in the SINN approach developed here, the use of an elliptic system $\mathcal{E}_{\mathcal{A}}$ removes the need for assigning or identifying a set of basis functions, and therefore the imposition of unnecessary structure on the operator $\mathcal{F}$ . Constructing an appropriate elliptic system only requires identifying the PDE coefficient matrix $\mathcal{A}$ , which potentially offers a significant reduction in dimension to identifying a set of basis function $\{\Phi_{i}\}\subset X$ . This advantage comes at the cost of requiring solution of a PDE, as opposed to an ODE, as the central component of the model $\mathcal{F}$ . However, the SINN methodology deliberately imposes a well-posed elliptic structure which, in many cases, enables this PDE to be solved at accurately and at low cost using existing algorithms. In addition, as will be explained in § 3 , since our aim is to identify a PDE, the cost function for SINN training can be chosen to involve only low-cost, local, solutions to elliptic systems during training. However, once trained, the identified elliptic systems can then be used to transfer information across a domain globally.

The philosophy underlying the SINN methodology for identifying operators, as presented in this paper, aligns with the recent trend of utilizing neural networks to identify operators between function spaces, exemplified by Neural Operators (Kovachki et al., 2023) and DeepONets (Lu et al., 2021). The Neural Operator framework aims to construct solution operators that solve PDEs with Dirichlet boundary conditions using distributed parameters, by globally transferring information within the domain through an iterative sequence of integral operators with neural network-identified kernels. To facilitate practical computational implementation, these integral kernels often require structural imposition, such as low-rank approximations, Convolutional Neural Networks, Graph Neural Networks (Pilva and Zareei, 2022), or Fourier Neural Operators (Li et al., 2021), which is philosophically analogous to prescribing a functional basis in the Koopman-based methodology. In contrast, the SINN methodology enables global information transfer with a relatively small number of coefficients by training elliptic operators, without imposing additional structure on the operator ansatz. Notably, the DeepONet methodology (Lu et al., 2021), a special case of the Neural Operator approach, also necessitates the identification of a functional basis during training, highlighting the distinction between these methods and the SINN approach.

2 Encoders and Decoders for SINNs

This section provides a detailed description of the mathematical structure of the encoder and decoder operators necessary for creating a SINN, which comprises three classes of operators: interior encoders, boundary encoders, and decoders. While only boundary encoders and decoders are needed to define a SINN mapping, as illustrated in Figure 1, interior encoders play a crucial role in enabling data-driven training, as discussed further in § 3.

The semi-local structure of all encoder and decoder operators will be implemented by defining generating functions (GFs), which act as the building blocks of the SINN methodology. In each of the follow sections we first introduce a generating function, use it to define the respective operator, then comment on the regularity properties inherited by that operator.

2.1 Interior Encoders

For model training purposes, we construct interior encoders $\epsilon$ that map any distribution of physical variables $\boldsymbol{u}\in X$ to a distribution of latent variables $\boldsymbol{\ell}$, where $\boldsymbol{\ell} = \epsilon(\boldsymbol{u})$, defined on the domain interior.

Given a compact set $0\in E\subset\mathbb{R}^{d}$, an interior encoder's generating function is a continuous, compact, and generally nonlinear mapping $e:L^{2}(E,\mathbb{R}^{n})\longrightarrow\mathbb{R}^{r}$, which can be interpreted as mapping a local patch of interior data to latent variables, thereby endowing the interior encoder $\epsilon$ with a semi-local structure.

For any point $\boldsymbol{x}$ in the domain $\Omega$, we define a local neighbourhood $E_{\boldsymbol{x}}$ as the set of points obtained by translating the set $E$ by $\boldsymbol{x}$, i.e., $E_{\boldsymbol{x}} = \{\boldsymbol{x} + \boldsymbol{y} : \boldsymbol{y} \in E\}$. The set $\Omega_E$ consists of all points $\boldsymbol{x}$ in $\Omega$ for which the entire neighbourhood $E_{\boldsymbol{x}}$ is contained within $\Omega$. These constructs are illustrated in Figure 3.

For any $\boldsymbol{x}\in\Omega_{E}$ and $\boldsymbol{u}\in X$, we define a local function $\boldsymbol{u}_{\boldsymbol{x}}:E\rightarrow\mathbb{R}^{n}$ as $\boldsymbol{u}_{\boldsymbol{x}}(\boldsymbol{y})=\boldsymbol{u}(\boldsymbol{x}+\boldsymbol{y})$ for $\boldsymbol{y}\in E$. Using a generating function $e:L^{2}(E,\mathbb{R}^{n})\rightarrow\mathbb{R}^{r}$, we then introduce an interior encoder defined by $\left(\epsilon\boldsymbol{u}\right)(\boldsymbol{x}):=e\left(\boldsymbol{u}_{\boldsymbol{x}}\right)$ for $\boldsymbol{x}\in\Omega_{E}$.

This definition should be thought of as mapping the physical data $\boldsymbol{u}$ , viewed as a function in $X=L^{2}(\Omega,\mathbb{R}^{n})$ , to a new function $\epsilon\boldsymbol{u}:\Omega_{E}\rightarrow\mathbb{R}^{r}$ . This allows the latent variables $\boldsymbol{\ell}(\boldsymbol{x})=(\epsilon\boldsymbol{u})(\boldsymbol{x})$ corresponding to $\boldsymbol{u}$ to be defined on the subdomain $\Omega_{E}$ .

It follows trivially from its definition that interior encoders are operators satisfying $\epsilon:L^{2}(\Omega,\mathbb{R}^{n})\rightarrow L^{2}(\Omega_{E},\mathbb{R}^{r})$ . However, the following result shows that the latent variable field created using the encoder $\epsilon$ are, in fact, continuous, uniformly bounded, functions.

Let $e:L^{2}(E,\mathbb{R}^{n})\rightarrow\mathbb{R}^{r}$ be an interior encoder generating function and let $\epsilon$ be defined by ( 7 ). Then $\epsilon:L^{2}(\Omega,\mathbb{R}^{n})\rightarrow C(\Omega_{E},\mathbb{R}^{r})$ .

See Appendix 8.1 . ∎

2.2 Boundary Encoders

We present a method for constructing a boundary encoder, denoted as $\epsilon^{\partial}$, which takes any distribution of boundary values $\boldsymbol{b}\in Y$ and maps them to a distribution of latent variables $\boldsymbol{\ell}=\epsilon^{\partial}\boldsymbol{b}$ on the boundary $\partial\Omega$. This construction is similar to that of the interior encoder in (7), but with the additional challenge of incorporating information about the boundary geometry.

A boundary encoder generating function is a continuous, compact, and generally nonlinear function $e^{\partial}$ that maps from the space of boundary data and geometry to a set of latent variables. Specifically, it is defined as $e^{\partial}:L^{2}(E_{\partial},\mathbb{R}^{n_{\partial}})\times L^{2}(E_{\partial},\mathbb{R}^{d})\rightarrow\mathbb{R}^{r}$, where $E_{\partial}$ is a fixed, compact subset of $\mathbb{R}^{d-1}$ containing the origin. This function can be thought of as taking in a section of boundary data and a section of boundary geometry and outputting a set of latent variables, and will be used repeatedly to define a semi-local boundary encoder operator.

We assume throughout that the boundary $\partial\Omega$ is sufficiently regular, allowing for a normal vector $\boldsymbol{n}({\boldsymbol{z}})\in\mathbb{R}^{d}$ and a tangent plane $T_{\boldsymbol{z}}\subset\mathbb{R}^{d-1}$ to exist at every point $\boldsymbol{z}\in\partial\Omega$. Each tangent plane $T_{\boldsymbol{z}}$ is defined by a local coordinate system centered at $\boldsymbol{z}$, with orthogonal basis vectors $(\boldsymbol{e}^{\boldsymbol{z}}_{i})_{i=1}^{d-1}$ that are perpendicular to $\boldsymbol{n}({\boldsymbol{z}})$. Furthermore, we assume that for each $\boldsymbol{z}\in\partial\Omega$, there exists a ball $B_{R}(\boldsymbol{z})\subset\mathbb{R}^{d}$ of radius $R$ such that the local projection $P_{\boldsymbol{z}}:\partial\Omega\cap B_{R}(\boldsymbol{z})\rightarrow T_{\boldsymbol{z}}$ is one-to-one. Additionally, we assume that there exists a constant $\tau>0$, independent of $\boldsymbol{z}$, such that the projection of the intersection of the ball and the boundary onto the tangent plane satisfies $\{t_{i}\boldsymbol{e}_{i}^{\boldsymbol{z}}:0\leq t_{i}<\tau\}\subset P_{\boldsymbol{z}}(B_{R}(\boldsymbol{z})\cap\partial\Omega)\subset T_{\boldsymbol{z}}$ for all $\boldsymbol{z}\in\partial\Omega$, and that $E_{\partial}\subseteq(0,\tau)^{d-1}$. These assumptions imply that $E_{\partial}$ can be viewed as a subset of the tangent plane, and that a well-defined, continuous, inverse mapping $P_{\boldsymbol{z}}^{-1}:\{t_{i}\boldsymbol{e}_{i}^{\boldsymbol{z}}:\boldsymbol{t}\in E_{\partial}\}\rightarrow\partial\Omega$ exists.

This technical construction allows us, for each $\boldsymbol{z}\in\partial\Omega$ , to define a function $\boldsymbol{b}_{\boldsymbol{z}}:E_{\partial}\rightarrow\mathbb{R}^{n_{\partial}}$ , which depends on the boundary data local to $\boldsymbol{z}$ , by 
 
 $\boldsymbol{b}_{\boldsymbol{z}}(\boldsymbol{t}):=\boldsymbol{b}\left(P^{-1}_{\boldsymbol{z}}(t_{i}\boldsymbol{e}_{i}^{\boldsymbol{z}})\right),\qquad\boldsymbol{t}=(t_{i})_{i=1}^{d-1}\in E_{\partial},$  (11) 
 Similarly, we can also define a function $\boldsymbol{n}_{\boldsymbol{z}}:E_{\partial}\rightarrow\mathbb{R}^{d}$ which describes the boundary geometry local to $\boldsymbol{z}$ by 
 
 $\boldsymbol{n}_{\boldsymbol{z}}(\boldsymbol{t}):=\boldsymbol{n}\left(P^{-1}_{\boldsymbol{z}}(t_{i}\boldsymbol{e}_{i}^{\boldsymbol{z}})\right),\qquad\boldsymbol{t}=(t_{i})_{i=1}^{d-1}\in E_{\partial}.$  (12) 


Using the boundary generating function $e^{\partial}$, we define the corresponding boundary encoder as $\left(\epsilon^{\partial}\boldsymbol{b}\right)(\boldsymbol{z}):=e^{\partial}(\boldsymbol{b}_{\boldsymbol{z}},\boldsymbol{n}_{\boldsymbol{z}})$ for $\boldsymbol{z}\in\partial\Omega$. Since $e^{\partial}$ is compact and continuous, a proof analogous to that of Lemma 1 shows that $\epsilon^{\partial}$ maps $L^{2}(\partial\Omega,\mathbb{R}^{n_{\partial}})$ to $C(\partial\Omega,\mathbb{R}^{r})$. Consequently, the boundary latent variables, given by $\boldsymbol{\ell}_{|_{\partial\Omega}}=\epsilon^{\partial}\boldsymbol{b}$, are continuous functions.

2.3 Decoders

We describe the construction of a decoder mapping $\delta$ that transforms a given distribution of latent variables $\ell\in X_{L}$ into a distribution of physical variables $\boldsymbol{u}$ on the domain interior, where $\boldsymbol{u}=\delta(\boldsymbol{\ell})$.

Decoder GFs: Given a compact, symmetric, set $0\in D\subset\mathbb{R}^{d}$ , a decoder generating function is any continuous, compact, and generally nonlinear mapping 
 
 $d:\mathbb{R}^{r}\rightarrow C(D,\mathbb{R}^{n}).$  (14) 
 This GF should be thought of as follows: given latent variables $\boldsymbol{\ell}(\boldsymbol{y})\in\mathbb{R}^{r}$ at a point $\boldsymbol{y}\in\Omega$ , then $d(\boldsymbol{\ell}(\boldsymbol{y}))(\boldsymbol{x})$ gives a local prediction of the physical variables $\boldsymbol{u}(\boldsymbol{x})\in\mathbb{R}^{n}$ for any $\boldsymbol{x}\in D_{\boldsymbol{y}}=\boldsymbol{y}+D$ . The idea is to use this map repeatably to build up a semi-local decoder operator.

We define decoders under two distinct scenarios, categorizing them as either partition decoders or averaging decoders.

A partition decoder is defined based on the assumption that there exists a set of points $\{\boldsymbol{y}_{i}\}_{i=1}^{N_{d}}$ in $\Omega$ such that the collections of sets $(\boldsymbol{y}_{i}+D)_{i=1}^{N_{d}}$ form a disjoint partition of $\Omega$. For a given latent variable distribution $\boldsymbol{\ell}\in X_{L}$ and a point $\boldsymbol{x}\in\Omega$, the partition property ensures that there is a unique index $j$ such that $\boldsymbol{x}\in\boldsymbol{y}_{j}+D$, implying $\boldsymbol{x}-\boldsymbol{y}_{j}\in D$. This allows us to define a decoded value $\boldsymbol{u}_{\boldsymbol{\ell}}(\boldsymbol{x})$ as $(\delta\boldsymbol{\ell})(\boldsymbol{x}) = d(\boldsymbol{\ell}(\boldsymbol{y}_{j}))(\boldsymbol{x}-\boldsymbol{y}_{j})$. The operator $\delta$ can then be viewed as a mapping from $X_{L}$ to $X$, enabling the creation of an approximation to the physical variables by setting $\boldsymbol{u}(\boldsymbol{x})=(\delta\boldsymbol{\ell})(\boldsymbol{x})$.

Using a partition decoder has an advantage in that choosing $D$ as a coarse discretization of $\Omega$ can reduce the computational cost of implementation. However, this choice also has two potential drawbacks. Firstly, requiring the decoder to extrapolate from latent to physical variables over a large set $D$ can introduce approximation errors. Secondly, although $\delta\boldsymbol{\ell}$ is guaranteed to be square-integrable as an element of $X$, the resulting physical solution may not be smooth or even continuous. If a smoother solution is desired, an alternative approach is to implement an averaging decoder, which can provide a more desirable outcome.

To define averaging decoders, consider a latent variable distribution $\boldsymbol{\ell}\in X_{L}$ and a fixed point $\boldsymbol{x}\in\Omega$. For any point $\boldsymbol{y}$ such that $\boldsymbol{x}\in D_{\boldsymbol{y}}$, the decoder function $d(\boldsymbol{\ell}({\boldsymbol{y}}))$ yields a prediction of the physical variables at $\boldsymbol{x}$. The goal is to average all possible predictions of this form. Since $D$ is symmetric, the condition $\boldsymbol{x}\in D_{\boldsymbol{y}}$ is equivalent to $\boldsymbol{y}\in D_{\boldsymbol{x}}$, meaning that $\boldsymbol{x}$ can be predicted from any $\boldsymbol{y}\in D_{\boldsymbol{x}}\cap\Omega$. The prediction from point $\boldsymbol{y}$ at $\boldsymbol{x}$ is given by $d(\boldsymbol{\ell}(\boldsymbol{y}))(\boldsymbol{x}-\boldsymbol{y})$, as illustrated in Figure 4.

Given a function $\ell\in C(\Omega,\mathbb{R}^{r})$, we define a decoder mapping $\delta$ as follows: for each $\boldsymbol{x}\in\Omega$, $(\delta\boldsymbol{\ell})(\boldsymbol{x})$ is the average value of the integral of $d(\boldsymbol{\ell}(\boldsymbol{y}))(\boldsymbol{x}-\boldsymbol{y})$ over the intersection of $D_{\boldsymbol{x}}$ and $\Omega$, which can be expressed mathematically as $\frac{1}{|D_{\boldsymbol{x}}\cap\Omega|}\int_{D_{\boldsymbol{x}}\cap\Omega}d(\boldsymbol{\ell}(\boldsymbol{y}))(\boldsymbol{x}-\boldsymbol{y})\,d\boldsymbol{y}$.

The following lemma demonstrates that continuous generating functions yield decoders that produce continuous functions throughout the entire domain $\Omega$.

Let $d:\mathbb{R}^{r}\rightarrow C(D,\mathbb{R}^{n})$ be a decoder generating function, where $\delta$ is defined as in equation (15). This induces a mapping $\delta:C(\Omega,\mathbb{R}^{r})\rightarrow C(\Omega,\mathbb{R}^{n})$.

See Appendix 8.2 . ∎

2.4 Elliptic Systems

The final component required to define a SINN operator is an elliptic system. We simply refer to any symmetric, strictly positive definite, matrix 
 
 $\mathcal{A}\in\mathbb{S}_{++}^{(rd)^{2}}$  (16) 
 as a generating function from which an elliptic operator $D_{\mathcal{A}}$ and the associated boundary value problem ( 4 ) can be defined. This then generates the solution operator $\mathcal{E}_{\mathcal{A}}:Y_{L}\rightarrow X_{L}$ .

3 Training generating functions

To recap the constructions from §2, consider localisation sets $G:=(D,E,E_{\partial})$ and local generating functions $(d,e,e^{\partial},\mathcal{A})$. Using equations (7), (13), and (15), we can define a globalisation mapping $\mathcal{G}_{G}$ that takes $(d,e,e^{\partial},\mathcal{A})$ and outputs an interior encoder $\epsilon$, a boundary encoder $\epsilon^{\partial}$, a decoder $\delta$, and an elliptic system solution operator $\mathcal{E}_{\mathcal{A}}$. Combining these components yields a SINN operator $\mathcal{F}=\delta\circ\mathcal{E}_{\mathcal{A}}\circ\epsilon^{\partial}$, as in equation (3). Our goal is now to leverage the available data ensemble $\mathcal{U}=\left(\boldsymbol{u}_{j}(x),\boldsymbol{b}_{j}(\boldsymbol{z})\right)_{j=1}^{N_{T}}$, where $\boldsymbol{x}\in\Omega$ and $\boldsymbol{z}\in\partial\Omega$, to identify optimal generating functions and, in turn, optimal SINN operators.

3.1 The cost function for SINN training

Training is formulated as a minimisation problem, with the cost function to be minimised defined by the four-stage process illustrated in Figure 5. Here, we consider a snapshot $(\boldsymbol{u},\boldsymbol{b})$ selected from the training data ensemble, where $(\boldsymbol{u},\boldsymbol{b})\in\mathcal{U}$.

Stage 1 involves fixing a set of training points $\{\boldsymbol{p}_{i}\}_{i=1}^{M}$ within the domain $\Omega_{E}$. At each training point $\boldsymbol{p}_{i}$, a training patch is defined as the convex hull of a set of local points $\{\boldsymbol{q}_{ij}\}_{j=1}^{N}$, which are either in $\Omega$ or on its boundary $\partial\Omega$. To be specific, for each $\boldsymbol{p}_{i}$, we assume there are points $\boldsymbol{q}_{ij}$ that satisfy three conditions: (i) each $\boldsymbol{q}_{ij}$ is in $\Omega_{E}$ or on $\partial\Omega$; (ii) $\boldsymbol{p}_{i}$ is inside the convex hull $Q_{i}$ formed by $\{\boldsymbol{q}_{ij}\}_{j=1}^{N}$; and (iii) each $\boldsymbol{q}_{ij}$ lies on the boundary of $Q_{i}$. Given generating functions $e$ and $e^{\partial}$, these assumptions allow us to compute latent variables: $\boldsymbol{\ell}_{i}$ for each $\boldsymbol{p}_{i}$ using $e(\boldsymbol{u}_{\boldsymbol{p}_{i}})$, and $\boldsymbol{\ell}_{ij}$ for each $\boldsymbol{q}_{ij}$, where $\boldsymbol{\ell}_{ij}$ is calculated using $e(\boldsymbol{u}_{\boldsymbol{q}_{ij}})$ if $\boldsymbol{q}_{ij}$ is in $\Omega$, or $e^{\partial}(\boldsymbol{b}_{\boldsymbol{q}_{ij}},\boldsymbol{\eta}_{\boldsymbol{q}_{ij}})$ if $\boldsymbol{q}_{ij}$ is on $\partial\Omega$.

Stage 2: Conditions $(ii)$ and $(iii)$ from Stage 1 ensure that the convex hull $Q_{i}$ is a polytope in $\mathbb{R}^{d}$, with each $\boldsymbol{q}_{ij}$ being an exterior point on its $(d-1)$-dimensional boundary $\partial Q_{i}$. Using linear interpolation, we can construct a continuous function $\boldsymbol{f}_{i}: \partial Q_{i} \to \mathbb{R}^{r}$ that satisfies $\boldsymbol{f}_{i}(\boldsymbol{q}_{ij}) = \boldsymbol{\ell}_{ij}$ for $j = 1, \dots, N$, as illustrated in Figure 5(b).

In Stage 3, we define the linear elliptic operator $D_{\mathcal{A}}$ associated with a given generating matrix $\mathcal{A}\in\mathbb{R}^{dr\times dr}$, and solve the boundary value problem $D_{\mathcal{A}}\hat{\boldsymbol{\ell}}=0$ in $Q_{i}$ with $\hat{\boldsymbol{\ell}}_{|_{\partial Q_{i}}}=\boldsymbol{f}_{i}$ on $\partial Q_{i}$. This yields a predicted value $\hat{\boldsymbol{\ell}}_{i}:=\hat{\boldsymbol{\ell}}(\boldsymbol{p}_{i})\in\mathbb{R}^{r}$, as illustrated in Figure 5(c). We then define an error function $\Psi_{1}((\boldsymbol{u},\boldsymbol{b}),(e,e^{\partial},\mathcal{A})):=\frac{1}{Mr}\sum_{i=1}^{M}\left\|\boldsymbol{\ell}_{i}-\hat{\boldsymbol{\ell}}_{i}\right\|_{2}^{2}$, which measures the average error between the encoded latent variables computed using the generating function $e$ and their predictions from the boundary $\partial Q_{i}$ using the elliptic system $\mathcal{E}_{\mathcal{A}}$, averaged over all $M$ training points $\{\boldsymbol{p}_{i}\}_{i=1}^{M}$.

At Stage 4, we utilize a decoder $d:\mathbb{R}^{r}\rightarrow C(D,\mathbb{R}^{n})$ to generate predictions for physical variables within local sets $D_{\boldsymbol{p}_{i}}$ surrounding the training points. Specifically, for $\boldsymbol{x}\in D_{\boldsymbol{p}_{i}}$, the prediction is given by $\hat{\boldsymbol{u}}(\boldsymbol{x}):=d(\hat{\boldsymbol{\ell}}_{i})(\boldsymbol{x}-\boldsymbol{p}_{i})$. To evaluate the accuracy of the decoder, we introduce a second error function, $\Psi_{2}((\boldsymbol{u},\boldsymbol{b}),(d,e,e^{\partial},\mathcal{A}))$, which quantifies the difference between the actual physical data and the predicted values averaged over a subset of $\Omega$ local to the chosen training points, calculated as $\frac{1}{M|D|}\sum_{i=1}^{M}\int_{D}|\boldsymbol{u}(\boldsymbol{p}_{i}+\boldsymbol{y})-\hat{\boldsymbol{u}}(\boldsymbol{p}_{i}+\boldsymbol{y})|_{2}^{2}d\boldsymbol{y}$.

The four-stage process described above allows us, for each data point $(\boldsymbol{u},\boldsymbol{b})\in\mathcal{U}$ to define two functions $\Psi_{1},\Psi_{2}$ which quantify the error associated with a given quadruple of generating functions $\mathcal{X}:=(d,e,e^{\partial},\mathcal{A})$ . We therefore define the ensemble cost function by 
 
 $\Psi\left(\mathcal{U},\mathcal{X}\right):=\frac{1}{N_{T}}\sum_{j=1}^{N_{T}}\left[\Psi_{1}((\boldsymbol{u}_{j},\boldsymbol{b}_{j}),\mathcal{X})+\alpha\Psi_{2}((\boldsymbol{u}_{j},\boldsymbol{b}_{j}),\mathcal{X})\right],$  (19) 
 where $\alpha>0$ is a weighting parameter.

4 Numerical Implementation

In § 2, it was demonstrated that the generating functions $(e,e^{\partial},d,\mathcal{A})$ serve as building blocks for SINNs. However, the generating functions $(e,e^{\partial},d)$ are still infinite-dimensional nonlinear functionals, necessitating a parameterisation to enable feasible SINN training. To address this, the infinite-dimensional generating functions $(e,e^{\partial},d)$ can be parameterised by mappings between finite-dimensional spaces, leveraging their assumed semi-local structure to achieve a simple and flexible approximation without imposing restrictive constraints.

4.1 Interior generating functions

The goal is to construct an ensemble of mappings $e:L^{2}(E,\mathbb{R}^{n})\rightarrow\mathbb{R}^{r}$ using only finite-dimensional functionals, which can be achieved through a two-step process. First, the domain $E\subset\mathbb{R}^{d}$ is partitioned into $N_{E}$ subsets, yielding $E=\bigcup_{i=1}^{N_{E}}E_{i}$. Then, for any function $\boldsymbol{u}\in L^{2}(E,\mathbb{R}^{n})$, a vector $\mathcal{D}\boldsymbol{u}\in\mathbb{R}^{n\times N_{E}}$ is created by taking the average of $\boldsymbol{u}$ over each subset. This allows any finite-dimensional mapping $\tilde{e}:\mathbb{R}^{n\times N_{E}}\rightarrow\mathbb{R}^{r}$ to be composed with $\mathcal{D}$, defining an encoder $e:L^{2}(E,\mathbb{R}^{n})\rightarrow\mathbb{R}^{r}$ as $e=\tilde{e}\circ\mathcal{D}$.

4.2 Boundary generating functions

The aim is to create boundary generating function $e^{\partial}:L^{2}(E_{\partial},\mathbb{R}^{n_{\partial}})\times L^{2}(E_{\partial},\mathbb{R}^{d})\rightarrow\mathbb{R}^{r}$ . Using the same idea as before, partition $E_{\partial}\subset\mathbb{R}^{d-1}$ into $N_{E_{\partial}}$ sets 
 
 $E_{\partial}=\bigcup_{i=1}^{N_{E_{\partial}}}(E_{\partial})_{i}.$  (21) 
 Then, for any functions $\boldsymbol{f}\in L^{2}(E_{\partial},\mathbb{R}^{n_{\partial}})$ and $\boldsymbol{\eta}\in L^{2}(E_{\partial},\mathbb{R}^{d})$ we can form vectors $\mathcal{D}^{\partial}\boldsymbol{f}\in\mathbb{R}^{n_{\partial}\times N_{E_{\partial}}}$ and $\mathcal{D}^{\partial}\boldsymbol{\eta}\in\mathbb{R}^{d\times N_{E_{\partial}}}$ by averaging these functions over each subset $(E_{\partial})_{i}$ . Consequently, any finite-dimensional map $\tilde{e}^{\partial}:\mathbb{R}^{(n_{\partial}+d)\times N_{E_{\partial}}}\rightarrow\mathbb{R}^{r}$ induces a boundary encoder $e^{\partial}:L^{2}(E_{\partial},\mathbb{R}^{n_{\partial}})\times L^{2}(E_{\partial},\mathbb{R}^{d})\rightarrow\mathbb{R}^{r}$ via the composition $e^{\partial}:=\tilde{e}^{\partial}\circ\mathcal{D}^{\partial}$ .

A summary of the constructions developed so-far is given in Table 1 . It should be emphasised that due to the semi-local role of the encoder generating function $e$ , it is not important to impose any particular structure on the partitions ( 20 ) or ( 21 ). The only requirement is to form a sufficiently resolved local approximation to the underlying data.

4.3 Decoder Generating Functions

We aim to create functions $d:\mathbb{R}^{r}\rightarrow C(D,\mathbb{R}^{n})$ . To do this, choose $N_{D}$ points $\{d_{i}\}_{i=1}^{N_{D}}\subset\mathbb{R}^{d}$ whose convex hull contains $D\in\mathbb{R}^{d}$ , and let $\mathcal{I}:\mathbb{R}^{N_{D}}\rightarrow C(D,\mathbb{R}^{n})$ be any interpolation operator, such as linear interpolation, which continuously extends known functional values at the points $d_{i}$ to the whole of $D$ . Then, any mapping $\tilde{d}:\mathbb{R}^{r}\rightarrow\mathbb{R}^{N_{D}}$ induces a decoder generating function via $d:=\mathcal{I}\circ\tilde{d}$ .

4.4 Elliptic system solution on training patches

Effective training of interior and boundary encoders requires a choice of training patches which provide a good sample of both the domain interior and the domain boundary. For simplicity, we only describe the case of rectangular domains $\Omega$ . In this case, by considering training patches $Q=\text{conv}\{\boldsymbol{q}_{i}\}\subset\bar{\Omega}$ whose boundary $\partial Q$ is also rectangular, these may be chosen to either lie entirely in the domain interior, or to coincide with a portion of the boundary $\partial\Omega$ .

After appropriate interpolation of the latent variables $\ell(\boldsymbol{q}_{i})$ to the rectangular boundary $\partial Q$ , standard second-order finite difference schemes can be used to solve the required elliptic system ( 18 ) on each training patch to obtain the prediction $\hat{\boldsymbol{\ell}}(\boldsymbol{p})$ at the training point $\boldsymbol{p}\in Q$ .

4.5 Cost function minimisation

Given an ensemble $\mathcal{U}$ of training data in the form of (17), and specified values for the latent space dimension $r$, partition dimensions $N_{E}$ and $N_{E_{\partial}}$ for the encoders, and decoder discretisation dimension $N_{D}$, the model training problem is now to determine the mappings $\tilde{e}:\mathbb{R}^{n\times N_{E}}\rightarrow\mathbb{R}^{r}$, $\tilde{e}^{\partial}:\mathbb{R}^{(n_{\partial}+d)\times N_{E_{\partial}}}\rightarrow\mathbb{R}^{r}$, $d:\mathbb{R}^{r}\rightarrow\mathbb{R}^{N_{D}}$, and a matrix $\mathcal{A}\in\mathbb{R}^{rd\times rd}$ that minimise the cost function (19).

We assume that each of the generating functions $\tilde{e},\tilde{e}^{\partial}$, and $d$ is a fully connected feed-forward neural network. A neural network with $L$ layers is a mapping $\mathcal{N}:\mathbb{R}^{m_{I}}\rightarrow\mathbb{R}^{m_{O}}$ composed of repeated applications of a nonlinear activation function $\phi:\mathbb{R}\rightarrow\mathbb{R}$ and affine maps $A_{i}:\boldsymbol{x}\mapsto W_{i}\boldsymbol{x}+\boldsymbol{b}_{i}$, where $i=1,\dots,L$. The affine maps are characterized by weights $W_{i}\in\mathbb{R}^{r_{i}\times r_{i-1}}$ and biases $\boldsymbol{b}_{i}\in\mathbb{R}^{r_{i}}$, with $r_{i}$ denoting the number of neurons in the $i^{\text{th}}$ layer, and $r_{0}=m_{I}$ and $r_{L}=m_{O}$ representing the input and output dimensions, respectively. The output of the $i^{\text{th}}$ layer is given by $\mathcal{N}_{i}(\boldsymbol{x}):=W_{i}\phi(\mathcal{N}_{i-1}(\boldsymbol{x}))+\boldsymbol{b}_{i}$ for $i\geq 2$, where $\phi$ acts component-wise, and $\mathcal{N}_{1}\boldsymbol{x}=W_{1}\boldsymbol{x}+\boldsymbol{b}_{1}$. The overall output of the neural network, after iterating through its $L$ layers, is $\mathcal{N}(\boldsymbol{x})=\mathcal{N}_{L}(\boldsymbol{x})$. Throughout this paper, we use the rectified linear unit (ReLU) activation function, defined as $\phi(x)=\text{ReLU}(x)=\left\{\begin{array}{rcl}x,&&x\geq 0,\\0&&x<0,\end{array}\right.$ This means that the tunable parameters of the considered neural networks are the weights and biases, collectively denoted as $\Theta=(W_{i},\boldsymbol{b}_{i})_{i=1}^{L}$. To emphasize the dependence on these parameters, we write $\mathcal{N}=\mathcal{N}_{\Theta}$.

The goal of model training is to find a quadruple $\mathcal{X}:=(e,e^{\partial},d,\mathcal{A})$ that minimises the modelling residual $\Psi(\mathcal{U},\mathcal{X})$ defined in (19). To ensure the positive definiteness of $\mathcal{A}$, as required by constraint (18), we introduce a logarithmic barrier function and reformulate the optimisation problem as $\min_{\Theta,\mathcal{A}}\Psi(\mathcal{U},\mathcal{X})-\rho\log{(\det{\mathcal{A}})}$, where $\Theta=(\Theta_{e},\Theta_{e^{\partial}},\Theta_{d})$, $\mathcal{A}\in\mathbb{R}^{dr\times dr}$, and $\rho>0$. The components of $\mathcal{X}$ are defined in terms of neural networks $\mathcal{N}$ with parameters $\Theta$, specifically $e=\tilde{e}\circ\mathcal{D}$, $e^{\partial}=\tilde{e}^{\partial}\circ\mathcal{D}^{\partial}$, and $d=\mathcal{I}\circ\tilde{d}$, where $\tilde{e}=\mathcal{N}_{\Theta_{e}}$, $\tilde{e}^{\partial}=\mathcal{N}_{\Theta_{e^{\partial}}}$, and $\tilde{d}=\mathcal{N}_{\Theta_{d}}$. The input and output dimensions of each neural network are determined by the encoder and decoder structure outlined in §§4.1-4.3, while the number of layers and neurons will be specified in §5 for the particular numerical examples considered. The $-\log{(\det{\mathcal{A}})}$ term acts as a barrier, promoting positive definiteness by penalising sign-changes in the eigenvalues of $\mathcal{A}$, as it approaches infinity when $\det{\mathcal{A}}$ approaches zero.

The optimisation problem ( 22 ) is solved using a stochastic gradient descent approach. At each iteration, a random set of training patches of the form described in § 4.4 are created. The gradient of cost function ( 19 ), dependent upon the chosen training patches, is then computed using automatic differentiation an appropriate step in the decision variables is taken. The Adam gradient-based optimization algorithm implemented in the TensorFlow package is used to identify a local minimum. Since solutions to the elliptic system ( 4 ) are invariant upon rescaling of $\mathcal{A}$ , after each iteration the elliptic decision variables are updated via $\mathcal{A}\mapsto\mathcal{A}/(\text{det}(\mathcal{A}))^{(1/(rd)^{2})}$ to maintain the value of the determinant to be unity. The process of random training patch selection and gradient-based weight updates is then iterated until the cost $\Psi$ has converged to a local minimum.

5 Numerical Examples

To demonstrate the performance of physics-informed neural networks (SINNs) in solving boundary observation problems, we consider two example nonlinear partial differential equations (PDEs).

5.1 A nonlinear heat equation

Let $\Omega=[0,1]\times[0,1]\subset\mathbb{R}^{2}$, and consider the PDE satisfied by $u(x,y)$: 
$\begin{split}\nabla\cdot(e^{u}\nabla u)&=0,\qquad\text{in}\;\Omega,\\
u&=g,\qquad\text{on}\;\partial\Omega.\end{split}$ 
This equation can be interpreted as the steady-state solution to a nonlinear diffusion equation with a diffusivity $e^{u(x,y)}$ that depends on the local solution $u(x,y)$. Alternatively, it is equivalent to the nonlinear PDE $\Delta u=-|\nabla u|^{2}$. The motivation for studying this example lies in its simplicity: the linearized PDE reduces to the Laplace equation $\Delta u=0$, allowing for a direct comparison between the SINN methodology and traditional approaches that utilize a system's linearized dynamics with an encoder/decoder architecture.

To obtain training and testing data, we first generate $10^{3}$ boundary functions $g_{i}\in L^{2}(\partial\Omega,\mathbb{R})$, with $900$ used for training and $100$ for testing. These boundary functions are created as random sums of sinusoids, where for each boundary point $\boldsymbol{z}=\boldsymbol{z}(x,y)\in\partial\Omega$, we define an angle $\alpha(\boldsymbol{z})$ measured anticlockwise from the centre of the square domain $\Omega$ as $\alpha(\boldsymbol{z}(x,y))=\text{atan2}\left(x-0.5,y-0.5\right)$. We then sample coefficients $X_{i},Y_{i}\sim N(0,1)$ from standard Normal distributions and compute $\tilde{g}_{i}(\boldsymbol{z})=\sum_{n=1}^{4}\frac{X_{n}\sin(n\alpha(\boldsymbol{z}))+Y_{n}\cos(n\alpha(\boldsymbol{z}))}{n}$, which introduces a random phase shift through the cosine component and moderately attenuates higher-order sinusoids to emphasize lower frequency data. Finally, we normalize each $\tilde{g}_{i}$ by a factor $\Delta_{g}$, which is randomly sampled from a triangular distribution with pdf $f(\Delta_{g})=\begin{cases}\Delta_{g}/8,&\text{if }0\leq\Delta_{g}\leq 4,\\ 0,&\text{otherwise},\end{cases}$ to obtain the final boundary function $g_{i}=\tilde{g}_{i}/\Delta_{g}$. This process yields an ensemble of boundary functions with large differences between their maximum and minimum values.

5.1.1 Numerical solution and SINN implementation

For each boundary data function $g_{i}\in L^{2}(\partial\Omega,\mathbb{R})$ , the PDE ( 23 ) is solved on a uniform grid $38\times 38$ grid with a Newton Linearization Method with finite difference equations to obtain solution data $u_{i}\in L^{2}(\Omega,\mathbb{R})$ . Given the computational domain discretisation, we view $\Omega$ as the union of $38\times 38$ square elements $\mathcal{T}=[1/38]\times[1/38]$ , with any solution $u(x,y)$ to ( 23 ) assumed to have a single value in each element.

We define an interior encoder as a square $E$ centered at the origin $(0,0)\in\mathbb{R}^{2}$, comprising $(2m_{e}+1)^{2}$ elements $\mathcal{T}$, where $m_{e}\in\mathbb{N}$. This construction ensures that the value of any interior latent variable at a point $(x,y)\in\Omega$ is determined solely by the solution values in the $(2m_{e}+1)^{2}$ elements that symmetrically surround $(x,y)\in\Omega$.

Boundary latent variables $\ell(\boldsymbol{z})$ are defined at the centre of each exterior element, using the construction described in Section 4.2 , with $E_{\partial}$ chosen to be a line segment, centred at $0\in\mathbb{R}$ , and formed of $N_{E_{\partial}}=2m_{e}+1$ line segments whose lengths are equal to the side length of the tile $\mathcal{T}$ . Consequently, boundary latent variables $\ell{(\boldsymbol{z})}$ depend on the boundary values $g(\boldsymbol{z})$ on the $2m_{e}+1$ tile boundaries symmetrically surrounding $\boldsymbol{z}\in\partial\Omega$ . Note that is has been assumed for simplicity that $N_{E}=N_{E_{\partial}}^{2}$ .

Decoders are constructed as described in § 4.3, where $D$ is a square centered at $(0,0)\in\mathbb{R}^{2}$, formed by the union of $N_{D}=(2m_{d}+1)^{2}$ tiles $\mathcal{T}$. These decoders aim to predict the solution $u(x,y)$ at a point $(x,y)\in\Omega$ using latent values, and do so symmetrically across $N_{D}$ tiles surrounding $(x,y)$. As the decoder output is square-shaped, the domain can be fully divided into non-overlapping regions, each corresponding to a decoder output, which is achieved using partition decoders as outlined in § 2.3.

The training data was divided into batches, each of which included samples from both the interior and boundary of the domain. To account for the square domain's geometry, a distinction was made between edge and corner training patches. Edge patches excluded the corner points at $(x_{c},y_{c})\in\left\{(0,0),(0,1),(1,0),(1,1)\right\}$, while corner patches included one of these points. Each batch consisted of 128 internal samples, 128 edge samples, and 32 corner samples, with the corner samples evenly distributed across the four sides of the square.

Finally, to implement the symmetric matrix $\mathcal{A}$ as an optimisation variable, we define matrices $P_{11},P_{12},P_{22}\in\mathbb{R}^{r\times r}$ , let $A_{ij}:=P_{ij}+P_{ij}^{\top}$ and form the block matrix $\mathcal{A}=(A_{ij})_{ij=1}^{2}\in\mathbb{R}^{4r^{2}}$ . The corresponding elliptic PDE for a latent variable function $\boldsymbol{\ell}:\Omega\rightarrow\mathbb{R}^{r}$ is then given by 
 
 $D_{\mathcal{A}}\boldsymbol{\ell}=A_{11}\boldsymbol{l}_{xx}+2A_{12}\boldsymbol{l}_{xy}+A_{22}\boldsymbol{l}_{yy}=0.$ 


We consider the performance of SINN models for two types of boundary data. In the first, for each boundary data function $g_{i}$ , we only assume that the boundary encoder can access pure boundary data via 
 
 $\boldsymbol{b}_{i}(\boldsymbol{z})=\left(g_{i}(\boldsymbol{z}),\boldsymbol{n}(\boldsymbol{z})\right),\qquad\boldsymbol{z}\in\partial\Omega,$  (24) 
 where at the corners of the square domain, the boundary vector is defined diagonally in an outward pointing manner (e.g. $(-\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}})$ for the north-west corner). Following this, we will also consider the case in which for each boundary data function $g_{i}$ , the boundary derivative of its associated solution $u_{i}$ is available to the encoder, by letting 
 
 $\boldsymbol{b}_{i}(\boldsymbol{z})=\left(g_{i}(\boldsymbol{z}),\frac{\partial u_{i}}{\partial\boldsymbol{n}}(\boldsymbol{z}),\boldsymbol{n}(\boldsymbol{z})\right),\qquad\boldsymbol{z}\in\partial\Omega,$  (25) 
 The code available for generating results is available at: https://github.com/jh6220/SINNs-for-boundary-observtion-problems.git

5.1.2 SINN performance with pure boundary data ( 24 )

Table 2 presents the mean square error between the test boundary functions solved by the SINN model $\mathcal{F}$ and the ground truth for various choices of latent space dimension $r$ and encoder and decoder complexities $N_{E}$ and $N_{D}$. The error is calculated using the formula $\mathcal{E}=\frac{1}{N_{\text{test}}}\sum_{i=1}^{N_{\text{test}}}\|u_{i}-\mathcal{F}(\boldsymbol{b}_{i})\|^{2}_{L^{2}(\Omega,\mathbb{R})}$, where the integrals are approximated as sums over the $38^{2}$ square elements of the domain $\Omega$. The SINN models used to obtain these results employ neural networks with 5 hidden layers of 60 nodes to parameterize each component, including the encoder, decoder, and boundary encoder.

It is evident that increasing either the latent dimension $r$ or the encoder complexity $N_{E}$ reduces the SINN error $\mathcal{E}$ . The former allows for a more complex latent space, while the latter effectively allows the encoder to access higher order derivatives of the underlying data. For example, when $N_{E}=3$ an encoder has access to nine local function values and is therefore has the potential to access approximate second-order derivatives. Conversely, increasing the decoder dimension $N_{D}$ increases the error $\mathcal{E}$ which occurs due to the choice of partition decoder used for this example. In this case, a increasing $N_{D}$ corresponds to requiring the decoder to extrapolate to a larger sets, naturally increasing $\mathcal{E}$ . However, if the number of degrees of freedom, i.e. $r/N_{D}^{2}$ , of a trained SINN are considered it can be seen from the penultimate row of Table 2 that a higher value of $N_{D}$ can possibly be viewed as computationally advantageous.

To examine the impact of the latent variable dimension $r$ on the trained internal elliptic models $D_{\mathcal{A}}$, we consider two cases. In the simplest scenario, where $r=1$, $N_{E}=1$, and $N_{D}=1$, the internal elliptic model simplifies to $1.608\,\ell_{xx}+0.001\,\ell_{xy}+1.613\,\ell_{yy}=0$, which closely resembles the linearised PDE $\Delta u=0$. In contrast, when more modelling degrees of freedom are available, as in the case where $r=3$, $N_{E}=3$, and $N_{D}=1$, the trained internal elliptic system becomes a non-trivial matrix equation, given by $\left(\begin{smallmatrix}1.46&0.49&-1.14\\ 0.49&1.57&-0.72\\ -1.14&-0.72&0.36\end{smallmatrix}\right)\ell_{xx}+\left(\begin{smallmatrix}0.01&-0.03&-0.02\\ -0.03&0.02&-0.02\\ -0.02&-0.02&0.06\end{smallmatrix}\right)\ell_{xy}+\left(\begin{smallmatrix}0.92&1.66&-1.30\\ 1.66&0.74&-1.58\\ -1.30&-1.58&1.69\end{smallmatrix}\right)\ell_{yy}=0$. Notably, the resulting error $\mathcal{E}$ for this more complex model is an order of magnitude lower than that of the simplest model.

We next examine the structure of the trained encoder and boundary encoders, starting with the simplest case where $r=1$, $N_{E}=1$, and $N_{D}=1$. Figure 6 illustrates the results for a selected test data pair $(u,\boldsymbol{b})$, including the SINN solution $\mathcal{F}(\boldsymbol{b})$ computed from the boundary data, the encoded latent variable $\epsilon u$, and the reconstructed latent variable $(\mathcal{E}_{\mathcal{A}}\circ\epsilon^{\partial})\boldsymbol{b}$. The figure also displays the error fields for both the SINN and internal latent variables, revealing small yet non-trivial discrepancies in (c) and (f).

Since the trained PDE is approximately equivalent the linearised PDE, and the latent variable space has scalar-values $(r=1)$ , it is not surprising that the encoded latent variable $\epsilon u$ and the original data $u$ are superficially similar. Model accuracy in this case is achieved purely from the nonlinearity of the boundary encoder and decoder. Indeed, Figure 7 (a–b) shows two slices of the SINN solution $\mathcal{F}(\boldsymbol{b})$ , ground-truth data $u$ , and the solution obtained by extending the boundary data using just the linearised PDE $\Delta u=0$ . It is clear that the nonlinear SINN solution is substantially more accurate than the linearised model. To understand the precise way in which the nonlinear structure achieves this increase in accuracy, Figure 7 (c) shows both the true boundary data $\boldsymbol{b}$ and the encoded data $\epsilon^{\partial}\boldsymbol{b}$ . The encoder $\epsilon^{\partial}$ behaves assymetricaly in the sense that it attenuates positive boundary values and amplifies negative ones. The reason for this behaviour is that the local diffusion coefficient $e^{u(x,y)}$ of the PDE ( 23 ) increases exponentially with the $u(x,y)$ . Thus, positive boundary values imply diffusion on shorter length scales compared to negative boundary values. The boundary encoder’s behaviour can now be interpreted as $\epsilon^{\partial}$ reflecting this local nonlinear structure of the underlying nonlinear diffusion coefficient, and this improves the accuracy of the SINN operator $\mathcal{F}$ . While this behaviour of the boundary encoder is now interpretable, there is a persistent error if a simple model with $r=1$ is used. As indicated in Table 2 , this error can be avoided by increasing the dimension $r$ of the latent space.

Finally, we seek to understand the geometric properties of the latent variables for cases in which $r=3$ and $r=5$ . While the latent variables do not have an strict physical meaning, one can use sensitive analysis to extract their underlying structure. In particular, Figure 8 shows 
 
 $\frac{\partial\delta}{\partial\boldsymbol{\ell}}(\bar{\boldsymbol{\ell}})$ 
 where $\bar{\ell}$ is the mean value of the latent variables computed across the entire data ensemble. The gradient was computed numerically using central finite difference method with a step size equal to a standard deviation of each latent dimension computed from the data-set in a similar manner to the mean.

The results of the sensitivity analysis for two with $(r=3,N_{E}=3,N_{D}=3)$ and $(r=5,N_{E}=3,N_{D}=3)$ are shown in Figure 8 . It can be observed that the implies latent variables are spatially coherent. In the case $r=3$ , the structures are approximately orthogonal linear surfaces, while for $r=5$ more complex, yet still coherent, spatial structures can be observed.

5.1.3 SINN performance with extended boundary data ( 25 )

We now consider the case in which extra boundary data, namely the normal boundary derivatives, are available to the boundary encoder. Table 2 shows SINN errors $\mathcal{E}$ for a variety of model choices. These follow a similar trend to the case of pure boundary conditions, although the final row of Table 5 indicates that extra boundary information gives a consistent performance improvement, and that this improvement is more pronounced for higher values of $N_{D}$ and of $r$ .

Figure 9 shows, for an test function $u$ whose error is indicative of the mean values presented in Table 5 , slices thought the true solution and SINN solutions. Both slices at $x=0$ and $y=0$ in Figure 9 (a–b) show very good agreement of the SINN solution $\mathcal{F}(\boldsymbol{b})$ with the true solution $u$ . Consider first the SINN models with $N_{D}=1$ , whose decoders are not required to extrapolate. It can be seen in both error plots of Figure 9 (c–d) that the pointwise error decreases uniformly as $r$ increases. Next, consider the SINN models with $N_{D}=3$ , whose decoders must extrapolate to two adjacent elements. In this case, while absolute error also decreases with increasing $r$ , the error plots are oscillatory as a result of the extrapolation error involved with the chosen partition decoder.

Finally, we discuss the influence of underlying neural network complexity on the SINN error. For the case $(r=5,N_{E}=3,N_{D}=3)$ , Table 4 shows the average SINN error $\mathcal{E}$ over the testing ensemble for different choices of Neural Network dimensions. Three different neural network structures are considered, in terms of the number of layers and nodes per layer, with each case applying to the encoder, boundary encoder and decoder. It is interesting to note that $\mathcal{E}$ decreases with neural network complexity, suggesting the model over fitting has not occurred for these parametric values. This highlights a potential benefit of the SINN methodology in that, due to the use of training patches, significant training information can be obtained from each training data pair $(\boldsymbol{u}_{i},\boldsymbol{b}_{i})$ . Consequently, the SINN approach appears robust to overfitting, even when employing only a relatively small training data ensemble.

5.2 Steady laminar fluid flow

Let $\Omega=[0,1]\times[0,1]\subset\mathbb{R}^{2}$ and suppose that $\boldsymbol{u}:\Omega\rightarrow\mathbb{R}^{2}$ and $p:\Omega\rightarrow\mathbb{R}$ satisfy the steady, incompressible, Navier-Stokes equations 
 
 $\displaystyle\begin{split}\boldsymbol{u}\cdot\nabla\boldsymbol{u}+\nabla p&=\nu\Delta\boldsymbol{u},\qquad\text{in}\;\Omega\\
\nabla\cdot\boldsymbol{u}&=0,\phantom{\Delta u}\qquad\;\text{in}\;\Omega,\\
\boldsymbol{u}&=\boldsymbol{g},\phantom{\Delta u}\qquad\;\text{on}\;\partial\Omega.\end{split}$  (26) 
 Here, $\boldsymbol{u}=(u_{x},u_{y})$ represents the velocity compents of a fluid contained in the square domain $\Omega$ , $p$ is the pressure of the fluid, and $\nu>0$ is the kinematic viscosity of the fluid. In this example, $\Omega$ should be thought of a control volume in a larger fluid flow. The corresponding velocity boundary conditions must, by the divergence theorem and incompressibility, then satisfy 
 
 $\int_{\partial\Omega}\boldsymbol{g}\cdot\boldsymbol{n}\,dS=\int_{\Omega}\nabla\cdot\boldsymbol{u}\,dV=0.$  (27) 


This example aims to investigate whether SINNs can reconstruct the fluid velocity in the domain $\Omega$ using only boundary velocity data, posing a more complex challenge than the previous nonlinear heat equation example due to the three-dimensional state space consisting of two velocity components and pressure. To further increase the difficulty, we will not utilize any pressure information, requiring the SINN to automatically discover its influence during training. Although not considered here, the SINN methodology can be similarly applied to fluid flow examples with no-slip boundary conditions, where the goal is to recover interior fluid velocity using only boundary pressure data.

A data ensemble is created using a method similar to that described in § 5.1, where $2\times 10^{3}$ random, sinusoidal boundary velocity functions $\boldsymbol{g}_{i}=((u_{x})_{i},(u_{y})_{i})$ are generated, each satisfying the constraint (27). For each of these boundary data functions, the PDE (26) is solved using a SIMPLE algorithm on a staggered grid, with the domain $\Omega$ discretized into $30\times 30$ rectangular cells, featuring pressure $p$ computed at each grid centre and velocity components $u_{x},u_{y}$ computed at the mid-point of the cell sides. To enhance numerical convergence, a non-uniform grid with increased resolution near the boundaries is employed. The resulting "ground-truth" solutions are then re-sampled onto a $38\times 38$ uniform grid, matching the properties outlined in § 4.2.

The generating functions and training loop was implemented in an equivalent manner to the example in § 5.1 . The only difference for this example is that encoders and boundary encoders take the two velocity components $u_{x},u_{y}$ as inputs. We note again, that pressure information is not available during training. Here, for brevity, we focus on case in which extra boundary information $\boldsymbol{b}=(\boldsymbol{u}_{|\partial\Omega}=\boldsymbol{g},\frac{\partial\boldsymbol{u}}{\partial\boldsymbol{n}}_{|\partial\Omega},\boldsymbol{n})$ is available. Each trained SINN model this section has the same architecture for generating functions $\epsilon$ , $\epsilon^{\partial}$ and uses a partition decoder $\delta$ . The models with $N_{D}=1$ have $5$ hidden layers of $60$ nodes, and models $N_{D}=3$ have $5$ hidden layers of $200$ nodes to compensate for the higher dimensional decoder output.

Table 5 shows the testing errors $\mathcal{E}$ of SINN models created with a selected parameter values. The errors exhibit the same trends as observed for the nonlinear heat equation in § 5.1 , with modelling error decreasing with increasing latent space dimension $r$ and encoder dimension $N_{E}$ , and with errors increasing as the extrapolation dimension $N_{D}$ of the partition decoder is increased. The final row of Table 5 shows the increase in error if only standard boundary conditions $(\boldsymbol{u}_{|_{\partial\Omega}},\boldsymbol{n})$ are available in model training although, for brevity, we do not discuss these results in detail here.

An indicative visualisation of the internal structure of two of the trained SINNs is given in Figure 10 for the cases $(r=6,N_{E}=3,N_{D}=1)$ and $(r=10,N_{E}=3,N_{D}=3)$ . The true test function velocity components $u_{x},u_{y}$ are shown in the top row. Both models give a solution to the boundary value problem $\mathcal{F}(\boldsymbol{b})$ which is a very good approximation to the true flow, as is shown in the right-hand column of Figure 10 . The main noticeable difference is that the SINN model with more latent variables $r=10$ is able to more accurately capture the elongated vertical structure corresponding to values $u_{y}<0$ . Both models have coherent, yet non-trivial, latent variables which are shown in the middle two columns of Figure 10 . The additional degrees of freedom enjoyed by the SINN with $r=10$ allows for more accurate reconstruction of the finer scale flow features than the simpler model with $r=6$ .

Figure 11 illustrates the performance of SINN models in capturing boundary observations of the Navier-Stokes PDE (26) for a representative example, showcasing slices through the SINN solutions $\mathcal{F}(\boldsymbol{b})$ at $x=0.5$ and $y=0.5$ for all models in Table 5. The results demonstrate that all models effectively approximate the primary trends of the true data, with the approximation error decreasing as the latent space dimension $r$ increases. Although modelling errors have not yet converged for the displayed parameter values, the reconstructed flow fields in Figures (10) reveal that all SINN models can accurately recreate the dominant internal vortex structures in the flow domain. This level of performance, if achieved in experimental applications, would be highly valuable in practice.

We conduct a latent variable sensitivity analysis on two trained models with parameters $(r=8,N_{E}=3,N_{D}=3)$ and $(r=10,N_{E}=3,N_{D}=3)$. The decoder sensitivities, $\frac{\partial\delta}{\partial\boldsymbol{\ell}_{i}}(\overline{\boldsymbol{\ell}})$, comprise two components corresponding to the velocity components in the $x$ and $y$ directions, as shown in Figure 12. Notably, the model with $r=8$ exhibits approximately planar sensitivities around the mean latent variable value $\bar{\boldsymbol{\ell}}$, indicating that the latent variables enable planar perturbations to the solution in four directions for each velocity component. In contrast, the model with $r=10$ displays nonlinear, yet spatially coherent, latent-variable sensitivities that allow for a more accurate solution to the underlying boundary observation problem, as evidenced by the more complex sensitivity patterns.

6 Discussion

The numerical examples in § 5 demonstrate that SINNs can effectively approximate nonlinear solution operators, specifically $\mathcal{F}:L^{2}(\partial\Omega,\mathbb{R}^{n_{\partial}})\rightarrow L^{2}(\Omega,\mathbb{R}^{n})$, for nonlinear boundary observation problems of the form (1). A key advantage of the SINN approach is its ability to learn nonlinear, infinite-dimensional operators $\mathcal{F}$ through training finite-dimensional neural networks, enabling the approximation of solutions $\mathcal{F}(\boldsymbol{b})\in L^{2}(\Omega,\mathbb{R}^{n})$ for any given boundary data function $\boldsymbol{b}\in L^{2}(\partial\Omega,\mathbb{R}^{n_{\partial}})$. This capability marks a significant improvement over traditional data-driven methods, where model training and the resulting models are tightly coupled to a specific instance of boundary data.

From the viewpoint of operator identification, the fact that ensemble errors in the range of $\mathcal{O}(10^{-3})$ to $\mathcal{O}(10^{-5})$ can be obtained by SINNs with very few latent variables ( $3\leq r\leq 10$ ) indicates that the approach has strong potential to be successfully applied to more complex examples. This is supported by the evidence, discussed in § 5.1 , that the semi-local structure of the SINN training algorithm endows the approach with significant robustness against over fitting. A further advantage of our data-driven approach is that SINNs can be obtained regardless of whether the available boundary data renders the underlying PDE boundary observation problem over- or under-determined. The data-driven operator $\mathcal{F}$ merely attempts to find an optimal approximation to the PDE solution, given the available training ensemble. We also emphasise that SINN training does not require knowledge of the underling PDE, meaning that our method can be applied to experimental data and subsequently used to solve unseen boundary conditions.

A natural question is to ask whether the approximation error will converge to zero with increased complexity of the trained SINN operator (e.g. as $r,N_{E}\rightarrow\infty$ , or with the complexity of the underlying neural networks). Since our aim is identify operators which solve nonlinear boundary observation problems which may have no closed-form solutions, and in view of the fact that linear elliptic systems are used as the central non-local building blocks of SINNs, it is unlikely that such convergence will hold in general. However, even without such a property, the numerical evidence presented in this paper suggests that SINNs can provide very good, low-complexity, approximations to nonlinear boundary observation problems which, furthermore, capture key physical features of the solution.

Viewing performance from an approximation accuracy philosophy is not out of line with the motivation for many well-established approaches to the simulation of complex nonlinear PDEs. For example, in fluid mechanics, if one numerically solves the Reynolds Averaged Navier Stokes (RANS) equations, there is no expectation that the solution will agree with a fully resolved direct numerical simulation (DNS) of the governing Navier-Stokes equations. However, in many practical cases, a RANS solution may provide sufficiently physical insights at a substantially reduced computational cost than DNS. A similar philosophy applies to more accurate, yet still approximate, numerical approaches such as Large Eddy Simulation (LES). From the perspective of creating low-cost SINN models, it should be noted that there is technically no limit to using of significantly larger choice of extrapolation dimension $N_{D}$ than those used in the numerical examples considered in this paper. Furthermore, even if a SINN is trained using a finely-resolved spatial grid, the fact that an elliptic PDE is identified implies that the SINN operator $\mathcal{F}=\delta\circ\mathcal{E}_{\mathcal{A}}\circ\epsilon^{\partial}$ can be implemented using an arbitrary resolution, and potentially low-cost, solution to the central elliptic system $\mathcal{D}_{\mathcal{A}}\boldsymbol{\ell}=0$ .

The computational cost of boundary encoding and internal decoding in SINNs can be high for large neural networks, but these operations can be easily parallelized and applied independently to each section of the domain. The computational complexity of decoding scales linearly with the number of internal points, $\mathcal{O}(n_{i})$, while boundary encoding scales more favourably, typically with $n_{b}\propto\sqrt{n_{i}}$ boundary points. In contrast, the latent elliptic system requires solving a linear system with $n_{i}\times r$ variables, which can be computationally expensive, scaling as $\mathcal{O}((n_{i}r)^{3})$ with direct methods like Cholesky decomposition. However, since SINNs provide approximate solutions, iterative methods with lower per-iteration complexity can be used. Even so, the encoding and decoding steps have a significantly lower computational cost, especially for large $n_{i}$, making SINNs scalable for large neural networks and enabling significant increases in modeling accuracy, as demonstrated in section §5.1.

The computational cost of training SINNs is a potential bottleneck, as updating the elliptic system coefficients $\mathcal{A}$ requires repeatedly solving a new elliptic system of PDEs on each training patch used to build the cost function $\Psi(\mathcal{U},\mathcal{X})$. However, this cost can be managed by using a fixed number of training patch geometries $Q$ and parallelizing the elliptic system solutions on each patch. For instance, if each training patch, such as the one shown in Figure 5, requires solving an elliptic system at $n_{p}$ internal points, this involves solving a linear system of the form $A(\mathcal{A},\boldsymbol{p}_{i},\boldsymbol{q}_{j})\boldsymbol{x}=\boldsymbol{b}(\mathcal{A},\boldsymbol{p}_{i},\boldsymbol{q}_{j},\boldsymbol{\ell}(\boldsymbol{q}_{j}))$, where $A$ is a symmetric positive definite matrix that depends on $\mathcal{A}$, boundary points, and interior points. The solution can be obtained in two steps: forming the Cholesky decomposition $A=LL^{\top}$ and then solving the system using $L$. By using a common training patch geometry, the decomposition only needs to be computed once per training iterate, with $L$ stored in memory, allowing for parallel computation across all patches. Similarly, evaluations of the encoder and decoder can also be parallelized, enabling efficient SINN training.

7 Conclusions

We have presented a data-driven method for solving boundary observation problems which identifies a solution operator which can approximate the PDE solution for arbitrary boundary data. The constructed models, referred to here as Structure Informed Neural Networks (SINNs), embed an elliptic system into a classical encoder/decoder Neural-Network architecture for reduced-order modelling. The use of elliptic systems, which are well-posed with respect to the global passage of problem data, enables very efficient model training to be performed on small patches of the underlying domain. Numerical evidence suggests that this endows the proposed SINN methodology with significant robustness to over-fitting.

This paper presents a methodology for solving boundary observation problems with time-independent conditions and fully known boundary data. Future research directions include extending this methodology to accommodate partial boundary data, integrating SINN operators into time-dependent algorithms for boundary observation, and applying the approach to more complex geometric domains.

8 Appendix

We present the proofs of the regularity results stated in the paper.

8.1 Proof of Lemma 1

Regularity of $\epsilon\boldsymbol{u}$ : Given $\boldsymbol{x},\boldsymbol{y}\in\Omega_{E}$ , note that 
 
 $|(\epsilon\boldsymbol{u})(\boldsymbol{x})-(\epsilon\boldsymbol{u})(\boldsymbol{y})|=|e(\boldsymbol{u}_{\boldsymbol{x}})-e(\boldsymbol{u}_{\boldsymbol{y}})|.$  (28) 
 Now, if $\boldsymbol{x}\rightarrow\boldsymbol{y}$ in $\Omega_{E}$ , then by a standard approximation argument, $\|\boldsymbol{u}_{\boldsymbol{x}}-\boldsymbol{u}_{\boldsymbol{y}}\|_{L^{2}(E)}\rightarrow 0$ . It then follows from ( 28 ) and the assumed continuity of the generating function $e$ that $(\epsilon\boldsymbol{u})(\boldsymbol{x})\rightarrow(\epsilon\boldsymbol{u})(\boldsymbol{y})$ , meaning that $(\epsilon\boldsymbol{u}):\Omega_{E}\rightarrow\mathbb{R}^{r}$ is continuous.

To prove the uniform boundedness of $\epsilon\boldsymbol{u}$, we first note that for any $\boldsymbol{u}\in L^{2}(\Omega)$, the following inequality holds: $\sup_{x\in\Omega_{E}}\|\boldsymbol{u}_{\boldsymbol{x}}\|_{L^{2}(E)}^{2} = \sup_{x\in\Omega_{E}}\int_{E}|u(\boldsymbol{x}+\boldsymbol{y})|^{2}d\boldsymbol{y} \leq \|\boldsymbol{u}\|^{2}_{L^{2}(\Omega)}$. Since the operator $e: L^{2}(E) \rightarrow \mathbb{R}^{r}$ is compact, it maps bounded subsets of $L^{2}(E)$ to bounded subsets of $\mathbb{R}^{r}$. Therefore, we have $\sup_{x\in\Omega_{E}}|(\epsilon\boldsymbol{u})(\boldsymbol{x})| = \sup_{x\in\Omega_{E}}|e(\boldsymbol{u}_{\boldsymbol{x}})|_{2} < \infty$, which implies that $\epsilon\boldsymbol{u}$ is uniformly bounded, and consequently, $\epsilon\boldsymbol{u} \in C(\Omega_{E}, \mathbb{R}^{r})$.

8.2 Proof of Lemma 2

Let $\boldsymbol{\ell}\in C(\Omega,\mathbb{R}^{r})$ and let $\epsilon>0$ . Let $\boldsymbol{x},\boldsymbol{z}\in\Omega$ and define sets $D_{\boldsymbol{x}\boldsymbol{y}}:=(D_{\boldsymbol{x}}\cap D_{\boldsymbol{z}}\cap\Omega)$ and 
 
 $D_{\boldsymbol{x}\setminus\boldsymbol{z}}=(D_{\boldsymbol{x}}\cap\Omega)\setminus D_{\boldsymbol{x}\boldsymbol{y}},\quad D_{\boldsymbol{z}\setminus\boldsymbol{x}}=(D_{\boldsymbol{z}}\cap\Omega)\setminus D_{\boldsymbol{x}\boldsymbol{y}}$ 
 and set volumes by 
 
 $c_{\boldsymbol{x}}=|D_{\boldsymbol{x}}\cap\Omega|,\quad c_{\boldsymbol{z}}=|D_{\boldsymbol{z}}\cap\Omega|.$ 
 For convenience, we also let $f(\cdot):=(\delta\boldsymbol{\ell})(\cdot)$ and $g_{\boldsymbol{y}}(\cdot):=d(\ell(\boldsymbol{y}))(\cdot)$ . Then, 
 
 $\displaystyle|f(\boldsymbol{x})-f(\boldsymbol{z})|$ $\displaystyle=\left|\frac{1}{c_{\boldsymbol{x}}}\int_{D_{\boldsymbol{x}}}g_{\boldsymbol{y}}(\boldsymbol{x}-\boldsymbol{y})d\boldsymbol{y}-\frac{1}{c_{\boldsymbol{z}}}\int_{D_{\boldsymbol{z}}}g_{\boldsymbol{y}}(\boldsymbol{z}-\boldsymbol{y})d\boldsymbol{y}\right|$ 
 
 $\displaystyle\leq\underbrace{\frac{1}{c_{\boldsymbol{x}}}\int_{D_{\boldsymbol{x}\setminus\boldsymbol{z}}}|g_{\boldsymbol{y}}(\boldsymbol{x}-\boldsymbol{y})|d\boldsymbol{y}+\frac{1}{c_{\boldsymbol{z}}}\int_{D_{\boldsymbol{z}\setminus\boldsymbol{x}}}|g_{\boldsymbol{y}}(\boldsymbol{z}-\boldsymbol{y})|d\boldsymbol{y}}_{:=I_{1}}$ 
 
 $\displaystyle\quad+\underbrace{\frac{1}{c_{\boldsymbol{x}}}\int_{D_{\boldsymbol{x}\boldsymbol{z}}}\left|g_{\boldsymbol{y}}(\boldsymbol{x}-\boldsymbol{y})-g_{\boldsymbol{y}}(\boldsymbol{z}-\boldsymbol{y})\right|d\boldsymbol{y}}_{:=I_{2}}$ 
 
 $\displaystyle\quad+\underbrace{\left|\frac{1}{c_{\boldsymbol{x}}}-\frac{1}{c_{\boldsymbol{z}}}\right|\int_{D_{\boldsymbol{x}{\boldsymbol{z}}}}|g_{\boldsymbol{y}}(\boldsymbol{z}-\boldsymbol{y})|d\boldsymbol{y}}_{:=I_{3}}$ 
 Now, since $\boldsymbol{\ell}\in C(\Omega,\mathbb{R}^{r})$ , it follows that $\boldsymbol{\ell}(\Omega)\subset\mathbb{R}^{r}$ is bounded. Then, using compactness of the encoder generating function $d$ , it follows that $\{d(\boldsymbol{\ell})(\boldsymbol{y})\}_{\boldsymbol{y}\in\Omega}=\{g_{\boldsymbol{y}}\}_{\boldsymbol{y}\in\Omega}$ is a bounded subset of $C(\Omega,\mathbb{R}^{n})$ . Hence, there exists $K>0$ such that 
 
 $\sup_{\boldsymbol{y}\in\Omega}\|g_{\boldsymbol{y}}\|_{C(\Omega,\mathbb{R}^{n})}\leq K<\infty.$  (29) 
 Then, since $D_{\boldsymbol{x}\setminus\boldsymbol{z}},D_{\boldsymbol{z}\setminus\boldsymbol{x}}\rightarrow 0$ and $c_{\boldsymbol{x}}-c_{\boldsymbol{z}}\rightarrow 0$ as $\boldsymbol{x}\rightarrow\boldsymbol{z}$ , it follows that there exists $\delta_{1}>0$ such that 
 
 $I_{1}+I_{3}\leq K\left(|D_{\boldsymbol{x}\setminus\boldsymbol{z}}|+|D_{\boldsymbol{z}\setminus\boldsymbol{x}}|+|D_{\boldsymbol{x}\boldsymbol{z}}|\left|\frac{1}{c_{\boldsymbol{x}}}-\frac{1}{c_{\boldsymbol{z}}}\right|\right)<\frac{\epsilon}{2}.$ 
 whenever $|\boldsymbol{x}-\boldsymbol{y}|<\delta_{1}$ .

Since $d$ is continuous and $\boldsymbol{\ell}(\bar{\Omega})\subset\mathbb{R}^{d}$ is compact, the set of functions $\mathcal{F}:=\{g_{\boldsymbol{y}}\}_{\boldsymbol{y}\in\bar{\Omega}}$ forms a compact subset of $C(\Omega,\mathbb{R}^{n})$. As a result, $\mathcal{F}$ is equicontinuous, which implies that there exists a $\delta_{2}>0$ such that for any $\boldsymbol{y}\in\Omega$, the inequality $|\boldsymbol{g}_{y}(\boldsymbol{x}-\boldsymbol{y})-g_{\boldsymbol{y}}(\boldsymbol{z}-\boldsymbol{y})|<\epsilon/2$ holds whenever $|\boldsymbol{x}-\boldsymbol{z}|<\delta_{2}$. This leads to the conclusion that $|f(\boldsymbol{x})-f(\boldsymbol{z})|\leq I_{1}+I_{2}+I_{3}\leq\epsilon$ whenever $|\boldsymbol{x}-\boldsymbol{z}|<\min\{\delta_{1},\delta_{2}\},$ demonstrating the continuity of $f=\delta\boldsymbol{\ell}$. Furthermore, the continuity of $\delta\boldsymbol{\ell}$ in $C(\Omega,\mathbb{R}^{n})$ is ensured by the upper bound (29).

References

Bevanda et al. [2021] P. Bevanda, S. Sosnowski, and S. Hirche. Koopman operator dynamical models: Learning, analysis and control. Annual Reviews in Control, 52:197–212, 2021.
Bryan and Caudill [1996] K. Bryan and L. F. Caudill. An inverse problem in thermal imaging. SIAM Journal on Applied Mathematics, 56(3):715–735, 1996.
Giaquinta and Martinazzi [2012] M. Giaquinta and L. Martinazzi. An Introduction to the Regularity Theory for Elliptic Systems, Harmonic Maps and Minimal Graphs. Publications of the Scuola Normale Superiore (PSNS). Springer, 2012.
Illingworth et al. [2018] S. J. Illingworth, J. P. Monty, and I. Marusic. Estimating large-scale structures in wall turbulence using linear models. Journal of Fluid Mechanics, 842:146–162, 2018.
Kovachki et al. [2023] N. Kovachki, Z. Li, B. Liu, K. Azizzadenesheli, K. Bhattacharya, A. Stuart, and A. Anandkumar. Neural operator: Learning maps between function spaces with applications to pdes. Journal of Machine Learning Research, 24(89):1–97, 2023. URLhttp://jmlr.org/papers/v24/21-1524.html.
Li et al. [2017] Q. Li, F. Dietrich, E. M. Bollt, and I. G. Kevrekidis. Extended dynamic mode decomposition with dictionary learning: A data-driven adaptive spectral decomposition of the Koopman operator. Chaos: An Interdisciplinary Journal of Nonlinear Science, 27(10):103111, 2017.
Li et al. [2023] X.-A. Li, Z.-Q. J. Xu, and L. Zhang. Subspace decomposition based dnn algorithm for elliptic type multi-scale pdes. Journal of Computational Physics, 488:112242, 2023.
Li et al. [2021] Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anandkumar. Fourier neural operator for parametric partial differential equations. arXiv, 2010.08895, 2021.
Lu et al. [2021] L. Lu, P. Jin, G. Pang, Z. Zhang, and G. E. Karniadakis. Learning nonlinear operators via deeponet based on the universal approximation theorem of operators. Nat. Mach. Intell., 3:218–229, 2021.
Lusch et al. [2018] B. Lusch, J. Kutz, and S. Brunton. Deep learning for universal linear embeddings of nonlinear dynamics. Nat. Commun., 9:4950, 2018.
Morrey Jr. and Nirenberg [1957] C. B. Morrey Jr. and L. Nirenberg. On the analyticity of the solutions of linear elliptic systems of partial differential equations. Communications on Pure and Applied Mathematics, 10(2):271–290, 1957.
Pilva and Zareei [2022] P. Pilva and A. Zareei. Learning time-dependent pde solver using message passing graph neural networks. arXiv, 2204.07651, 2022.
Raissi et al. [2019] M. Raissi, P. Perdikaris, and G. Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378:686–707, 2019.
Schmid [2022] P. J. Schmid. Dynamic mode decomposition and its variants. Annual Review of Fluid Mechanics, 54(1):225–254, 2022.
Snieder [1988] R. Snieder. Large-scale waveform inversions of surface waves for lateral heterogeneity: 1. theory and numerical examples. Journal of Geophysical Research: Solid Earth, 93(B10):12055–12065, 1988. doi: https://doi.org/10.1029/JB093iB10p12055.
Song et al. [2022] Y. Song, L. Shen, L. Xing, and S. Ermon. Solving inverse problems in medical imaging with score-based generative models. InICLR-22. 10th International Conference on Learning Representations, 2022. doi: arXiv:2111.08005v2.
Wynn et al. [2013] A. Wynn, D. S. Pearson, B. Ganapathisubramani, and P. J. Goulart. Optimal mode decomposition for unsteady flows. Journal of Fluid Mechanics, 733:473–503, 2013.
