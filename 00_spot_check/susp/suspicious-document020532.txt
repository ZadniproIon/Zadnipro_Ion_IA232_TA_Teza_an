CustomNet: Zero-shot Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models

By Ziyang Yuan and Mingdeng Cao and Xintao Wang and Zhongang Qi and Chun Yuan and Ying Shan

Abstract

This research examines the field of multi-view image synthesis, focusing on generating specific image components or entire scenes that align visually with reference images. The study divides this task into two categories: local synthesis, which utilizes structural cues from reference images (similar to reference-based inpainting or Ref-inpainting), and global synthesis, which creates entirely new images based solely on reference examples (Novel View Synthesis or NVS). In the past, Text-to-Image (T2I) generative models have gained traction in numerous sectors. However, applying them to multi-view synthesis presents difficulties due to the complex interdependencies between reference and target images. To tackle these challenges effectively, we present Attention Reactivated Contextual Inpainting (ARCI), a unified approach that reformulates both local and global reference-based multi-view synthesis as contextual inpainting, enhanced with attention mechanisms found in existing T2I models. In essence, self-attention is employed to learn feature relationships across different reference views, while cross-attention is used to manage the generation through prompt tuning. ARCI's contributions, built upon the StableDiffusion model fine-tuned for text-guided inpainting, include efficiently handling complex multi-view synthesis tasks with off-the-shelf T2I models, introducing task and view-specific prompt tuning for generative control, achieving end-to-end Ref-inpainting, and implementing block causal masking for autoregressive NVS. The versatility of ARCI is demonstrated by extending it to multi-view generation for enhanced consistency with the same architecture, a claim supported by extensive experiments. The codes and models will be available at https://github.com/ewrfcas/ARCI.

1 Introduction

Of late, Text-to-Image (T2I) generative models have garnered considerable interest across a multitude of disciplines, finding utility in a wide array of applications ranging from personalization Gal et al. [2022], controllable image-to-image generation Zhang and Agrawala [2023], to 3D generation Poole et al. [2023]. Despite their potential, it might appear natural to utilize T2I generative models to tackle multi-view synthesis directly by fine-tuning additional adaptors with zero-initialization, as demonstrated in preceding research Hu et al. [2021]. Nevertheless, these adaptor-based fine-tuning strategies exhibit inherent limitations. They falter in capturing minute correlations, encompassing object orientations and precise object positions, between reference and target images. Such intricate details are vital for tasks such as multi-view generation, as evidenced by Ref-inpainting. Moreover, employing T2I models to handle image-to-image tasks necessitates guidance based on images instead of text. Consequently, certain methods substitute textual encoders with visual ones and fine-tune them comprehensively for the complete T2I model. This transformation frequently requires substantial computational resources, with training periods extending to hundreds of GPU hours, as in Yang et al. [2023]. However, training these extensive T2I models with unaccustomed visual encoders can be computationally taxing and challenging to converge, particularly when managing small batch sizes. Furthermore, most visual encoders, such as image CLIP Radford et al. [2021], primarily focus on the learning of semantic features rather than the intricate spatial details indispensable for multi-view synthesis tasks.

Consequently, researchers have started exploring encoder-based methods Yang et al. (2023) . These methods only require training an encoder to explicitly represent visual concepts of objects. Once trained, the concept embeddings obtained by encoding the image can be directly fed into the denoising process during inference, achieving a speed comparable to the standard diffusion model sampling process. However, simply injecting an image into a compressed concept embedding often leads to inadequate identity preservation Yang et al. (2023) . To address this issue, several methods have been proposed to enhance detail preservation by introducing local features Wei et al. (2023) or spatial details Chen et al. (2023b) . Despite these improvements, a closer examination of the results produced by these methods reveals a prevalent copy-pasting effect, i.e. , the objects in the synthesized image and source image are identical. The only variations observed stem from basic data augmentations applied during training, such as flipping and rotation. This limitation makes it difficult to achieve harmonized results with the background and negatively impacts output diversity.

The challenge of Novel View Synthesis (NVS) from a single image is complex, necessitating a deep understanding of geometry and textural representation [2021]. Previous approaches have attempted to address this issue through single view 3D reconstruction [2017], 2D generative models [2019], feature cost volumes [2023], and GAN-based methods [2020]. Despite these efforts, they are still limited by their inability to generalize well or handle small angle variations. Recent developments have focused on integrating 2D diffusion-based T2I model knowledge into 3D reconstruction [2023], reducing the need for 3D annotated data. In contrast, Zero123 [2023b] employs a different image CLIP encoder to incorporate image features from the reference view for NVS. While Zero123 enables the use of T2I models for view synthesis, it requires a large batch size and expensive computational resources to stabilize training with an unspecified reference image encoder. Additionally, the image encoder in Zero123 only supports one reference image, preventing the generation of consistent multi-view images.

This research explores the complex field of multi-view image synthesis, primarily concentrating on the production of particular image elements or entire scenes, using reference images as a base. The goal is to create specific elements or even whole images through these references, while maintaining visual consistency in terms of geometry, color, and texture between the reference and synthesized images. The task can be divided into two main categories: local and global multi-view image synthesis from reference images. Local synthesis involves the creation of specific image segments by aligning with the structural cues found locally in the reference images, similar to Reference-guided inpainting (Ref-inpainting) Zhou et al., 2021. On the other hand, global synthesis aims to generate entirely new images, inspired solely by reference examples, as demonstrated in Novel View Synthesis (NVS) Liu et al., 2023b. In this study, we present a unified approach to address this task by reusing attention priors derived from extensive text-to-image models, Rombach et al., 2022.

We summarize our contributions as follows: (1) . In contrast to previous customization approaches that predominantly rely on 2D input images, we propose CustomNet to explicitly incorporate 3D novel view synthesis capabilities ( e.g. , Zero-1-to-3) into the object customization process. This allows for the adjustment of spatial position relationships and viewpoints, leading to improved identity preservation and diverse outputs. (2) . Our approach features intricate designs that enable location control and flexible background control, addressing inherent limitations in Zero-1-to-3, such as simplistic white backgrounds, exclusively centered objects, and overly synthetic effects. (3) . We design a dataset construction pipeline that effectively utilizes synthetic multiview data and massive natural images to better handle real-world objects and complex backgrounds. (4) . Equipped with the aforementioned designs, our method enables zero-shot object customization without test-time optimization while controlling location, viewpoints, and background simultaneously. This results in enhanced identity preservation and diverse, harmonious outputs.

2 Related works

Object customization in diffusion models . With the promising progress of text-to-image diffusion models Sohl-Dickstein et al. (2015) , researches explore to capture the information of a reference object image and maintain its identity throughout the diffusion model generation process, i.e. , object customization. These methods can be broadly classified into optimization-based techniques Ruiz et al. (2023) and encoder-based approaches Yang et al. (2023) . Optimization-based methods can achieve high-fidelity identity preservation; however, they are time-consuming and may sometimes result in overfitting. In contrast, current encoder-based methods enable zero-shot performance but may either lose the identity or produce trivial results resembling copy-pasting. In contrast, our proposed CustomNet aims to preserve high fidelity while supporting controllable viewpoint variations, thereby achieving more diverse outcomes.

Image harmonization . In image composition, a foreground object is typically integrated into a given background image to achieve harmonized results. Various image harmonization methods Sunkavalli et al. (2010) have been proposed to further refine the foreground region, ensuring more plausible lighting and color adjustments Xue et al. (2022) . However, these methods focus on low-level modifications and are unable to alter the viewpoint or pose of the foreground objects. In contrast, our proposed CustomNet not only achieves flexible background generation using user-provided images but also offers additional viewpoint control and enhanced harmonization.

In the realm of T2I (Text-to-Image) models, advancements have led to spectacular visual renderings, with potential for expansion into local editing, as demonstrated by Avrahami et al. (2022). However, these models are limited in their control, being manipulated solely by natural language. The shortcoming of these models lies in their inability to generate images with precision in terms of textures, locations, identities, and aesthetics, as highlighted by Gal et al. (2022). To address this issue, textual inversion (Gal et al., 2022) and fine-tuning techniques (Ruiz et al., 2022) have been proposed for personalized T2I. Simultaneously, there is a growing focus on image-guided generations (Voynov et al., 2022). Models such as ControlNet (Zhang and Agrawala, 2023) and T2I-Adapter (Mou et al., 2023) employ trainable adapters (Houlsby et al., 2019) to incorporate visual cues into pre-trained T2I models without compromising generalization and diversity. Nevertheless, these methods are only effective for basic style transfers. More intricate spatial tasks, like Ref-inpainting, pose a challenge for ControlNet, as evidenced in Section 4. In contrast, T2I-based exemplar-editing and Neural Variational Systems necessitate fine-tuning on extensive datasets with robust data augmentation (Yang et al., 2023) and large batch sizes (Liu et al., 2023b). In comparison, ARCI offers a unique combination of spatial modeling capacity and computational efficiency.

3 Proposed Method

Introduction. Our goal is to produce a customized image, $\hat{x}$, featuring an object of the same identity as a given reference image, $x$, with dimensions $H \times W \times 3$. The objective is to place this object harmoniously into a desired background, adjusting for viewpoint and location variations. To achieve this, we begin with a background-free image, which can be obtained using methods such as segmentation, as described in SAM Kirillov et al. (2023).


As illustrated in Fig. 2 , we propose CustomNet, a novel architecture designed to achieve this customization conditioned on the viewpoint $[R,T]$ (where $R$ and $T$ represent the relative camera rotation and translation of the desired viewpoint, respectively), object location $L$ , and background condition $B$ . The background can be controlled either through textual descriptions or by providing a specific user-defined image $x_{bg}$ . : 
 
 $\hat{x}=\textsc{CustomNet}(x,[R,T],L,B).$  (1) 


The masking strategy employed in Figure 9 (b) has been questioned due to shape leakages, resulting in unreliable metrics as depicted in Table 2. It is essential to note that our method demonstrates efficient performance when utilizing the reference mask, which can be easily obtained through the salient object detection technique by Qin et al. [2020]. In detail, we expand the reference mask, as demonstrated in Figure 9 (b). Subsequently, a few DDIM steps, as outlined by Song et al. [2020], are executed to generate a preliminary synthesis in the target view. Following this, we identify the foreground mask based on the preliminary synthesis, also by Qin et al. [2020], and further expand this mask for the second synthesis using complete DDIM steps. The adaptive masking strategy can effectively be applied to the NVS, as demonstrated in Figure 11. All the testing results presented in this paper are based on adaptive masking. Furthermore, we propose that the provision of target masks according to distance and direction priors, done manually, could also be a viable solution to tackle the challenging single-view NVS.

Moreover, we offer flexible background control through a textual description or a specific background image. This leads to improved identity preservation and diverse, harmonious outputs.

This paper introduces ARCI, a method that formulates reference-based multi-view image synthesis as inpainting tasks and addresses them end-to-end. The efficiency of ARCI is attributed to the adaptive prompt tuning and attention modules found in large T2I models, enabling it to handle complex reference-based inpainting and non-volatile synthesis (NVS) tasks with ease. Furthermore, ARCI is versatile and can be extended to handle multi-view generation tasks. Additionally, we propose block casual masking to achieve autoregressive NVS with consistent results. Extensive experiments on reference-inpainting and NVS demonstrate the efficacy of ARCI.

3.1 Control the Viewpoint and Location of Objects

Viewpoint Control. To enable synthesizing a target customized image complied with the given viewpoint parameter $[R,T]$ , we follow the view-conditioned diffusion method introduced by Zero-1-to-3. As shown in Fig. 2 (the left-top part), we first apply a pre-trained CLIP Radford et al. (2021) image encoder to encode the reference background-free object image into an object embedding, containing high-level semantic information of the input object. Then the object embedding is concatenated with $[R,T]$ and passed through a trainable lightweight multi-layer perception (MLP). The fused object embedding further passes to the denoising UNet as a condition with the cross-attention mechanism to control viewpoints of the synthesized images.

Location Control. We further control the object location in the synthesized image by concatenating the reference object image with the desired location and size to the UNet input. The process is illustrated in Fig. 2 (the ‘Generation’ branch). The desired location and size $L$ , represented as a bounding box $[x,y,w,h]$ , are given by the user. Then, we resize the reference object image into the size of $[w,h]$ and place its left-top corner at the $[x,y]$ coordinate of a background-free image (this image is the same size as the target image being denoised but without background). The additional concatenated reference object image helps the model synthesize the desired image while keeping the identity and the texture details Rombach et al. (2022) . Note that Zero-1-to-3 directly concatenates the centrally-located reference object to the UNet input, which can only synthesize an image where the object is centered. Our method enables synthesizing the target object at the desired position with the proposed explicit location control.

3.2 Flexible Background Control by Text or Reference Images

It has been proposed that the implementation of the masking strategy depicted in Figure 9b may encounter issues related to shape leakages, which in turn can produce unreliable results as observed in Table 2. It is essential to acknowledge that our method demonstrates optimal performance exclusively when utilizing the reference mask, which can be effortlessly obtained through the salient object detection methods outlined by Qin et al. [2020]. To elucidate, we initially expand the reference mask as shown in Figure 9b. Subsequently, a series of DDIM steps, as detailed by Song et al. [2020], are employed to create a preliminary synthesis in the desired view. Following this, we identify the foreground mask based on the preliminary synthesis, again using the methods of Qin et al. [2020]. This mask is then dilated for a second synthesis, incorporating the full set of DDIM steps.

The adaptive masking strategy described above can be effectively generalized across various non-rigid surfaces (NVS), as demonstrated in Figure 11. All experimental results presented in this paper are predicated upon the adaptive masking approach. Moreover, we posit that supplying target masks based on distance and directional priors manually could further strengthen the argument for addressing the arduous single-view NVS challenge.

Generation-based Background Control. In this mode, the diffusion model takes $[\mathbf{z}_{t-1},\text{{object w/ loc}}]$ as inputs, where $\mathbf{z}_{t-1}$ represents the noisy latent at the time step $t{-}1$ . The diffusion model is required to generate an appropriate background based on the textual description. Different from Zero-1-to-3, which solely accepts the object embedding without textual descriptions for background, we propose a novel dual cross-attention conditioning strategy that accepts both the fused object embedding with viewpoint control and textual descriptions for background. The dual cross-attention mechanism integrates the fused object embedding and the textual embedding through two distinct cross-attention modules. Specifically, we first employ the CLIP text encoder to obtain the textual embeddings and subsequently inject them into the denoising UNet, along with the fused object embedding, using the DualAttn : 
 
 $\textsc{DualAttn}(Q,K_{o},V_{o},K_{b},V_{b})=\text{Softmax}(\frac{QK_{o}^{T}}{\sqrt{d}})V_{o}+\text{Softmax}(\frac{QK_{b}^{T}}{\sqrt{d}})V_{b},\vspace{-0.1cm}$  (2) 
 where the query features $Q$ come from the UNet, while $K_{o},V_{o}$ are the object features projected from fused object embeddings with viewpoint control, and $K_{b},V_{b}$ are the background features from the textural embeddings. $d$ is the dimension of the aforementioned feature embeddings. During training, we randomly drop the background text description to disentangle the viewpoint control and background control. This straightforward yet effective design enables us to achieve accurate background control without affecting the object viewpoint control.

Composition-based Background Control. In many practical scenarios, users desire to seamlessly insert objects into pre-existing background images with specific viewpoints and locations. Our proposed CustomNet is designed to accommodate this functionality. More precisely, we extend the input channels of the UNet by concatenating the provided background image channel-wise, adhering to the Stable Diffusion inpainting pipeline. Consequently, the diffusion model accepts $[\mathbf{z}_{t-1},\text{{object w/ loc}},x_{bg}]$ as inputs. Note that in this mode, the textual description is optional, allowing for straightforward input of NULL to the text prompt. In comparison to existing image composition methods Yang et al. (2023) which often struggle with copy-pasting artifacts or identity loss issues, our method offers viewpoint and location control over objects.

3.3 Training Strategies

For the purpose of Ref-inpainting, we utilize 512 $\times$ 512 image pairs obtained from MegaDepth Li and Snavely [2018], featuring a variety of well-known scenes sourced from the internet. To strike a balance between image similarity and inpainting complexity, we select image pairs that share a 40% to 70% overlap, resulting in approximately 80k images and 820k pairs. We validate Ref-inpainting by incorporating manual masks from ETH3D scenes Schops et al. [2017] to assess the model's generalization. For the NVS, we utilize the 800k diverse scenes rendered by Liu et al. [2023b] from Objaverse Deitke et al. [2022], which are accompanied by object masks. All images are resized to 256 $\times$ 256, following the resizing method used by Liu et al. [2023b]. It's worth mentioning that extreme views with elevation angles below -10 \textsuperscript{∘} were filtered due to excessive ambiguity. Detailed information about the dataset and masking process can be found in Appendix A.2.

To alleviate this problem, we propose a training data construction pipeline that is the reverse of the above-mentioned way, i.e. , directly utilizing natural images as the target image and extracting objects from the image as the reference. Specifically, for a natural image, we first segment the foreground object using SAM model Kirillov et al. (2023) . Then we synthesize a novel view of the object using Zero-1-to-3 with randomly sampled relative viewpoints. The textual description of the image can be also obtained using the BLIP2 model. In this way, we can synthesize a large amount of data pairs from natural image datasets, like OpenImages Kuznetsova et al. (2020) . Meanwhile, the model trained with these data can synthesize more harmonious results with these natural images. More details are in the Appendix.

In the context of the NVS (Neural View Synthesis), we expand the object mask and generate random sampling points within the expanded mask's bounding box to create the irregular mask. This expanded mask is then merged with the target images to ensure complete coverage, as demonstrated in Fig. 9 (b). Our findings indicate that local masking remains crucial for quick convergence and consistent fine-tuning, a conclusion supported by experimental results. The dataset processing for Objaverse, as detailed in Deitke et al. [2022] and Liu et al. [2023b], consists of 800k images featuring diverse scenes accompanied by object masks. For each scene, 12 images are generated in a 256 $\times$ 256 resolution, with varying viewpoints. Adhering to Liu et al. [2023b], the spherical coordinate system is employed to transform the relative pose $\Delta p$ into the polar angle $\theta$, azimuth angle $\phi$, and radius $r$ at a distance from the canonical center, represented as $\Delta p=(\Delta\theta,\mathrm{sin}\Delta\phi,\mathrm{cos}\Delta\phi,\Delta r)$. The azimuth angle is encoded sinusoidally to address the non-continuity. In the masking process for Objaverse images, we dilate the object mask and its related bounding box with a kernel size ranging from 10 to 25 and a percentage from 5% to 20 respectively. Subsequently, we randomly select 20 to 45 points to paint the irregular masks.

4 Experiments

In the realm of artistic image generation, we have developed our ARCI, an advanced model inspired by the fine-tuned StyleGAN-R2 of Rombach et al. [2022], boasting a robust 0.8 billion parameters. In the process of task and perspective customization, we employ a flexible array of 50 trainable prompt tokens. Ninety percent of these tokens are dedicated to encoding the task-specific embeddings, while the remaining 10% serve to denote distinct perspectives.

For the optimization of ARCI, we employ the AdamW optimizer with a weight decay of 0.01. The learning rate for the Ref-inpainting phase is set at 3e-5. Of the masks employed, 75% are randomly generated, while the remaining 25% are generated through a matching-based algorithm (refer to Appendix A.2.1 for more details).

In the context of the Neighborhood View Sampling (NVS), ARCI can be evaluated using an adaptive masking mechanism bolstered by a foreground segmentation model, as outlined in Appendix A.3.1. ARCI demonstrates a remarkable ability to converge, achieving this with a batch size of 48 and a learning rate of 1e-5.

Our multi-view ARCI is a refinement of the one-view version, with additional fine-tuning. Additionally, we have trained a multi-view ARCI with a batch size of 512 and a learning rate of 3e-5 for the NVS (refer to Appendix A.2.3 for more details).

We compare our CustomNet to the optimization-based methods Textual Inversion Gal et al. (2022) , Dreambooth Ruiz et al. (2023) , and encoder-based (zero-shot) method GLIGEN Li et al. (2023c) , ELITE Wei et al. (2023) , BLIP-Diffusion Li et al. (2023a) . We use their official implementation (for GLIDEN, ELITE, and BLIP-Diffusion) or the diffuser implementations von Platen et al. (2022) (for Textual Inversion, Dreambooth) to obtain the results. Note that Dreambooth requires several images of the same object to finetune.

Figure 3 shows the images generated with different methods (more results are in the Appendix). We see that the zero-shot methods GLIGEN, ELITE, BLIP-Diffusion, and the optimization-based method Textual Inversion are far from the identity consistent with the reference object. Dreambooth and the proposed CustomNet achieve highly promising harmonious customization results, while our method allows the user to control the object viewpoint easily and obatain diverse results. In addition, our method does not require time-consuming model fine-tuning and textual embedding optimization.

The process of merely superimposing reference images and masked targets within the spatial space and self-attention mechanisms did not establish a robust correlation between them, resulting in the inability to produce the intended target results. Text-directed image repair necessitates image-specific textual prompts to steer the diffusion model towards the desired generation. It is challenging to delineate Ref-inpainting and NVS using natural language. However, an independent textual prompt can prove advantageous in guiding the diffusion model to execute specific tasks. To address this, we advocate for the use of prompt optimization to acquire task- and perspective-specific prompts, as detailed in Section 3.3. In Ref-inpainting, the weights of LDM remain unchanged to preserve the appropriate generalization, as demonstrated in Figure 2 (b). On the contrary, for NVS, the entire LDM requires fine-tuning, as depicted in Figure 2 (c), yet we emphasize that ARCI exhibits superior convergence compared to other fine-tuning-based techniques, such as Zero123, as elucidated by Liu et al. [2023b].

In order to assess the efficiency of our ARCI in the domain of Ref-inpainting, we conducted a user study, employing human perception as a benchmark, as illustrated in Figure 21. Specifically, 50 image pairs, masked from our test set, were randomly selected and compared against the results produced by SD Rombach et al. (2022), ControlNet Zhang and Agrawala (2023) +match Tang et al. (2022), Perciver Jaegle et al. (2021), Paint-by-Example Yang et al. (2023), TransFill Zhou et al. (2021) (although this method was not open-sourced, we are grateful to TransFill’s creators for testing our samples), and our ARCI. Due to the generosity of TransFill's authors, we were able to evaluate our samples using their testing platform. A total of 10 participants, who were unfamiliar with image generation, were invited to participate in this study. Each participant was presented with masked target images and their corresponding reference images, and was asked to select the best image recovery among the six competitors mentioned above. The selection criteria required participants to consider both the accuracy of the recovery in relation to the reference image and the naturalness of color and texture generation. As depicted in Figure 21, our ARCI demonstrated superior performance compared to the other competitors.

Implementing Incremental Positional Encoding. To enhance the efficiency of NVS (neural volumetric rendering with self-attention), we incrementally incorporate a combination of learnable view embedding and absolute positional encoding into each attention block, as detailed in Eq. 1. This modification notably boosts the performance of both single-view and multi-view-based NVS, as demonstrated in Table 4. Further investigations, including additional ablation studies, are presented in the Appendix A.4.1.


Through the refinement of the ARCI in the context of multi-perspective non-rigid shape analysis, we employ advanced techniques such as artificial generation and increased batch size (referred to as ARCI+). Quantitative data is presented at the foot of Table 2, illustrating that an increased number of reference viewpoints results in enhanced overall reconstruction quality for ARCI+. Furthermore, the incorporation of additional reference images serves to significantly diminish ambiguity, thereby enhancing the final results, as demonstrated by the consistent geometric alignment evident in Figure 19. The utilization of artificial reality in ARCI+ enables the synthesis of a cohesive collection of images from diverse viewpoints, as demonstrated in Figures 7 and 18. Additionally, our method has been proven to be applicable to real-world datasets, as demonstrated in Figure 12.

In essence, the refined ARCI+ model, employing artificial reality and a larger batch size, delivers superior reconstruction quality in multi-view non-rigid shape analysis. The inclusion of additional reference images helps to clarify ambiguity, producing more accurate and consistent results. Furthermore, the application of artificial reality allows for the synthesis of multiple images from a single viewpoint, as demonstrated by the examples provided in Figures 7, 18, and 12.

Pretraining with massive multi-view data is important to preserve identity in CustomNet. We adopt Zero-1-to-3 as the initialization, i.e. , our CustomNet is pre-trained with massive multi-view Objarverse data, so that view-consistency information has been already encoded into the model. When we train CustomNet from the SD checkpoint (see the 2nd column in Fig. 6 ), the synthesized images cannot maintain view consistency from other viewpoints and suffer from quality degradation.

Input concatenation helps better maintain texture details. Previous methods Wei et al. (2023) also try to integrate local features to maintain texture details in synthesized images. In our method, concatenating the reference object to the UNet input can also preserve textures. Without the concatenation (see the 3rd column in Fig. 6 ), the color, shape, and texture of the generated images differ significantly from the reference object image. We also note that the model would generate copying-and-pasting images without any view variations when we do not adopt explicit viewpoint control (see Fig. 5 ). This is to say, the combination of input concatenation and explicit view conditions enables precise and harmonious customization.

For our validation, we utilised 505 pairs from the MegaDepth Li and Snavely [2018] dataset, incorporating select manual masks from the ETH3D scenes Schops et al. [2017]. For our multi-view testing set, we conducted additional filtering of all scenes, retaining only those with a minimum of 4 reference views. This resulted in a final multi-view testing set consisting of 482 images.

Dual cross-attention enables disentangled object and background controls. We introduce dual-attention for the disentangled object-level control and background control with textual descriptions. When directly concatenating the text embedding and fused object embedding as the condition to be injected into the UNet, the model tends to learn a coupled control with viewpoint parameters and textual description. As a result, the viewpoint control capacity would degrade significantly and the model cannot generate desired background (see 5th column in Fig. 6 ).

ARCI, an advanced methodology, is grounded in the fine-tuning of StableDiffusion 2.0 Rombach et al. [2022] under text-guided inpainting. The purpose of ARCI is to efficiently handle a variety of image generation tasks, such as Ref-inpainting and NVS, from reference images. Notably, these tasks can also be applied in a multi-view context, as visualized in Figure 1 (c) and (d). Furthermore, we introduce task and view-specific prompt tuning to manage generative tasks and establish specific view sequences. Impressively, ARCI addresses the challenging Ref-inpainting task, which usually necessitates advanced 3D geometrical warping and 2D inpainting techniques Zhou et al. [2021], end-to-end with minimal extra parameters, while keeping all other weights from StableDiffusion 2.0 unchanged. Conversely, to handle the more complex NVS task, we introduce the innovative technique of block causal masking, enabling self-attention-based T2I models to generate autoregressively, as depicted in Figure 1 (d).

Precise Location Control. With explicit location control design, our method also places an object into the desired location shown in the second row of Fig. 7 , where the location is specified by the user and the background is synthesized directly by the textual description.

4.1 Training Datasets and Implementation Details

In our investigation, we examined the potential for mastering the complexities of noisy-view synthesis (NVS) using minimally available resources, as depicted in the uppermost portion of Table 2 and Figure 6. Employing the CLIP score, initially presented by Radford et al. [2021], we assessed the likeness between the generated and the targeted outputs. Our findings indicate that prompt tuning and LoRA-based fine-tuning, as detailed by Hu et al. [2021], are inadequate for producing high-quality results, as shown in Table 2. However, fine-tuning the entire Large Dynamic Model (LDM) yields significant enhancements. All variations of our proposed Adaptive Recurrent Contextual Inpainting (ARCI) surpass Zero123, the current state-of-the-art competitor, under identical training conditions (batch size 48 with 2 A6000 GPUs), as demonstrated in Table 2. This superiority is due to the fact that Zero123 necessitates a far larger batch size (1536) for stable training. Hence, our ARCI model demonstrates a commendable balance between training efficiency and performance.

Furthermore, we evaluated the utility of Classifier-Free Guidance (CFG) during the training phase, as introduced by Ho and Salimans [2022]. Our results indicate that CFG offers improved pose guidance with a CFG coefficient of 2.5 for the inference. In addition, we present the training logs for ARCI and Zero123 in Figure 10 of the Appendix. Evidently, our ARCI model exhibits a notably faster convergence and superior performance compared to its counterpart.

4.2 Comparison to Existing Methods

We compare our CustomNet to the optimization-based methods Textual Inversion Gal et al. (2022) , Dreambooth Ruiz et al. (2023) , and encoder-based (zero-shot) method GLIGEN Li et al. (2023c) , ELITE Wei et al. (2023) , BLIP-Diffusion Li et al. (2023a) . We use their official implementation (for GLIDEN, ELITE, and BLIP-Diffusion) or the diffuser implementations von Platen et al. (2022) (for Textual Inversion, Dreambooth) to obtain the results. Note that Dreambooth requires several images of the same object to finetune.

Figure 3 shows the images generated with different methods (more results are in the Appendix). We see that the zero-shot methods GLIGEN, ELITE, BLIP-Diffusion, and the optimization-based method Textual Inversion are far from the identity consistent with the reference object. Dreambooth and the proposed CustomNet achieve highly promising harmonious customization results, while our method allows the user to control the object viewpoint easily and obatain diverse results. In addition, our method does not require time-consuming model fine-tuning and textual embedding optimization.

We also evaluate the synthesized results quantitatively. All methods apply 26 different prompts to perform customizations 3 times randomly on 50 objects. We calculate the visual similarity with CLIP image encoder and DINO encoder, denoted as CLIP-I and DINO-I , respectively. We measure the text-image similarity with CLIP directly, denoting CLIP-T . Tab. 1 shows the quantitative results, where CustomNet achieves better identity preservation ( DINO-I and CLIP-I than other methods. Meanwhile, CustomNet shows comparable capacity to the state-of-the-art methods regarding textual control ( CLIP-T ). We also conducted a user study and collected 2700 answers for Identity similarity (ID), View variation (View), and Text alignment (Text), respectively. As shown in the right part of Tab. 1 , most participants prefer CustomNet in all three aspects (78.78%, 64.67%, 67.84%).

Comparison to Inpainting-based Methods Existing inpainting-based methods (SD-Inpainting model, Paint-by-Example Yang et al. (2023) , AnyDoor Chen et al. (2023b) ) can also place a reference object in the desired background in an inpainting pipeline. Given an object, the background can be inpainted with textual descriptions in the SD-Inpainting model, while this kind of method easily suffers from unreal and disharmonious results and cannot cast variations to the reference object. Our CustomNet can obtain more harmonious customization with diverse viewpoint control. Another line of methods Paint-by-Example and AnyDoor can inpaint the reference object to a given background image. AnyDoor has not open-sourced yet and we compare CustomNet with Paint-by-Example in Fig. 4 . From Fig. 4 , we see that Paint-by-Example cannot maintain the identity and differs significantly from the reference object.

4.3 Ablation Studies and Analysis

We conduct detailed ablation studies to demonstrate the effectiveness of each design in CustomNet and the necessity of explicit control for identity preservation in harmonious customization.

 By employing this design, we can extend the size of the initial input image without the requirement for any additional adjustments. To delve into this, let's envision a situation featuring a standalone reference image. Our input, denoted as $\mathbf{I}^{\prime}$, is a composite of $\mathbf{I}_{ref}$ and the masked objective $\hat{\mathbf{I}}_{tar}$, arranged as $\mathbf{I}^{\prime}=[\mathbf{I}_{ref};\hat{\mathbf{I}}_{tar}]\in\mathbb{R}^{H\times 2W}$. Typically, the reference image is situated on the left, while the target image is placed on the right. In instances with multiple reference images, each reference is combined with its corresponding target, as depicted in Figure 3. When it comes to computations, convolutions and cross-attention are handled independently for each view, while they share the same self-attention mechanism as explained in Section 3.2. As Figure 3 (a) reveals, the multi-view Ref-inpainting strategy uses information from various reference perspectives to restore the same target, whereas multi-view NVS can be likened to augmented reality (AR) generation for consistent sequential view synthesis (Figure 3 (b)). Successful generation of masked targets can occur alongside the ARCI, which fills in the empty space in the target region. The inpainting process effectively leverages the contextual learning capacity of the pre-trained self-attention in LDM to tackle reference-guided generation. Further insights regarding data manipulation and mask handling can be found in Appendix A.2.

Pretraining with massive multi-view data is important to preserve identity in CustomNet. We adopt Zero-1-to-3 as the initialization, i.e. , our CustomNet is pre-trained with massive multi-view Objarverse data, so that view-consistency information has been already encoded into the model. When we train CustomNet from the SD checkpoint (see the 2nd column in Fig. 6 ), the synthesized images cannot maintain view consistency from other viewpoints and suffer from quality degradation.

Input concatenation helps better maintain texture details. Previous methods Wei et al. (2023) also try to integrate local features to maintain texture details in synthesized images. In our method, concatenating the reference object to the UNet input can also preserve textures. Without the concatenation (see the 3rd column in Fig. 6 ), the color, shape, and texture of the generated images differ significantly from the reference object image. We also note that the model would generate copying-and-pasting images without any view variations when we do not adopt explicit viewpoint control (see Fig. 5 ). This is to say, the combination of input concatenation and explicit view conditions enables precise and harmonious customization.

Our data construction pipeline enables more harmonious outputs. We adopted a new data construction pipeline for utilizing the OpenImages dataset in Sec. 3.3 . Without this design, the model trained with only the data constructed by the naive combination between multi-view object images in Objaverse and background images can result in unrealistic and unnatural customized results, usually leading to ‘floating’ artifacts (see the 4th column in Fig. 5 ).

Dual cross-attention enables disentangled object and background controls. We introduce dual-attention for the disentangled object-level control and background control with textual descriptions. When directly concatenating the text embedding and fused object embedding as the condition to be injected into the UNet, the model tends to learn a coupled control with viewpoint parameters and textual description. As a result, the viewpoint control capacity would degrade significantly and the model cannot generate desired background (see 5th column in Fig. 6 ).

4.4 More Applications


The question at hand is: "Could pre-existing attention mechanisms have already created significant associations between reference images and the desired generative targets?" To harness this underutilized capability, we propose Attention Reactivated Contextual Inpainting (ARCI), a comprehensive strategy that combines reference-based local and global multi-view synthesis. ARCI creatively reformulates reference-based multi-view synthesis as a contextual inpainting process. This entails blending the reference conditions and masked targets into a unified tensor within the self-attention module, as depicted in Figure 2 (a). We then utilize pre-trained textual encoders and cross-attention modules to guide the generation of T2I models, providing them with essential information for specific generative tasks and desired view orders. The contextual inpainting, initially proposed by Yu et al. [2018], aimed to use attention to fill in missing features for inpainting, showing some parallels with our work, which generates target views through information gathered from references.

Explicit Location Control: Our method uses a design that allows for the placement of an object at a location specified by the user, which is shown in the second row of Figure 7. This location is determined by a textual description, and the background is synthesized directly as a result.


5 Conclusion

Key contributions of the research are outlined as follows:
1) T2I model-based multi-view synthesis: The advanced contextual inpainting formulation and attention mechanisms from generative T2I models facilitate an efficient multi-view synthesis solution, without the need for extensive re-training of T2I models.
2) Task and view-specific prompt tuning: This research introduces the use of task and view-specific prompt tuning, providing control over generative tasks and view orders with precision.
3) End-to-end Reference Inpainting: The research addresses the challenging Reference Inpainting task end-to-end, without relying on intricate 3D geometrical warping or 2D inpainting techniques.
4) Autoregressive NVS with block causal masking: For the complex NVS task, the novel concept of block causal masking is introduced, allowing self-attention-based T2I models to generate results with superior quality and geometric consistency using Autoregressive methods.

In the context of NVS, we expand the object mask and randomly select points within the expanded mask's boundary to create an irregular mask. This expanded mask is then combined with the target images, as shown in Fig. 9 (b). Our findings indicate that local masking is crucial for efficient convergence and stable fine-tuning, as confirmed through experiments. For the data processing in Objaverse, as detailed by Deitke et al. [2022] and Liu et al. [2023b], a dataset consisting of 800k images across various scenes with corresponding object masks is provided. Each scene contains 12 images, each rendered in a 256 $\times$ 256 resolution with varying viewpoints. Similar to Liu et al. [2023b], the spherical coordinate system is employed to convert the relative pose $\Delta p$ into the polar angle $\theta$, azimuth angle $\phi$, and radius $r$ at a distance from the canonical center, represented as $\Delta p=(\Delta\theta,\mathrm{sin}\Delta\phi,\mathrm{cos}\Delta\phi,\Delta r)$. The azimuth angle is encoded sinusoidally to address its non-continuity. When it comes to masking Objaverse images, we dilate the object mask and its corresponding bounding box by 10 to 25 kernel size and 5% to 20%, respectively. Subsequently, we randomly select 20 to 45 points to paint the irregular masks.

Appendix A Appendix


In the context of the NVS, we initiate the process by expanding the object mask and randomly selecting points within the expanded mask's boundary to apply the irregular mask. Subsequently, we incorporate the dilated object mask to encapsulate the entirety of the target images, as demonstrated in Figure 9 (b). We have observed that local masking remains crucial for facilitating rapid convergence and ensuring stabilized fine-tuning, a finding substantiated by experimental evidence. In the realm of data processing for Objaverse, Deitke et al. (2022) and Zero123 Liu et al. (2023b) have supplied a collection of 800k images encompassing a wide array of scenes, each accompanied by an object mask. For each scene, 12 images are generated in a 256 $\times$ 256 resolution with varied viewpoints. In accordance with Liu et al. (2023b), we employ the spherical coordinate system to transform the relative pose $\Delta p$ into the polar angle $\theta$, azimuth angle $\phi$, and radius $r$ that is a distance from the canonical center, with $\Delta p=(\Delta\theta,\mathrm{sin}\Delta\phi,\mathrm{cos}\Delta\phi,\Delta r)$. Here, the azimuth angle is encoded sinusoidally to mitigate its non-continuity. In the masking of Objaverse images, we dilate the object mask and its related boundary with a kernel size ranging from 10 to 25 and a dilation rate of 5% to 20%. Subsequently, we randomly select 20 to 45 points to paint the irregular masks.

We show our proposed data construction pipeline for natural images in Fig. 8 (b). It is the reverse of the first way, i.e. , directly utilizing natural images as the target image and extracting objects from the image as the reference. Specifically, for a natural image, we first use BLIP-2 5T Li et al. (2023b) to extract the foreground object with the instruction {”image”: image, ”prompt”: ”Question: What foreground objects are in the image? find them and separate them using commas. Answer:”}. Then we feed the object and its corresponding text to SAM, SAM can receive text as input and output the segmentation mask of the corresponding object. Then we synthesize a novel view of the object using Zero-1-to-3 with randomly sampled relative viewpoints. In this way, we can synthesize a large amount of data pairs from natural image datasets, like OpenImages Kuznetsova et al. (2020) . Fig. 9 shows some data samples from the two dataset construction pipelines.

the definition and design of RT is consistent with the setting in Zero-1-to-3. In Zero-1-to-3, they render 3D objects to images with a predefined camera pose that always points at the center of the object. This means the viewpoint is actually controlled by polar angle $\theta$ , azimuth angle $\phi$ , and radius $r$ (distance away from the center), resulting all objects to be central of the rendered image. Therefore, the rotation matrix R can only control polar angle $\theta$ , azimuth angle $\phi$ , and the translation vector T can only control the radius $r$ between the object and camera. Thus the Zero-1-to-3 can only synthesize objects at the image center, lacking the capacity to place the object at the arbitrary location in the image. We tackle this problem by specifying the bounding box at the desired location to the latent.

We show our improvements of Zero-1-to-3 compared with the limitations of Zero-1-to-3 in Fig. 10 .

In order to substantiate the applicability of our proposed approach, we create additional sets of multi-perspective images, as shown in Figure 18, using a solitary initial viewpoint. Furthermore, we subject our method to numerous practical scenarios, depicted in Figure 12, employing a single RGB input. It is noteworthy that the initial orientations for all the poses are set at [ $0.5\pi,0,1.5$ ] for the polar angle, azimuth angle, and radial distance, respectively. The advanced ARCI model exhibits a commendable ability to adapt to real-world scenarios.

In the 3rd, 4th row, We compare with Zero-1-to-3 the ability of novel-view synthesis. We can see from the figure that Zero-1-to-3 fails to generate the resonable geometry for the dog and cartoon character. Our CustomNet, training with our real world constructed datasets, have a better comprehension of object geometry. We can generate a normal dog and a resonable cartoon chracter in multi-views.

This research utilizes text-to-image synthesis models, renowned for their powerful generative capabilities. However, these models can potentially produce misleading or fabricated images, a point worthy of user attention. Additionally, privacy and consent should be prioritized, as these models are frequently trained on extensive datasets. It's crucial to note that these models may also perpetuate biases from the training data, resulting in unfair outcomes. Consequently, we urge users to exercise responsibility and inclusivity when employing text-to-image generative models. It's important to highlight that our approach concentrates solely on technical aspects. All images and pre-trained models used in this study are publicly available.

We provide additional qualitative comparison results to demonstrate the superiority of the proposed CustomNet shown in Fig. 11 .

We provide more real world object customized results shown in Fig. 12 , Fig. 13 and 14 .

A.1 Data Construction Pipeline

For the validation of our MegaDepth Li and Snavely [2018] model, we utilized 505 pairs, some of which were manually masked from ETH3D scenes Schops et al. [2017]. To create the multi-view testing set, we filtered all scenes, keeping only those with a minimum of 4 reference views. This resulted in a final multi-view testing set consisting of 482 images.

We show our proposed data construction pipeline for natural images in Fig. 8 (b). It is the reverse of the first way, i.e. , directly utilizing natural images as the target image and extracting objects from the image as the reference. Specifically, for a natural image, we first use BLIP-2 5T Li et al. (2023b) to extract the foreground object with the instruction {”image”: image, ”prompt”: ”Question: What foreground objects are in the image? find them and separate them using commas. Answer:”}. Then we feed the object and its corresponding text to SAM, SAM can receive text as input and output the segmentation mask of the corresponding object. Then we synthesize a novel view of the object using Zero-1-to-3 with randomly sampled relative viewpoints. In this way, we can synthesize a large amount of data pairs from natural image datasets, like OpenImages Kuznetsova et al. (2020) . Fig. 9 shows some data samples from the two dataset construction pipelines.

A.2 Definition of camera model

the definition and design of RT is consistent with the setting in Zero-1-to-3. In Zero-1-to-3, they render 3D objects to images with a predefined camera pose that always points at the center of the object. This means the viewpoint is actually controlled by polar angle $\theta$ , azimuth angle $\phi$ , and radius $r$ (distance away from the center), resulting all objects to be central of the rendered image. Therefore, the rotation matrix R can only control polar angle $\theta$ , azimuth angle $\phi$ , and the translation vector T can only control the radius $r$ between the object and camera. Thus the Zero-1-to-3 can only synthesize objects at the image center, lacking the capacity to place the object at the arbitrary location in the image. We tackle this problem by specifying the bounding box at the desired location to the latent.

A.3 Improvements of Zero-1-to-3

We show our improvements of Zero-1-to-3 compared with the limitations of Zero-1-to-3 in Fig. 10 .

In the 1st, 2nd row, We compare with Zero-1-to-3 the ability to control the object location generation. As Zero-1-to-3 can only control polar angle $\theta$ , azimuth angle $\phi$ , and radius $r$ , it can control the object location directly, so we apply our location control method on Zero-1-to-3. We resize the object and move it to the location in different bounding boxes (top left and top right in 1st row, bottom left and bottom right in 2nd row.). We can see that Zero-1-to-3 generate distortion in the generated images. This is because Zero-1-to-3 only trained on the object centred image, it can not really control the object location.

In the 3rd, 4th row, We compare with Zero-1-to-3 the ability of novel-view synthesis. We can see from the figure that Zero-1-to-3 fails to generate the resonable geometry for the dog and cartoon character. Our CustomNet, training with our real world constructed datasets, have a better comprehension of object geometry. We can generate a normal dog and a resonable cartoon chracter in multi-views.

In all rows, our CustomNet generate harmonious customized image with the control of different text prompts.

A.4 More Comparison Results

We provide additional qualitative comparison results to demonstrate the superiority of the proposed CustomNet shown in Fig. 11 .

A.5 More Results of CustomNet

We provide more real world object customized results shown in Fig. 12 , Fig. 13 and 14 .

References

Chan et al. (2022) Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.  16123–16133, 2022.
Chen & Kae (2019) Bor-Chun Chen and Andrew Kae. Toward realistic image compositing with adversarial learning. InIEEE Conf. Comput. Vis. Pattern Recog., 2019.
Chen et al. (2022) Haoxing Chen, Zhangxuan Gu, Yaohui Li, Jun Lan, Changhua Meng, Weiqiang Wang, and Huaxiong Li. Hierarchical dynamic image harmonization. arXiv:2211.08639, 2022.
Chen et al. (2023a) Hong Chen, Yipeng Zhang, Xin Wang, Xuguang Duan, Yuwei Zhou, and Wenwu Zhu. Disenbooth: Disentangled parameter-efficient tuning for subject-driven text-to-image generation. arXiv preprint arXiv:2305.03374, 2023a.
Chen et al. (2023b) Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot object-level image customization. arXiv preprint arXiv:2307.09481, 2023b.
Cong et al. (2020) Wenyan Cong, Jianfu Zhang, Li Niu, Liu Liu, Zhixin Ling, Weiyuan Li, and Liqing Zhang. Dovenet: Deep image harmonization via domain verification. InIEEE Conf. Comput. Vis. Pattern Recog., 2020.
Cong et al. (2022) Wenyan Cong, Xinhao Tao, Li Niu, Jing Liang, Xuesong Gao, Qihao Sun, and Liqing Zhang. High-resolution image harmonization via collaborative dual transformations. InIEEE Conf. Comput. Vis. Pattern Recog., 2022.
Deitke et al. (2023) Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3d objects. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.  13142–13153, 2023.
Gal et al. (2022) Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv:2208.01618, 2022.
Guo et al. (2021) Zonghui Guo, Haiyong Zheng, Yufeng Jiang, Zhaorui Gu, and Bing Zheng. Intrinsic image harmonization. InIEEE Conf. Comput. Vis. Pattern Recog., 2021.
Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840–6851, 2020.
Jang & Agapito (2021) Wonbong Jang and Lourdes Agapito. Codenerf: Disentangled neural radiance fields for object categories. InProceedings of the IEEE/CVF International Conference on Computer Vision, pp.  12949–12958, 2021.
Kirillov et al. (2023) Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv:2304.02643, 2023.
Kuznetsova et al. (2020) Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. IJCV, 2020.
Li et al. (2023a) Dongxu Li, Junnan Li, and Steven CH Hoi. Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. arXiv:2305.14720, 2023a.
Li et al. (2023b) Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv:2301.12597, 2023b.
Li et al. (2023c) Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.  22511–22521, 2023c.
Liu et al. (2023a) Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. arXiv preprint arXiv:2303.11328, 2023a.
Liu et al. (2023b) Zhiheng Liu, Ruili Feng, Kai Zhu, Yifei Zhang, Kecheng Zheng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. Cones: Concept neurons in diffusion models for customized generation. arXiv preprint arXiv:2303.05125, 2023b.
Loshchilov & Hutter (2017) Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.
Ma et al. (2023) Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subject-diffusion: Open domain personalized text-to-image generation without test-time fine-tuning. arXiv preprint arXiv:2307.11410, 2023.
Mildenhall et al. (2020) Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. InEuropean conference on computer vision, pp.  405–421. Springer, 2020.
Mou et al. (2023) Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv preprint arXiv:2302.08453, 2023.
Nichol et al. (2021) Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv:2112.10741, 2021.
Park et al. (2017) Eunbyung Park, Jimei Yang, Ersin Yumer, Duygu Ceylan, and Alexander C Berg. Transformation-grounded image generation network for novel 3d view synthesis. InProceedings of the ieee conference on computer vision and pattern recognition, pp.  3500–3509, 2017.
Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. InInt. Conf. Mach. Learn., 2021.
Ramesh et al. (2022) Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv:2204.06125, 2022.
Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. InIEEE Conf. Comput. Vis. Pattern Recog., 2022.
Ruiz et al. (2023) Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. InIEEE Conf. Comput. Vis. Pattern Recog., 2023.
RunwayML (2022) RunwayML. Stable diffusion. https://github.com/runwayml/stable-diffusion, 2022.
Saharia et al. (2022) Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Adv. Neural Inform. Process. Syst., 2022.
Sitzmann et al. (2019) Vincent Sitzmann, Michael Zollhöfer, and Gordon Wetzstein. Scene representation networks: Continuous 3d-structure-aware neural scene representations. Advances in Neural Information Processing Systems, 32, 2019.
Sohl-Dickstein et al. (2015) Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. InInternational conference on machine learning, pp.  2256–2265. PMLR, 2015.
Song & Ermon (2019) Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019.
Song et al. (2023) Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price, Jianming Zhang, Soo Ye Kim, and Daniel Aliaga. Objectstitch: Object compositing with diffusion model. InIEEE Conf. Comput. Vis. Pattern Recog., 2023.
Sun et al. (2018) Shao-Hua Sun, Minyoung Huh, Yuan-Hong Liao, Ning Zhang, and Joseph J Lim. Multi-view to novel view: Synthesizing novel views with self-learned confidence. InProceedings of the European Conference on Computer Vision (ECCV), pp.  155–171, 2018.
Sunkavalli et al. (2010) Kalyan Sunkavalli, Micah K Johnson, Wojciech Matusik, and Hanspeter Pfister. Multi-scale image harmonization. ACM Trans. Graph., 2010.
von Platen et al. (2022) Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. https://github.com/huggingface/diffusers, 2022.
Watson et al. (2022) Daniel Watson, William Chan, Ricardo Martin-Brualla, Jonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi. Novel view synthesis with diffusion models. arXiv preprint arXiv:2210.04628, 2022.
Wei et al. (2023) Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. arXiv preprint arXiv:2302.13848, 2023.
Xue et al. (2022) Ben Xue, Shenghui Ran, Quan Chen, Rongfei Jia, Binqiang Zhao, and Xing Tang. Dccf: Deep comprehensible color filter learning framework for high-resolution image harmonization. InEur. Conf. Comput. Vis., 2022.
Yang et al. (2023) Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. InIEEE Conf. Comput. Vis. Pattern Recog., 2023.
Yu et al. (2021) Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.  4578–4587, 2021.
Zhang & Agrawala (2023) Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv:2302.05543, 2023.
Zhou et al. (2018) Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. arXiv preprint arXiv:1805.09817, 2018.
